# Finetune Configuration
# Architecture: WavLM + Attentive Pooling + LayerNorm + Deeper Heads
# Uses pre-extracted features from prepare_data.py
# Copy this file to finetune.yaml and update paths

# Model (for classification heads only - features are pre-extracted)
model:
  name: "microsoft/wavlm-base-plus"  # Used for hidden_size reference
  hidden_size: 768                   # WavLM base hidden dimension
  num_genders: 2
  num_dialects: 3
  dropout: 0.1
  head_hidden_dim: 256

# Training
training:
  batch_size: 32
  learning_rate: 5e-5
  num_epochs: 15
  warmup_ratio: 0.125
  weight_decay: 0.0125
  gradient_clip: 1.0
  lr_scheduler: "linear"
  fp16: true
  dataloader_num_workers: 4

# Loss
loss:
  dialect_weight: 3.0

# MLflow Configuration
mlflow:
  enabled: true
  tracking_uri: "mlruns"
  experiment_name: "speaker-profiling"
  run_name: null
  registered_model_name: null

# Dataset paths
# ============================================================
# STEP 1: Update RAW DATASET PATHS to your local ViSpeech location
# STEP 2: Run prepare_data.py to extract features
# STEP 3: Features will be saved to train_dir/val_dir folders
# ============================================================
data:
  # === RAW DATASET PATHS (for prepare_data.py) ===
  # Download ViSpeech: https://drive.google.com/file/d/1-BbOHf42o6eBje2WqQiiRKMtNxmZiRf9
  # Update these paths to match your local dataset location
  vispeech_root: "/path/to/ViSpeech"  # <-- UPDATE THIS
  
  # Training data
  train_meta: "/path/to/ViSpeech/metadata/trainset.csv"      # <-- UPDATE
  train_audio: "/path/to/ViSpeech/trainset"                   # <-- UPDATE
  
  # Test data
  clean_test_meta: "/path/to/ViSpeech/metadata/clean_testset.csv"
  clean_test_audio: "/path/to/ViSpeech/clean_testset"
  noisy_test_meta: "/path/to/ViSpeech/metadata/noisy_testset.csv"
  noisy_test_audio: "/path/to/ViSpeech/noisy_testset"
  
  # Validation split ratio (extracted from trainset)
  val_split: 0.15
  
  # === EXTRACTED FEATURES PATHS (for finetune.py) ===
  # After running prepare_data.py, features will be saved here
  # These paths are relative to project root
  train_dir: "datasets/ViSpeech/train"
  val_dir: "datasets/ViSpeech/val"

# Output
output:
  dir: "output/speaker-profiling"
  save_total_limit: 3
  metric_for_best_model: "dialect_acc"

# Early Stopping
early_stopping:
  patience: 3
  threshold: 0.0025

# Label Mappings (must match prepare_data.py)
labels:
  gender:
    Male: 0
    Female: 1
  dialect:
    North: 0
    Central: 1
    South: 2

# Reproducibility
seed: 42
