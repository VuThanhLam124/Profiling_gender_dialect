{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13858485,"sourceType":"datasetVersion","datasetId":8828507},{"sourceId":13859486,"sourceType":"datasetVersion","datasetId":8829252}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9769d740","cell_type":"code","source":"# Cài đặt PyTorch 2.8.0 + torchvision + torchaudio tương thích\n!pip install torch==2.8.0+cu128 torchvision==0.23.0+cu128 torchaudio==2.8.0+cu128 --extra-index-url https://download.pytorch.org/whl/cu128\n\n# Cài transformers và accelerate tương thích\nprint(\"Installing transformers and dependencies...\")\n!pip install transformers==4.52.4 accelerate==1.7.0 datasets>=3.6.0\n\nprint(\"Installing other dependencies...\")\n!pip install librosa soundfile audiomentations==0.35.0 wandb safetensors\n\nimport torch\nprint(\"\\n\" + \"=\"*60)\nprint(\"✓ Installation complete!\")\nprint(\"=\"*60)\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\nimport torchvision\nprint(f\"Torchvision version: {torchvision.__version__}\")\n\nimport transformers\nprint(f\"Transformers version: {transformers.__version__}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"97f406cf-f195-440a-9442-74a3890e8ae5","cell_type":"code","source":"print(\"Installing dependencies...\")\n!pip install -q transformers==4.44.0 accelerate==0.33.0 datasets==2.21.0\n!pip install -q librosa soundfile audiomentations==0.35.0 wandb safetensors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T02:15:36.077265Z","iopub.execute_input":"2025-11-27T02:15:36.077753Z","iopub.status.idle":"2025-11-27T02:17:02.171230Z","shell.execute_reply.started":"2025-11-27T02:15:36.077727Z","shell.execute_reply":"2025-11-27T02:17:02.170518Z"}},"outputs":[{"name":"stdout","text":"Installing dependencies...\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2024.6.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"id":"17bd229d","cell_type":"code","source":"# 2. IMPORTS\nimport os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport librosa\nimport json\nimport wandb\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nfrom transformers import (\n    Wav2Vec2FeatureExtractor,\n    AutoModel,\n    Trainer,\n    TrainingArguments,\n    EarlyStoppingCallback\n)\nfrom torch.utils.data import Dataset, ConcatDataset\nfrom audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, Gain\nfrom datasets import load_dataset\nimport random\nimport warnings\nimport gc\nwarnings.filterwarnings('ignore')\n\nprint(\"All libraries imported successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T02:17:04.384194Z","iopub.execute_input":"2025-11-27T02:17:04.384731Z","iopub.status.idle":"2025-11-27T02:17:28.244598Z","shell.execute_reply.started":"2025-11-27T02:17:04.384699Z","shell.execute_reply":"2025-11-27T02:17:28.243810Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-11-27 02:17:14.127165: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764209834.321485      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764209834.379374      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"All libraries imported successfully\n","output_type":"stream"}],"execution_count":2},{"id":"2d7368cf","cell_type":"code","source":"# 3. CONFIGURATION\nclass Config:\n    \"\"\"Optimized configuration for combined datasets\"\"\"\n    \n    # ViSpeech Paths\n    VISPEECH_ROOT = Path('/kaggle/input/vispeech')\n    TRAIN_META = VISPEECH_ROOT / 'metadata/trainset.csv'\n    CLEAN_TEST_META = VISPEECH_ROOT / 'metadata/clean_testset.csv'\n    NOISY_TEST_META = VISPEECH_ROOT / 'metadata/noisy_testset.csv'\n    TRAIN_AUDIO = VISPEECH_ROOT / 'trainset'\n    CLEAN_TEST_AUDIO = VISPEECH_ROOT / 'clean_testset'\n    NOISY_TEST_AUDIO = VISPEECH_ROOT / 'noisy_testset'\n    \n    # ViMD Path\n    VIMD_PATH = '/kaggle/input/vimd-dataset'\n    \n    # Model\n    MODEL_NAME = \"microsoft/wavlm-base-plus\"\n    \n    # Audio Processing\n    SAMPLING_RATE = 16000\n    MAX_DURATION = 5  \n    \n    # Training\n    BATCH_SIZE = 32       \n    LEARNING_RATE = 5e-5    \n    NUM_EPOCHS = 5      \n    WARMUP_RATIO = 0.12  \n    WEIGHT_DECAY = 0.012  \n    GRADIENT_CLIP = 1.0\n    \n    # Data Augmentation\n    AUGMENT_PROB = 0.8\n    \n    # Loss weighting\n    DIALECT_LOSS_WEIGHT = 3.0 \n    \n    # Label mappings (unified for both datasets)\n    GENDER_MAP = {'Male': 0, 'Female': 1, 0: 0, 1: 1}  # Support both string and int\n    DIALECT_MAP = {'North': 0, 'Central': 1, 'South': 2}\n    \n    # ViMD specific: region name to dialect mapping\n    REGION_TO_DIALECT = {'North': 0, 'Central': 1, 'South': 2}\n    \n    # Wandb tracking\n    WANDB_PROJECT = 'vietnamese-speaker-profiling'\n    WANDB_API_KEY = 'f05e29c3466ec288e97041e0e3d541c4087096a6'\n    \n    # Logging (save to file as backup)\n    LOG_DIR = '/kaggle/working/logs'\n    \n    # Output\n    OUTPUT_DIR = '/kaggle/working/speaker-profiling-combined'\n    \n    # Reproducibility\n    SEED = 42\n\nconfig = Config()\n\n# Create necessary directories\nos.makedirs(config.LOG_DIR, exist_ok=True)\nos.makedirs(config.OUTPUT_DIR, exist_ok=True)\n\n# Set seeds\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(config.SEED)\n# Setup Wandb\nos.environ['WANDB_API_KEY'] = config.WANDB_API_KEY\nwandb.login(key=config.WANDB_API_KEY)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"CONFIGURATION\")\nprint(\"=\"*80)\nprint(f\"Model:              {config.MODEL_NAME}\")\nprint(f\"Sampling Rate:      {config.SAMPLING_RATE} Hz\")\nprint(f\"Max Duration:       {config.MAX_DURATION}s\")\nprint(f\"Batch Size:         {config.BATCH_SIZE}\")\nprint(f\"Learning Rate:      {config.LEARNING_RATE}\")\nprint(f\"Epochs:             {config.NUM_EPOCHS}\")\nprint(\"=\"*80)\nprint(f\"Log Directory:      {config.LOG_DIR}\")\nprint(f\"Wandb Project:      {config.WANDB_PROJECT}\")\nprint(f\"Dialect Loss Weight:{config.DIALECT_LOSS_WEIGHT}x\")\nprint(f\"Augmentation Prob:  {config.AUGMENT_PROB}\")\nprint(\"=\"*80)\nprint(f\"✓ Directories created successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T02:17:30.122032Z","iopub.execute_input":"2025-11-27T02:17:30.122675Z","iopub.status.idle":"2025-11-27T02:17:38.550916Z","shell.execute_reply.started":"2025-11-27T02:17:30.122652Z","shell.execute_reply":"2025-11-27T02:17:38.550129Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvuthanhlam848\u001b[0m (\u001b[33mvuthanhlam848-vnpost\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nCONFIGURATION\n================================================================================\nModel:              microsoft/wavlm-base-plus\nSampling Rate:      16000 Hz\nMax Duration:       5s\nBatch Size:         32\nLearning Rate:      5e-05\nEpochs:             5\n================================================================================\nLog Directory:      /kaggle/working/logs\nWandb Project:      vietnamese-speaker-profiling\nDialect Loss Weight:3.0x\nAugmentation Prob:  0.8\n================================================================================\n✓ Directories created successfully\n","output_type":"stream"}],"execution_count":3},{"id":"b8d70132","cell_type":"code","source":"# 4. DATA LOADING (Memory Optimized - Không dùng torchcodec)\n\ndef load_vispeech_data():\n    \"\"\"Load ViSpeech dataset\"\"\"\n    print(\"\\nLoading ViSpeech dataset...\")\n    \n    # Load metadata\n    train_df = pd.read_csv(config.TRAIN_META)\n    clean_test_df = pd.read_csv(config.CLEAN_TEST_META)\n    noisy_test_df = pd.read_csv(config.NOISY_TEST_META)\n    \n    # Combine clean + noisy test into one test set\n    test_df = pd.concat([\n        clean_test_df.assign(audio_dir=str(config.CLEAN_TEST_AUDIO)),\n        noisy_test_df.assign(audio_dir=str(config.NOISY_TEST_AUDIO))\n    ], ignore_index=True)\n    \n    # Add audio_dir to train\n    train_df['audio_dir'] = str(config.TRAIN_AUDIO)\n    \n    # Encode labels\n    for df in [train_df, test_df]:\n        df['gender_label'] = df['gender'].map(config.GENDER_MAP)\n        df['dialect_label'] = df['dialect'].map(config.DIALECT_MAP)\n    \n    # Speaker-based split\n    unique_speakers = train_df['speaker'].unique()\n    train_speakers, val_speakers = train_test_split(\n        unique_speakers,\n        test_size=0.15,\n        random_state=config.SEED,\n        shuffle=True\n    )\n    \n    train_data = train_df[train_df['speaker'].isin(train_speakers)].reset_index(drop=True)\n    val_data = train_df[train_df['speaker'].isin(val_speakers)].reset_index(drop=True)\n    \n    print(f\"  ViSpeech Train:      {len(train_data):,} samples ({len(train_speakers)} speakers)\")\n    print(f\"  ViSpeech Validation: {len(val_data):,} samples ({len(val_speakers)} speakers)\")\n    print(f\"  ViSpeech Test:       {len(test_df):,} samples (clean + noisy combined)\")\n    \n    return train_data, val_data, test_df\n\n\ndef load_vimd_data():\n    \"\"\"Load ViMD dataset - KHÔNG dùng Audio feature để tránh lỗi torchcodec\"\"\"\n    print(\"\\nLoading ViMD dataset...\")\n    \n    try:\n        # Load dataset WITHOUT audio decoding (chỉ lấy metadata)\n        # Audio sẽ được load trực tiếp trong Dataset class bằng librosa\n        ds = load_dataset(\n            config.VIMD_PATH, \n            keep_in_memory=False\n        )\n        \n        # Check available splits\n        available_splits = list(ds.keys())\n        print(f\"  Available splits: {available_splits}\")\n        \n        # Handle both 'valid' and 'validation' keys\n        val_key = None\n        if 'validation' in available_splits:\n            val_key = 'validation'\n        elif 'valid' in available_splits:\n            val_key = 'valid'\n        else:\n            raise KeyError(f\"No validation split found. Available: {available_splits}\")\n        \n        print(f\"  ViMD Train:          {len(ds['train']):,} samples\")\n        print(f\"  ViMD Validation:     {len(ds[val_key]):,} samples\")\n        print(f\"  ViMD Test:           {len(ds['test']):,} samples\")\n        \n        # Normalize split name to 'valid'\n        if val_key == 'validation':\n            ds['valid'] = ds['validation']\n            del ds['validation']\n            gc.collect()\n        \n        # Kiểm tra columns\n        print(f\"  Available columns: {ds['train'].column_names}\")\n        \n        # Tìm column chứa audio path\n        audio_col = None\n        for col in ['audio', 'path', 'file', 'audio_path', 'filename']:\n            if col in ds['train'].column_names:\n                audio_col = col\n                break\n        \n        if audio_col:\n            print(f\"  Audio column: '{audio_col}'\")\n            # Kiểm tra cấu trúc audio column\n            sample = ds['train'][0]\n            if audio_col in sample:\n                audio_val = sample[audio_col]\n                if isinstance(audio_val, dict) and 'path' in audio_val:\n                    print(f\"  Audio path example: {audio_val['path']}\")\n                elif isinstance(audio_val, str):\n                    print(f\"  Audio path example: {audio_val}\")\n        \n        return ds\n    \n    except Exception as e:\n        print(f\"Error loading ViMD dataset: {e}\")\n        raise\n\n\n# Load both datasets\nvispeech_train, vispeech_val, vispeech_test = load_vispeech_data()\nvimd_dataset = load_vimd_data()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TOTAL DATA\")\nprint(\"=\"*80)\nprint(f\"Train:      {len(vispeech_train) + len(vimd_dataset['train']):,} samples\")\nprint(f\"Validation: {len(vispeech_val) + len(vimd_dataset['valid']):,} samples\")\nprint(f\"Test:       {len(vispeech_test) + len(vimd_dataset['test']):,} samples\")\nprint(\"=\"*80)\n\n# Clean up\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T02:17:38.552976Z","iopub.execute_input":"2025-11-27T02:17:38.553529Z","iopub.status.idle":"2025-11-27T02:23:56.747418Z","shell.execute_reply.started":"2025-11-27T02:17:38.553492Z","shell.execute_reply":"2025-11-27T02:23:56.746518Z"}},"outputs":[{"name":"stdout","text":"\nLoading ViSpeech dataset...\n  ViSpeech Train:      7,137 samples (263 speakers)\n  ViSpeech Validation: 1,029 samples (47 speakers)\n  ViSpeech Test:       2,520 samples (clean + noisy combined)\n\nLoading ViMD dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/87 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c35ba288f941499e95d001f69898aabb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0/87 [00:00<?, ?files/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c4f7e9bbcd94ecba8af7047dcbefad6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2a42435be33421db4c9a947a07257c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9f87ab0175b40b6b8e9e56f3859d065"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26e4951e6150472cb5ae4f86e7780890"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading dataset shards:   0%|          | 0/87 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49625fd860494b39a05ab7c861095586"}},"metadata":{}},{"name":"stdout","text":"  Available splits: ['train', 'validation', 'test']\n  ViMD Train:          15,023 samples\n  ViMD Validation:     1,900 samples\n  ViMD Test:           2,026 samples\n  Available columns: ['region', 'province_code', 'province_name', 'filename', 'text', 'speakerID', 'gender', 'audio']\n  Audio column: 'audio'\n  Audio path example: 11_0001.wav\n\n================================================================================\nTOTAL DATA\n================================================================================\nTrain:      22,160 samples\nValidation: 2,929 samples\nTest:       4,546 samples\n================================================================================\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"67788"},"metadata":{}}],"execution_count":4},{"id":"1fe862c0","cell_type":"code","source":"# TEST: Kiểm tra cấu trúc dataset ViMD\nprint(\"Kiểm tra cấu trúc ViMD dataset...\")\n\ntry:\n    # Get one sample from ViMD train\n    sample = vimd_dataset['train'][0]\n    \n    print(f\"\\n1. Dataset columns: {vimd_dataset['train'].column_names}\")\n    print(f\"2. Sample keys: {sample.keys()}\")\n    \n    # Kiểm tra audio column\n    if 'audio' in sample:\n        audio_data = sample['audio']\n        print(f\"\\n3. Audio type: {type(audio_data)}\")\n        \n        if isinstance(audio_data, dict):\n            print(f\"   Audio keys: {audio_data.keys()}\")\n            if 'path' in audio_data:\n                print(f\"   Audio path: {audio_data['path']}\")\n            if 'bytes' in audio_data:\n                print(f\"   Audio bytes length: {len(audio_data['bytes']) if audio_data['bytes'] else 0}\")\n        elif isinstance(audio_data, str):\n            print(f\"   Audio path: {audio_data}\")\n    \n    # Check labels\n    print(f\"\\n4. Labels:\")\n    print(f\"   Region (dialect): {sample.get('region', 'N/A')}\")\n    print(f\"   Gender: {sample.get('gender', 'N/A')}\")\n    \n    print(f\"\\n✓ Dataset structure verified!\")\n    \nexcept Exception as e:\n    import traceback\n    print(f\"✗ Error:\")\n    traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T02:28:43.126856Z","iopub.execute_input":"2025-11-27T02:28:43.127445Z","iopub.status.idle":"2025-11-27T02:28:43.181426Z","shell.execute_reply.started":"2025-11-27T02:28:43.127421Z","shell.execute_reply":"2025-11-27T02:28:43.180275Z"}},"outputs":[{"name":"stdout","text":"Kiểm tra cấu trúc ViMD dataset...\n\n1. Dataset columns: ['region', 'province_code', 'province_name', 'filename', 'text', 'speakerID', 'gender', 'audio']\n2. Sample keys: dict_keys(['region', 'province_code', 'province_name', 'filename', 'text', 'speakerID', 'gender', 'audio'])\n\n3. Audio type: <class 'dict'>\n   Audio keys: dict_keys(['path', 'array', 'sampling_rate'])\n   Audio path: 11_0001.wav\n\n4. Labels:\n   Region (dialect): North\n   Gender: 1\n\n✓ Dataset structure verified!\n","output_type":"stream"}],"execution_count":5},{"id":"9092b6eb-0c55-4ebf-a679-beef48c6c35a","cell_type":"code","source":"# 5. AUDIO AUGMENTATION\n\nclass AudioAugmentation:\n    \"\"\"Audio augmentation pipeline\"\"\"\n    \n    def __init__(self, sampling_rate=16000, augment_prob=0.8):\n        self.sampling_rate = sampling_rate\n        self.augment_prob = augment_prob\n        \n        self.augment = Compose([\n            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n            TimeStretch(min_rate=0.8, max_rate=1.2, leave_length_unchanged=False, p=0.5),\n            PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n            Shift(min_shift=-0.5, max_shift=0.5, p=0.3),\n            Gain(min_gain_db=-12, max_gain_db=12, p=0.5),\n        ])\n    \n    def __call__(self, audio):\n        if random.random() < self.augment_prob:\n            return self.augment(samples=audio, sample_rate=self.sampling_rate)\n        return audio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T02:28:47.307236Z","iopub.execute_input":"2025-11-27T02:28:47.307729Z","iopub.status.idle":"2025-11-27T02:28:47.313311Z","shell.execute_reply.started":"2025-11-27T02:28:47.307703Z","shell.execute_reply":"2025-11-27T02:28:47.312714Z"}},"outputs":[],"execution_count":6},{"id":"1b6195d2","cell_type":"code","source":"# 6. DATASET CLASSES (Sử dụng librosa để load audio, không dùng torchcodec)\nimport soundfile as sf\nimport io\n\nclass ViSpeechDataset(Dataset):\n    \"\"\"Dataset for ViSpeech (CSV-based)\"\"\"\n    \n    def __init__(self, dataframe, feature_extractor,\n                 sampling_rate=16000, max_duration=5, is_training=True):\n        self.df = dataframe.reset_index(drop=True)\n        self.feature_extractor = feature_extractor\n        self.sampling_rate = sampling_rate\n        self.max_length = int(sampling_rate * max_duration)\n        self.is_training = is_training\n        \n        if is_training:\n            self.augmentation = AudioAugmentation(sampling_rate, augment_prob=config.AUGMENT_PROB)\n        else:\n            self.augmentation = None\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def load_audio(self, audio_name, audio_dir):\n        audio_path = Path(audio_dir) / audio_name\n        \n        try:\n            audio, sr = librosa.load(audio_path, sr=self.sampling_rate, mono=True)\n            audio, _ = librosa.effects.trim(audio, top_db=20)\n            \n            if self.is_training and self.augmentation is not None:\n                audio = self.augmentation(audio)\n            \n            audio = audio / (np.max(np.abs(audio)) + 1e-8)\n            \n            if len(audio) < self.max_length:\n                audio = np.pad(audio, (0, self.max_length - len(audio)))\n            else:\n                if self.is_training:\n                    start = np.random.randint(0, len(audio) - self.max_length + 1)\n                else:\n                    start = (len(audio) - self.max_length) // 2\n                audio = audio[start:start + self.max_length]\n            \n            return audio\n            \n        except Exception as e:\n            print(f\"Error loading {audio_path}: {e}\")\n            return np.zeros(self.max_length)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        audio = self.load_audio(row['audio_name'], row['audio_dir'])\n        \n        inputs = self.feature_extractor(\n            audio,\n            sampling_rate=self.sampling_rate,\n            return_tensors=\"pt\",\n            padding=True\n        )\n        \n        return {\n            'input_values': inputs.input_values.squeeze(0),\n            'gender_labels': torch.tensor(row['gender_label'], dtype=torch.long),\n            'dialect_labels': torch.tensor(row['dialect_label'], dtype=torch.long)\n        }\n\n\nclass ViMDDataset(Dataset):\n    \"\"\"Dataset for ViMD (HuggingFace format) - Load audio bằng librosa/soundfile\"\"\"\n    \n    def __init__(self, hf_dataset, feature_extractor,\n                 sampling_rate=16000, max_duration=5, is_training=True):\n        self.dataset = hf_dataset\n        self.feature_extractor = feature_extractor\n        self.sampling_rate = sampling_rate\n        self.max_length = int(sampling_rate * max_duration)\n        self.is_training = is_training\n        \n        if is_training:\n            self.augmentation = AudioAugmentation(sampling_rate, augment_prob=config.AUGMENT_PROB)\n        else:\n            self.augmentation = None\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def load_audio_from_hf(self, audio_data):\n        \"\"\"Load audio từ HuggingFace dataset bằng librosa/soundfile (không dùng torchcodec)\"\"\"\n        try:\n            audio = None\n            sr = None\n            \n            # Case 1: audio_data là dict với 'path' hoặc 'bytes'\n            if isinstance(audio_data, dict):\n                # Thử load từ path trước\n                if 'path' in audio_data and audio_data['path']:\n                    try:\n                        audio, sr = librosa.load(audio_data['path'], sr=self.sampling_rate, mono=True)\n                    except Exception:\n                        pass\n                \n                # Nếu không có path, thử load từ bytes\n                if audio is None and 'bytes' in audio_data and audio_data['bytes']:\n                    try:\n                        audio_bytes = io.BytesIO(audio_data['bytes'])\n                        audio, sr = sf.read(audio_bytes)\n                        if len(audio.shape) > 1:\n                            audio = np.mean(audio, axis=1)\n                        if sr != self.sampling_rate:\n                            audio = librosa.resample(audio, orig_sr=sr, target_sr=self.sampling_rate)\n                    except Exception:\n                        pass\n                \n                # Nếu có array (đã decode sẵn)\n                if audio is None and 'array' in audio_data:\n                    try:\n                        audio = np.array(audio_data['array'], dtype=np.float32)\n                        sr = audio_data.get('sampling_rate', self.sampling_rate)\n                        if sr != self.sampling_rate:\n                            audio = librosa.resample(audio, orig_sr=sr, target_sr=self.sampling_rate)\n                    except Exception:\n                        pass\n            \n            # Case 2: audio_data là string (path)\n            elif isinstance(audio_data, str):\n                audio, sr = librosa.load(audio_data, sr=self.sampling_rate, mono=True)\n            \n            # Validation\n            if audio is None or len(audio) == 0:\n                return np.zeros(self.max_length, dtype=np.float32)\n            \n            # Convert to mono if stereo\n            if len(audio.shape) > 1:\n                audio = np.mean(audio, axis=1)\n            \n            # Trim silence\n            if len(audio) > 100:\n                try:\n                    audio, _ = librosa.effects.trim(audio, top_db=20)\n                except Exception:\n                    pass\n            \n            # Apply augmentation\n            if self.is_training and self.augmentation is not None and len(audio) > 100:\n                try:\n                    audio = self.augmentation(audio)\n                except Exception:\n                    pass\n            \n            # Normalize\n            if len(audio) > 0:\n                max_val = np.max(np.abs(audio))\n                if max_val > 1e-8:\n                    audio = audio / max_val\n                else:\n                    return np.zeros(self.max_length, dtype=np.float32)\n            \n            # Pad or crop\n            if len(audio) < self.max_length:\n                audio = np.pad(audio, (0, self.max_length - len(audio)))\n            else:\n                if self.is_training:\n                    start = np.random.randint(0, max(1, len(audio) - self.max_length))\n                else:\n                    start = max(0, (len(audio) - self.max_length) // 2)\n                audio = audio[start:start + self.max_length]\n            \n            return audio.astype(np.float32)\n            \n        except Exception as e:\n            # Không in lỗi để tránh spam console\n            return np.zeros(self.max_length, dtype=np.float32)\n    \n    def __getitem__(self, idx):\n        try:\n            item = self.dataset[idx]\n            \n            # Load audio bằng librosa/soundfile\n            audio = self.load_audio_from_hf(item.get('audio'))\n            \n            # Extract features\n            inputs = self.feature_extractor(\n                audio,\n                sampling_rate=self.sampling_rate,\n                return_tensors=\"pt\",\n                padding=True\n            )\n            \n            # Map labels\n            gender_label = item.get('gender', 0)\n            region = item.get('region', 'North')\n            dialect_label = config.REGION_TO_DIALECT.get(region, 0)\n            \n            return {\n                'input_values': inputs.input_values.squeeze(0),\n                'gender_labels': torch.tensor(gender_label, dtype=torch.long),\n                'dialect_labels': torch.tensor(dialect_label, dtype=torch.long)\n            }\n            \n        except Exception as e:\n            # Return dummy data to prevent crash\n            dummy_audio = np.zeros(self.max_length, dtype=np.float32)\n            inputs = self.feature_extractor(\n                dummy_audio,\n                sampling_rate=self.sampling_rate,\n                return_tensors=\"pt\",\n                padding=True\n            )\n            return {\n                'input_values': inputs.input_values.squeeze(0),\n                'gender_labels': torch.tensor(0, dtype=torch.long),\n                'dialect_labels': torch.tensor(0, dtype=torch.long)\n            }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T02:28:49.011407Z","iopub.execute_input":"2025-11-27T02:28:49.011696Z","iopub.status.idle":"2025-11-27T02:28:49.032443Z","shell.execute_reply.started":"2025-11-27T02:28:49.011674Z","shell.execute_reply":"2025-11-27T02:28:49.031869Z"}},"outputs":[],"execution_count":7},{"id":"e141301f","cell_type":"code","source":"# 7. MULTI-TASK MODEL\n\nclass MultiTaskSpeakerModel(torch.nn.Module):\n    def __init__(self, model_name, num_genders=2, num_dialects=3,\n                 freeze_encoder=False, dropout=0.1):\n        super().__init__()\n        \n        self.wavlm = AutoModel.from_pretrained(model_name)\n        \n        if freeze_encoder:\n            for param in self.wavlm.parameters():\n                param.requires_grad = False\n            print(\"  Encoder FROZEN\")\n        \n        hidden_size = self.wavlm.config.hidden_size\n        self.dropout = torch.nn.Dropout(dropout)\n        self.gender_head = torch.nn.Linear(hidden_size, num_genders)\n        self.dialect_head = torch.nn.Linear(hidden_size, num_dialects)\n        \n    def forward(self, input_values, gender_labels=None, dialect_labels=None):\n        outputs = self.wavlm(input_values)\n        hidden_states = outputs.last_hidden_state\n        pooled = torch.mean(hidden_states, dim=1)\n        pooled = self.dropout(pooled)\n        \n        gender_logits = self.gender_head(pooled)\n        dialect_logits = self.dialect_head(pooled)\n        \n        loss = None\n        if gender_labels is not None and dialect_labels is not None:\n            loss_fct = torch.nn.CrossEntropyLoss()\n            gender_loss = loss_fct(gender_logits, gender_labels)\n            dialect_loss = loss_fct(dialect_logits, dialect_labels)\n            loss = gender_loss + config.DIALECT_LOSS_WEIGHT * dialect_loss\n        \n        return {\n            'loss': loss,\n            'gender_logits': gender_logits,\n            'dialect_logits': dialect_logits\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T02:28:51.938778Z","iopub.execute_input":"2025-11-27T02:28:51.939086Z","iopub.status.idle":"2025-11-27T02:28:51.945851Z","shell.execute_reply.started":"2025-11-27T02:28:51.939065Z","shell.execute_reply":"2025-11-27T02:28:51.945123Z"}},"outputs":[],"execution_count":8},{"id":"f3f06dd5","cell_type":"code","source":"# 8. CUSTOM TRAINER\n\nclass MultiTaskTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        gender_labels = inputs.pop(\"gender_labels\")\n        dialect_labels = inputs.pop(\"dialect_labels\")\n        \n        outputs = model(\n            input_values=inputs[\"input_values\"],\n            gender_labels=gender_labels,\n            dialect_labels=dialect_labels\n        )\n        \n        loss = outputs[\"loss\"]\n        return (loss, outputs) if return_outputs else loss\n    \n    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n        gender_labels = inputs.pop(\"gender_labels\")\n        dialect_labels = inputs.pop(\"dialect_labels\")\n        \n        with torch.no_grad():\n            outputs = model(\n                input_values=inputs[\"input_values\"],\n                gender_labels=gender_labels,\n                dialect_labels=dialect_labels\n            )\n            loss = outputs[\"loss\"]\n        \n        return (\n            loss,\n            (outputs[\"gender_logits\"], outputs[\"dialect_logits\"]),\n            (gender_labels, dialect_labels)\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T02:28:52.967590Z","iopub.execute_input":"2025-11-27T02:28:52.967900Z","iopub.status.idle":"2025-11-27T02:28:52.973627Z","shell.execute_reply.started":"2025-11-27T02:28:52.967877Z","shell.execute_reply":"2025-11-27T02:28:52.972868Z"}},"outputs":[],"execution_count":9},{"id":"ba8f38a3","cell_type":"code","source":"# 9. METRICS\n\ndef compute_metrics(pred):\n    gender_logits, dialect_logits = pred.predictions\n    gender_labels, dialect_labels = pred.label_ids\n    \n    gender_preds = np.argmax(gender_logits, axis=-1)\n    dialect_preds = np.argmax(dialect_logits, axis=-1)\n    \n    gender_acc = accuracy_score(gender_labels, gender_preds)\n    gender_f1 = f1_score(gender_labels, gender_preds, average='weighted')\n    dialect_acc = accuracy_score(dialect_labels, dialect_preds)\n    dialect_f1 = f1_score(dialect_labels, dialect_preds, average='weighted')\n    \n    return {\n        'gender_acc': gender_acc,\n        'gender_f1': gender_f1,\n        'dialect_acc': dialect_acc,\n        'dialect_f1': dialect_f1,\n        'combined_f1': (gender_f1 + dialect_f1) / 2\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T02:28:53.756946Z","iopub.execute_input":"2025-11-27T02:28:53.757241Z","iopub.status.idle":"2025-11-27T02:28:53.762805Z","shell.execute_reply.started":"2025-11-27T02:28:53.757222Z","shell.execute_reply":"2025-11-27T02:28:53.762000Z"}},"outputs":[],"execution_count":10},{"id":"a52597af","cell_type":"code","source":"# 10. INITIALIZE MODEL & DATASETS\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"INITIALIZING\")\nprint(\"=\"*80)\n\n# Feature extractor\nprint(\"\\nLoading feature extractor...\")\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(config.MODEL_NAME)\nprint(\"  Feature extractor loaded\")\n\n# Create ViSpeech datasets\nprint(\"\\nCreating ViSpeech datasets...\")\nvispeech_train_dataset = ViSpeechDataset(\n    vispeech_train, feature_extractor,\n    sampling_rate=config.SAMPLING_RATE,\n    max_duration=config.MAX_DURATION,\n    is_training=True\n)\nvispeech_val_dataset = ViSpeechDataset(\n    vispeech_val, feature_extractor,\n    sampling_rate=config.SAMPLING_RATE,\n    max_duration=config.MAX_DURATION,\n    is_training=False\n)\nvispeech_test_dataset = ViSpeechDataset(\n    vispeech_test, feature_extractor,\n    sampling_rate=config.SAMPLING_RATE,\n    max_duration=config.MAX_DURATION,\n    is_training=False\n)\nprint(f\"  ViSpeech: {len(vispeech_train_dataset)} train, {len(vispeech_val_dataset)} val, {len(vispeech_test_dataset)} test\")\n\n# Create ViMD datasets\nprint(\"\\nCreating ViMD datasets...\")\nvimd_train_dataset = ViMDDataset(\n    vimd_dataset['train'], feature_extractor,\n    sampling_rate=config.SAMPLING_RATE,\n    max_duration=config.MAX_DURATION,\n    is_training=True\n)\nvimd_val_dataset = ViMDDataset(\n    vimd_dataset['valid'], feature_extractor,\n    sampling_rate=config.SAMPLING_RATE,\n    max_duration=config.MAX_DURATION,\n    is_training=False\n)\nvimd_test_dataset = ViMDDataset(\n    vimd_dataset['test'], feature_extractor,\n    sampling_rate=config.SAMPLING_RATE,\n    max_duration=config.MAX_DURATION,\n    is_training=False\n)\nprint(f\"  ViMD: {len(vimd_train_dataset)} train, {len(vimd_val_dataset)} val, {len(vimd_test_dataset)} test\")\n\n# Combine datasets\nprint(\"\\nCombining datasets...\")\ncombined_train_dataset = ConcatDataset([vispeech_train_dataset, vimd_train_dataset])\ncombined_val_dataset = ConcatDataset([vispeech_val_dataset, vimd_val_dataset])\ncombined_test_dataset = ConcatDataset([vispeech_test_dataset, vimd_test_dataset])\n\nprint(f\"  Combined Train:      {len(combined_train_dataset):,} samples\")\nprint(f\"  Combined Validation: {len(combined_val_dataset):,} samples\")\nprint(f\"  Combined Test:       {len(combined_test_dataset):,} samples\")\n\n# Model\nprint(\"\\nLoading model...\")\nmodel = MultiTaskSpeakerModel(\n    config.MODEL_NAME,\n    num_genders=2,\n    num_dialects=3,\n    freeze_encoder=False,\n    dropout=0.1\n)\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"  Model: {config.MODEL_NAME}\")\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\n\n# Check GPU availability\nif torch.cuda.is_available():\n    print(f\"  Device: GPU - {torch.cuda.get_device_name(0)}\")\n    print(f\"  CUDA Version: {torch.version.cuda}\")\nelse:\n    print(f\"  Device: CPU (no GPU available)\")\n\nprint(\"\\n\" + \"=\"*80)\n\n# Clean up memory\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T02:28:55.191136Z","iopub.execute_input":"2025-11-27T02:28:55.191407Z","iopub.status.idle":"2025-11-27T02:29:00.817023Z","shell.execute_reply.started":"2025-11-27T02:28:55.191388Z","shell.execute_reply":"2025-11-27T02:29:00.816160Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nINITIALIZING\n================================================================================\n\nLoading feature extractor...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2a551888bab4bc4b307ba0998e1f477"}},"metadata":{}},{"name":"stdout","text":"  Feature extractor loaded\n\nCreating ViSpeech datasets...\n  ViSpeech: 7137 train, 1029 val, 2520 test\n\nCreating ViMD datasets...\n  ViMD: 15023 train, 1900 val, 2026 test\n\nCombining datasets...\n  Combined Train:      22,160 samples\n  Combined Validation: 2,929 samples\n  Combined Test:       4,546 samples\n\nLoading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21030666cf2d45acabc2f917b41e5014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e54c941fbe894fc297c662ec6bce278c"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/wavlm-base-plus were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of WavLMModel were not initialized from the model checkpoint at microsoft/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"  Model: microsoft/wavlm-base-plus\n  Total parameters: 94,385,781\n  Trainable parameters: 94,385,781\n  Device: GPU - Tesla P100-PCIE-16GB\n  CUDA Version: 12.4\n\n================================================================================\n","output_type":"stream"}],"execution_count":11},{"id":"daad20f5","cell_type":"code","source":"# 11. TRAINING ARGUMENTS\n\ntraining_args = TrainingArguments(\n    output_dir=config.OUTPUT_DIR,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    \n    # Optimization\n    learning_rate=config.LEARNING_RATE,\n    per_device_train_batch_size=config.BATCH_SIZE,\n    per_device_eval_batch_size=config.BATCH_SIZE,\n    num_train_epochs=config.NUM_EPOCHS,\n    weight_decay=config.WEIGHT_DECAY,\n    warmup_ratio=config.WARMUP_RATIO,\n    max_grad_norm=config.GRADIENT_CLIP,\n    \n    # Scheduler\n    lr_scheduler_type='linear',\n    \n    # Best model selection\n    load_best_model_at_end=True,\n    metric_for_best_model='dialect_acc',\n    greater_is_better=True,\n    save_total_limit=3,\n    \n    # Performance\n    fp16=True,  # Enable mixed precision training for faster GPU training\n    dataloader_num_workers=0,  # Fix multiprocessing issue with HuggingFace datasets\n    \n    # Logging\n    logging_steps=50,\n    logging_first_step=True,\n    report_to='wandb',  # Enable Wandb logging\n    \n    # Misc\n    remove_unused_columns=False,\n    seed=config.SEED,\n)\n\nearly_stopping = EarlyStoppingCallback(\n    early_stopping_patience=3,\n    early_stopping_threshold=0.0025\n)\n\nprint(\"✓ Training arguments configured\")\nprint(f\"  FP16: {training_args.fp16}\")\nprint(f\"  Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T02:29:09.605017Z","iopub.execute_input":"2025-11-27T02:29:09.605327Z","iopub.status.idle":"2025-11-27T02:29:09.648305Z","shell.execute_reply.started":"2025-11-27T02:29:09.605305Z","shell.execute_reply":"2025-11-27T02:29:09.647635Z"}},"outputs":[{"name":"stdout","text":"✓ Training arguments configured\n  FP16: True\n  Device: GPU\n","output_type":"stream"}],"execution_count":12},{"id":"9cb795b6","cell_type":"code","source":"# 12. START TRAINING\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING\")\nprint(\"=\"*80)\n\n# Ensure log directory exists\nos.makedirs(config.LOG_DIR, exist_ok=True)\n\n# Initialize Wandb run\nwandb.init(\n    project=config.WANDB_PROJECT,\n    name=f\"combined-training-{config.SEED}\",\n    config={\n        'model_name': config.MODEL_NAME,\n        'sampling_rate': config.SAMPLING_RATE,\n        'max_duration': config.MAX_DURATION,\n        'batch_size': config.BATCH_SIZE,\n        'learning_rate': config.LEARNING_RATE,\n        'num_epochs': config.NUM_EPOCHS,\n        'warmup_ratio': config.WARMUP_RATIO,\n        'weight_decay': config.WEIGHT_DECAY,\n        'augment_prob': config.AUGMENT_PROB,\n        'dialect_loss_weight': config.DIALECT_LOSS_WEIGHT,\n        'train_samples': len(combined_train_dataset),\n        'val_samples': len(combined_val_dataset),\n        'test_samples': len(combined_test_dataset),\n        'vispeech_train': len(vispeech_train_dataset),\n        'vimd_train': len(vimd_train_dataset),\n        'seed': config.SEED\n    }\n)\n\nprint(f\"Wandb run initialized: {wandb.run.name}\")\nprint(f\"Track at: {wandb.run.url}\")\n\n# Save training config to file as backup\nconfig_dict = wandb.config.as_dict()\nwith open(f'{config.LOG_DIR}/training_config.json', 'w') as f:\n    json.dump(config_dict, f, indent=2)\n\nprint(f\"Training config saved to: {config.LOG_DIR}/training_config.json\")\n\n# Initialize trainer (Wandb logging is automatic)\ntrainer = MultiTaskTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=combined_train_dataset,\n    eval_dataset=combined_val_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[early_stopping],\n)\n\nprint(f\"\\nStarting training...\")\nprint(f\"  Epochs: {config.NUM_EPOCHS}\")\nprint(f\"  Steps per epoch: ~{len(combined_train_dataset) // config.BATCH_SIZE}\")\nprint(f\"  Total steps: ~{(len(combined_train_dataset) // config.BATCH_SIZE) * config.NUM_EPOCHS}\")\n\n# Train\ntrain_result = trainer.train()\n\n# Log final training metrics to Wandb\nwandb.log({\n    'final/train_loss': train_result.training_loss,\n    'final/train_runtime': train_result.metrics['train_runtime'],\n    'final/train_samples_per_second': train_result.metrics['train_samples_per_second']\n})\n\n# Save final training metrics to file as backup\nfinal_metrics = {\n    'train_loss': float(train_result.training_loss),\n    'train_runtime': float(train_result.metrics['train_runtime']),\n    'train_samples_per_second': float(train_result.metrics['train_samples_per_second'])\n}\n\nwith open(f'{config.LOG_DIR}/final_train_metrics.json', 'w') as f:\n    json.dump(final_metrics, f, indent=2)\n\nprint(\"\\nTraining completed!\")\nprint(f\"Final metrics saved to: {config.LOG_DIR}/final_train_metrics.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T02:29:12.820708Z","iopub.execute_input":"2025-11-27T02:29:12.821383Z","iopub.status.idle":"2025-11-27T09:08:05.232205Z","shell.execute_reply.started":"2025-11-27T02:29:12.821357Z","shell.execute_reply":"2025-11-27T09:08:05.230674Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nTRAINING\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251127_022912-ia5duf07</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/vuthanhlam848-vnpost/vietnamese-speaker-profiling/runs/ia5duf07' target=\"_blank\">combined-training-42</a></strong> to <a href='https://wandb.ai/vuthanhlam848-vnpost/vietnamese-speaker-profiling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/vuthanhlam848-vnpost/vietnamese-speaker-profiling' target=\"_blank\">https://wandb.ai/vuthanhlam848-vnpost/vietnamese-speaker-profiling</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/vuthanhlam848-vnpost/vietnamese-speaker-profiling/runs/ia5duf07' target=\"_blank\">https://wandb.ai/vuthanhlam848-vnpost/vietnamese-speaker-profiling/runs/ia5duf07</a>"},"metadata":{}},{"name":"stdout","text":"Wandb run initialized: combined-training-42\nTrack at: https://wandb.ai/vuthanhlam848-vnpost/vietnamese-speaker-profiling/runs/ia5duf07\nTraining config saved to: /kaggle/working/logs/training_config.json\n\nStarting training...\n  Epochs: 5\n  Steps per epoch: ~692\n  Total steps: ~3460\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3465' max='3465' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3465/3465 6:38:25, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Gender Acc</th>\n      <th>Gender F1</th>\n      <th>Dialect Acc</th>\n      <th>Dialect F1</th>\n      <th>Combined F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.716500</td>\n      <td>2.034827</td>\n      <td>0.779788</td>\n      <td>0.782697</td>\n      <td>0.804029</td>\n      <td>0.792405</td>\n      <td>0.787551</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.033000</td>\n      <td>1.775537</td>\n      <td>0.891431</td>\n      <td>0.892059</td>\n      <td>0.806419</td>\n      <td>0.804766</td>\n      <td>0.848413</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.743000</td>\n      <td>1.619355</td>\n      <td>0.943667</td>\n      <td>0.943479</td>\n      <td>0.819392</td>\n      <td>0.818469</td>\n      <td>0.880974</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.540000</td>\n      <td>1.379736</td>\n      <td>0.950836</td>\n      <td>0.950628</td>\n      <td>0.852168</td>\n      <td>0.851085</td>\n      <td>0.900856</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.473000</td>\n      <td>1.428137</td>\n      <td>0.960055</td>\n      <td>0.960031</td>\n      <td>0.840901</td>\n      <td>0.840487</td>\n      <td>0.900259</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nTraining completed!\nFinal metrics saved to: /kaggle/working/logs/final_train_metrics.json\n","output_type":"stream"}],"execution_count":13},{"id":"5c65208f","cell_type":"code","source":"# 13. EVALUATION\n\ndef evaluate_and_report(trainer, dataset, dataset_name):\n    \"\"\"Comprehensive evaluation with file logging\"\"\"\n    # Ensure log directory exists\n    os.makedirs(config.LOG_DIR, exist_ok=True)\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"EVALUATING ON {dataset_name.upper()}\")\n    print('='*80)\n    \n    results = trainer.predict(dataset)\n    gender_logits, dialect_logits = results.predictions\n    gender_labels, dialect_labels = results.label_ids\n    \n    gender_preds = np.argmax(gender_logits, axis=-1)\n    dialect_preds = np.argmax(dialect_logits, axis=-1)\n    \n    # Overall metrics\n    gender_acc = accuracy_score(gender_labels, gender_preds) * 100\n    dialect_acc = accuracy_score(dialect_labels, dialect_preds) * 100\n    gender_f1 = f1_score(gender_labels, gender_preds, average='weighted') * 100\n    dialect_f1 = f1_score(dialect_labels, dialect_preds, average='weighted') * 100\n    \n    print(f\"\\nOverall Metrics:\")\n    print(f\"  Gender  → Accuracy: {gender_acc:.2f}%  |  F1: {gender_f1:.2f}%\")\n    print(f\"  Dialect → Accuracy: {dialect_acc:.2f}%  |  F1: {dialect_f1:.2f}%\")\n    \n    # Log metrics to Wandb\n    wandb.log({\n        f'{dataset_name}/gender_accuracy': gender_acc,\n        f'{dataset_name}/gender_f1': gender_f1,\n        f'{dataset_name}/dialect_accuracy': dialect_acc,\n        f'{dataset_name}/dialect_f1': dialect_f1,\n    })\n    \n    # Save metrics to JSON as backup\n    metrics = {\n        'gender_accuracy': gender_acc,\n        'gender_f1': gender_f1,\n        'dialect_accuracy': dialect_acc,\n        'dialect_f1': dialect_f1,\n    }\n    \n    with open(f'{config.LOG_DIR}/{dataset_name}_metrics.json', 'w') as f:\n        json.dump(metrics, f, indent=2)\n    \n    # Classification reports\n    print(f\"\\nGender Classification Report:\")\n    gender_report = classification_report(gender_labels, gender_preds,\n                                         target_names=['Male', 'Female'],\n                                         digits=4)\n    print(gender_report)\n    \n    print(f\"\\nDialect Classification Report:\")\n    dialect_report = classification_report(dialect_labels, dialect_preds,\n                                          target_names=['North', 'Central', 'South'],\n                                          digits=4)\n    print(dialect_report)\n    \n    # Save reports to text files\n    with open(f'{config.LOG_DIR}/{dataset_name}_gender_report.txt', 'w') as f:\n        f.write(gender_report)\n    with open(f'{config.LOG_DIR}/{dataset_name}_dialect_report.txt', 'w') as f:\n        f.write(dialect_report)\n    \n    # Confusion matrices\n    gender_cm = confusion_matrix(gender_labels, gender_preds)\n    dialect_cm = confusion_matrix(dialect_labels, dialect_preds)\n    \n    print(f\"\\nGender Confusion Matrix:\")\n    print(gender_cm)\n    \n    print(f\"\\nDialect Confusion Matrix:\")\n    print(dialect_cm)\n    \n    # Log confusion matrices to Wandb\n    wandb.log({\n        f'{dataset_name}/gender_confusion_matrix': wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=gender_labels,\n            preds=gender_preds,\n            class_names=['Male', 'Female']\n        ),\n        f'{dataset_name}/dialect_confusion_matrix': wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=dialect_labels,\n            preds=dialect_preds,\n            class_names=['North', 'Central', 'South']\n        )\n    })\n    \n    return {\n        'gender_acc': gender_acc,\n        'gender_f1': gender_f1,\n        'dialect_acc': dialect_acc,\n        'dialect_f1': dialect_f1\n    }\n\n# Evaluate on test sets\ntest_results = evaluate_and_report(trainer, combined_test_dataset, \"combined_test\")\n\n# Also evaluate on individual datasets\nvimd_results = evaluate_and_report(trainer, vimd_test_dataset, \"vimd_test\")\nvispeech_results = evaluate_and_report(trainer, vispeech_test_dataset, \"vispeech_test\")\n\n# Create summary table for Wandb\nsummary_data = [\n    ['Combined', test_results['gender_acc'], test_results['dialect_acc']],\n    ['ViMD', vimd_results['gender_acc'], vimd_results['dialect_acc']],\n    ['ViSpeech', vispeech_results['gender_acc'], vispeech_results['dialect_acc']],\n]\nwandb.log({\n    \"test_results_summary\": wandb.Table(data=summary_data, columns=[\"Dataset\", \"Gender Acc (%)\", \"Dialect Acc (%)\"])\n})\n\nprint(f\"\\n✓ All evaluation results saved to: {config.LOG_DIR}/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f8187336","cell_type":"code","source":"# 14. SAVE MODEL\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SAVING MODEL\")\nprint(\"=\"*80)\n\n# Ensure directories exist\nos.makedirs(config.LOG_DIR, exist_ok=True)\noutput_dir = config.OUTPUT_DIR + '/best_model'\nos.makedirs(output_dir, exist_ok=True)\n\ntrainer.save_model(output_dir)\nfeature_extractor.save_pretrained(output_dir)\n\nprint(f\"  ✓ Model saved to: {output_dir}\")\n\n# Save model info\nmodel_info = {\n    'output_dir': output_dir,\n    'model_name': config.MODEL_NAME,\n    'total_params': sum(p.numel() for p in model.parameters()),\n    'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad),\n}\n\nwith open(f'{config.LOG_DIR}/model_info.json', 'w') as f:\n    json.dump(model_info, f, indent=2)\n\nprint(f\"  ✓ Model info saved to: {config.LOG_DIR}/model_info.json\")\n\n# Log model artifact to Wandb\nmodel_artifact = wandb.Artifact(\n    name=f'vietnamese-speaker-model-{wandb.run.id}',\n    type='model',\n    description='Multi-task speaker profiling model (gender + dialect)'\n)\nmodel_artifact.add_dir(output_dir)\nwandb.log_artifact(model_artifact)\n\nprint(f\"  ✓ Model logged to Wandb\")\n\n# Finish Wandb run\nwandb.finish()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"PIPELINE COMPLETED SUCCESSFULLY!\")\nprint(\"=\"*80)\nprint(f\"\\n  View results on Wandb:\")\nprint(f\"   {wandb.run.url if wandb.run else 'https://wandb.ai'}\")\nprint(f\"\\n  Logs and model also saved locally:\")\nprint(f\"   - Logs:  {config.LOG_DIR}/\")\nprint(f\"   - Model: {output_dir}/\")\nprint(\"\\n  Files created:\")\nprint(f\"   - training_config.json       (training parameters)\")\nprint(f\"   - final_train_metrics.json   (final training results)\")\nprint(f\"   - *_metrics.json             (evaluation metrics)\")\nprint(f\"   - *_report.txt               (classification reports)\")\nprint(f\"   - *_cm.npy                   (confusion matrices)\")\nprint(f\"   - model_info.json            (model information)\")\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"96ae75fc","cell_type":"code","source":"# 15. SUMMARY STATISTICS\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL SUMMARY\")\nprint(\"=\"*80)\n\nprint(f\"\\nDataset Statistics:\")\nprint(f\"  Total Training Samples:   {len(combined_train_dataset):,}\")\nprint(f\"    - ViSpeech:             {len(vispeech_train_dataset):,}\")\nprint(f\"    - ViMD:                 {len(vimd_train_dataset):,}\")\nprint(f\"  Total Test Samples:       {len(combined_test_dataset):,}\")\nprint(f\"    - ViSpeech:             {len(vispeech_test_dataset):,}\")\nprint(f\"    - ViMD:                 {len(vimd_test_dataset):,}\")\n\nprint(f\"\\nTest Results (Combined):\")\nprint(f\"  Gender Accuracy:          {test_results['gender_acc']:.2f}%\")\nprint(f\"  Dialect Accuracy:         {test_results['dialect_acc']:.2f}%\")\n\nprint(f\"\\nTest Results (ViSpeech Only):\")\nprint(f\"  Gender Accuracy:          {vispeech_results['gender_acc']:.2f}%\")\nprint(f\"  Dialect Accuracy:         {vispeech_results['dialect_acc']:.2f}%\")\n\nprint(f\"\\nTest Results (ViMD Only):\")\nprint(f\"  Gender Accuracy:          {vimd_results['gender_acc']:.2f}%\")\nprint(f\"  Dialect Accuracy:         {vimd_results['dialect_acc']:.2f}%\")\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}