{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T09:23:54.265141Z",
     "iopub.status.busy": "2025-11-25T09:23:54.264563Z",
     "iopub.status.idle": "2025-11-25T09:23:58.393417Z",
     "shell.execute_reply": "2025-11-25T09:23:58.392268Z",
     "shell.execute_reply.started": "2025-11-25T09:23:54.265116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install librosa soundfile wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T09:26:54.602308Z",
     "iopub.status.busy": "2025-11-25T09:26:54.601635Z",
     "iopub.status.idle": "2025-11-25T09:26:58.967049Z",
     "shell.execute_reply": "2025-11-25T09:26:58.966306Z",
     "shell.execute_reply.started": "2025-11-25T09:26:54.602278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install audiomentations==0.35.0\n",
    "# phần này phải tải đúng phiên bản này, nếu không sẽ dễ lỗi do từ các bản sau con audio augment này xài numpy 2.0 dễ conflict với bọn hugging face\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=\"your_wandb_api_key_here\")\n",
    "print(\"WandB logged in successfully!\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-25T09:27:02.643881Z",
     "iopub.status.busy": "2025-11-25T09:27:02.643580Z",
     "iopub.status.idle": "2025-11-25T09:27:35.114607Z",
     "shell.execute_reply": "2025-11-25T09:27:35.113763Z",
     "shell.execute_reply.started": "2025-11-25T09:27:02.643850Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    WavLMModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, Gain\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully\")\n",
    "\n",
    "# 2. CONFIGURATION\n",
    "class Config:\n",
    "    # Paths\n",
    "    VISPEECH_ROOT = Path('điền đường dẫn đúng vào đây')\n",
    "    TRAIN_META = VISPEECH_ROOT / 'metadata/trainset.csv'\n",
    "    CLEAN_TEST_META = VISPEECH_ROOT / 'metadata/clean_testset.csv'\n",
    "    NOISY_TEST_META = VISPEECH_ROOT / 'metadata/noisy_testset.csv'\n",
    "    TRAIN_AUDIO = VISPEECH_ROOT / 'trainset'\n",
    "    CLEAN_TEST_AUDIO = VISPEECH_ROOT / 'clean_testset'\n",
    "    NOISY_TEST_AUDIO = VISPEECH_ROOT / 'noisy_testset'\n",
    "    \n",
    "    # Model\n",
    "    MODEL_NAME = \"microsoft/wavlm-base-plus\"\n",
    "    \n",
    "    # Audio Processing\n",
    "    SAMPLING_RATE = 16000\n",
    "    MAX_DURATION = 5  \n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 32       \n",
    "    LEARNING_RATE = 5e-5    \n",
    "    NUM_EPOCHS = 15      \n",
    "    WARMUP_RATIO = 0.125    \n",
    "    WEIGHT_DECAY = 0.0125  \n",
    "    GRADIENT_CLIP = 1.0\n",
    "    \n",
    "    # Data Augmentation\n",
    "    AUGMENT_PROB = 0.8\n",
    "    \n",
    "    # Loss weighting\n",
    "    DIALECT_LOSS_WEIGHT = 3.0 \n",
    "    \n",
    "    # Label mappings\n",
    "    GENDER_MAP = {'Male': 0, 'Female': 1}\n",
    "    DIALECT_MAP = {'North': 0, 'Central': 1, 'South': 2}\n",
    "    \n",
    "    # Output\n",
    "    OUTPUT_DIR = '/kaggle/working/speaker-profiling'\n",
    "    \n",
    "    # WandB\n",
    "    WANDB_PROJECT = \"speaker-profiling-vispeech\"\n",
    "    \n",
    "    # Reproducibility\n",
    "    SEED = 42\n",
    "\n",
    "config = Config()\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(config.SEED)\n",
    "\n",
    "print(\"\\nCONFIGURATION\")\n",
    "print(f\"Model: {config.MODEL_NAME}\")\n",
    "print(f\"Architecture: WavLM + Attentive Pooling + LayerNorm\")\n",
    "print(f\"Sampling Rate: {config.SAMPLING_RATE} Hz\")\n",
    "print(f\"Max Duration: {config.MAX_DURATION}s\")\n",
    "print(f\"Batch Size: {config.BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {config.LEARNING_RATE}\")\n",
    "print(f\"Epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"Weight Decay: {config.WEIGHT_DECAY}\")\n",
    "print(f\"Augmentation Prob: {config.AUGMENT_PROB}\")\n",
    "print(f\"Dialect Loss Weight: {config.DIALECT_LOSS_WEIGHT}x\")\n",
    "print(f\"WandB Project: {config.WANDB_PROJECT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T09:27:41.763954Z",
     "iopub.status.busy": "2025-11-25T09:27:41.762993Z",
     "iopub.status.idle": "2025-11-25T09:27:41.842459Z",
     "shell.execute_reply": "2025-11-25T09:27:41.841597Z",
     "shell.execute_reply.started": "2025-11-25T09:27:41.763927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3. DATA LOADING & PREPARATION\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    print(\"\\nLoading metadata...\")\n",
    "    train_df = pd.read_csv(config.TRAIN_META)\n",
    "    clean_test_df = pd.read_csv(config.CLEAN_TEST_META)\n",
    "    noisy_test_df = pd.read_csv(config.NOISY_TEST_META)\n",
    "    \n",
    "    for df in [train_df, clean_test_df, noisy_test_df]:\n",
    "        df['gender_label'] = df['gender'].map(config.GENDER_MAP)\n",
    "        df['dialect_label'] = df['dialect'].map(config.DIALECT_MAP)\n",
    "    \n",
    "    unique_speakers = train_df['speaker'].unique()\n",
    "    train_speakers, val_speakers = train_test_split(\n",
    "        unique_speakers,\n",
    "        test_size=0.15,\n",
    "        random_state=config.SEED,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    train_data = train_df[train_df['speaker'].isin(train_speakers)].reset_index(drop=True)\n",
    "    val_data = train_df[train_df['speaker'].isin(val_speakers)].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nData loaded:\")\n",
    "    print(f\"Train: {len(train_data):,} samples ({len(train_speakers)} speakers)\")\n",
    "    print(f\"Validation: {len(val_data):,} samples ({len(val_speakers)} speakers)\")\n",
    "    print(f\"Clean Test: {len(clean_test_df):,} samples\")\n",
    "    print(f\"Noisy Test: {len(noisy_test_df):,} samples\")\n",
    "    \n",
    "    assert len(set(train_speakers) & set(val_speakers)) == 0, \"Speaker leakage detected!\"\n",
    "    print(\"No speaker leakage between train/val\")\n",
    "    \n",
    "    return train_data, val_data, clean_test_df, noisy_test_df\n",
    "\n",
    "train_df, val_df, clean_test_df, noisy_test_df = load_and_prepare_data()\n",
    "\n",
    "\n",
    "# 4. DATA AUGMENTATION\n",
    "\n",
    "class AudioAugmentation:\n",
    "    def __init__(self, sampling_rate=16000, augment_prob=0.8):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.augment_prob = augment_prob\n",
    "        \n",
    "        self.augment = Compose([\n",
    "            AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "            TimeStretch(min_rate=0.8, max_rate=1.2, leave_length_unchanged=False, p=0.5),\n",
    "            PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "            Shift(min_shift=-0.5, max_shift=0.5, p=0.3),\n",
    "            Gain(min_gain_db=-12, max_gain_db=12, p=0.5),\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, audio):\n",
    "        if random.random() < self.augment_prob:\n",
    "            return self.augment(samples=audio, sample_rate=self.sampling_rate)\n",
    "        return audio\n",
    "\n",
    "\n",
    "# 5. DATASET CLASS\n",
    "\n",
    "class ViSpeechDataset(Dataset):\n",
    "    def __init__(self, dataframe, audio_dir, feature_extractor,\n",
    "                 sampling_rate=16000, max_duration=5, is_training=True):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.audio_dir = Path(audio_dir)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.max_length = int(sampling_rate * max_duration)\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        if is_training:\n",
    "            self.augmentation = AudioAugmentation(sampling_rate, augment_prob=config.AUGMENT_PROB)\n",
    "            print(f\"Augmentation ENABLED (prob={config.AUGMENT_PROB})\")\n",
    "        else:\n",
    "            self.augmentation = None\n",
    "            print(f\"Augmentation DISABLED\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def load_audio(self, audio_name):\n",
    "        audio_path = self.audio_dir / audio_name\n",
    "        \n",
    "        try:\n",
    "            audio, sr = librosa.load(audio_path, sr=self.sampling_rate, mono=True)\n",
    "            audio, _ = librosa.effects.trim(audio, top_db=20)\n",
    "            \n",
    "            if self.is_training and self.augmentation is not None:\n",
    "                audio = self.augmentation(audio)\n",
    "            \n",
    "            audio = audio / (np.max(np.abs(audio)) + 1e-8)\n",
    "            \n",
    "            if len(audio) < self.max_length:\n",
    "                audio = np.pad(audio, (0, self.max_length - len(audio)))\n",
    "            else:\n",
    "                if self.is_training:\n",
    "                    start = np.random.randint(0, len(audio) - self.max_length + 1)\n",
    "                else:\n",
    "                    start = (len(audio) - self.max_length) // 2\n",
    "                audio = audio[start:start + self.max_length]\n",
    "            \n",
    "            return audio\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {audio_path}: {e}\")\n",
    "            return np.zeros(self.max_length)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        audio = self.load_audio(row['audio_name'])\n",
    "        \n",
    "        inputs = self.feature_extractor(\n",
    "            audio,\n",
    "            sampling_rate=self.sampling_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_values': inputs.input_values.squeeze(0),\n",
    "            'gender_labels': torch.tensor(row['gender_label'], dtype=torch.long),\n",
    "            'dialect_labels': torch.tensor(row['dialect_label'], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T09:27:43.732758Z",
     "iopub.status.busy": "2025-11-25T09:27:43.732256Z",
     "iopub.status.idle": "2025-11-25T09:27:48.521686Z",
     "shell.execute_reply": "2025-11-25T09:27:48.520850Z",
     "shell.execute_reply.started": "2025-11-25T09:27:43.732733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 6. ATTENTIVE POOLING MODULE\n",
    "\n",
    "class AttentivePooling(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_weights = self.attention(x)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(-1)\n",
    "            attn_weights = attn_weights.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(attn_weights, dim=1)\n",
    "        pooled = torch.sum(x * attn_weights, dim=1)\n",
    "        \n",
    "        return pooled, attn_weights.squeeze(-1)\n",
    "\n",
    "\n",
    "# 7. MULTI-TASK MODEL\n",
    "\n",
    "class MultiTaskSpeakerModel(nn.Module):\n",
    "    def __init__(self, model_name, num_genders=2, num_dialects=3,\n",
    "                 freeze_encoder=False, dropout=0.1, head_hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.wavlm = WavLMModel.from_pretrained(model_name)\n",
    "        \n",
    "        if freeze_encoder:\n",
    "            for param in self.wavlm.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"Encoder FROZEN\")\n",
    "        \n",
    "        hidden_size = self.wavlm.config.hidden_size\n",
    "        \n",
    "        self.attentive_pooling = AttentivePooling(hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.gender_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, head_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden_dim, num_genders)\n",
    "        )\n",
    "        \n",
    "        self.dialect_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, head_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden_dim, head_hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(head_hidden_dim // 2, num_dialects)\n",
    "        )\n",
    "        \n",
    "        print(f\"Architecture: WavLM + Attentive Pooling + LayerNorm\")\n",
    "        print(f\"Hidden size: {hidden_size}\")\n",
    "        print(f\"Head hidden dim: {head_hidden_dim}\")\n",
    "        print(f\"Dropout: {dropout}\")\n",
    "        \n",
    "    def forward(self, input_values, attention_mask=None, \n",
    "                gender_labels=None, dialect_labels=None):\n",
    "        \n",
    "        outputs = self.wavlm(input_values, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        pooled, attn_weights = self.attentive_pooling(hidden_states, attention_mask)\n",
    "        pooled = self.layer_norm(pooled)\n",
    "        pooled = self.dropout(pooled)\n",
    "        \n",
    "        gender_logits = self.gender_head(pooled)\n",
    "        dialect_logits = self.dialect_head(pooled)\n",
    "        \n",
    "        loss = None\n",
    "        if gender_labels is not None and dialect_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            gender_loss = loss_fct(gender_logits, gender_labels)\n",
    "            dialect_loss = loss_fct(dialect_logits, dialect_labels)\n",
    "            loss = gender_loss + config.DIALECT_LOSS_WEIGHT * dialect_loss\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'gender_logits': gender_logits,\n",
    "            'dialect_logits': dialect_logits,\n",
    "            'attention_weights': attn_weights\n",
    "        }\n",
    "\n",
    "\n",
    "# 8. CUSTOM TRAINER\n",
    "\n",
    "class MultiTaskTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        gender_labels = inputs.pop(\"gender_labels\")\n",
    "        dialect_labels = inputs.pop(\"dialect_labels\")\n",
    "        \n",
    "        outputs = model(\n",
    "            input_values=inputs[\"input_values\"],\n",
    "            gender_labels=gender_labels,\n",
    "            dialect_labels=dialect_labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs[\"loss\"]\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        gender_labels = inputs.pop(\"gender_labels\")\n",
    "        dialect_labels = inputs.pop(\"dialect_labels\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_values=inputs[\"input_values\"],\n",
    "                gender_labels=gender_labels,\n",
    "                dialect_labels=dialect_labels\n",
    "            )\n",
    "            loss = outputs[\"loss\"]\n",
    "        \n",
    "        return (\n",
    "            loss,\n",
    "            (outputs[\"gender_logits\"], outputs[\"dialect_logits\"]),\n",
    "            (gender_labels, dialect_labels)\n",
    "        )\n",
    "\n",
    "\n",
    "# 9. METRICS\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    gender_logits, dialect_logits = pred.predictions\n",
    "    gender_labels, dialect_labels = pred.label_ids\n",
    "    \n",
    "    gender_preds = np.argmax(gender_logits, axis=-1)\n",
    "    dialect_preds = np.argmax(dialect_logits, axis=-1)\n",
    "    \n",
    "    gender_acc = accuracy_score(gender_labels, gender_preds)\n",
    "    gender_f1 = f1_score(gender_labels, gender_preds, average='weighted')\n",
    "    dialect_acc = accuracy_score(dialect_labels, dialect_preds)\n",
    "    dialect_f1 = f1_score(dialect_labels, dialect_preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'gender_acc': gender_acc,\n",
    "        'gender_f1': gender_f1,\n",
    "        'dialect_acc': dialect_acc,\n",
    "        'dialect_f1': dialect_f1,\n",
    "        'combined_f1': (gender_f1 + dialect_f1) / 2\n",
    "    }\n",
    "\n",
    "\n",
    "# 10. INITIALIZE MODEL & DATASETS\n",
    "print(\"INITIALIZING MODEL\")\n",
    "\n",
    "print(\"\\nLoading feature extractor...\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(config.MODEL_NAME)\n",
    "print(\"Feature extractor loaded\")\n",
    "\n",
    "print(\"\\nCreating datasets...\")\n",
    "print(\"Training dataset:\")\n",
    "train_dataset = ViSpeechDataset(\n",
    "    train_df, config.TRAIN_AUDIO, feature_extractor,\n",
    "    sampling_rate=config.SAMPLING_RATE,\n",
    "    max_duration=config.MAX_DURATION,\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "print(\"Validation dataset:\")\n",
    "val_dataset = ViSpeechDataset(\n",
    "    val_df, config.TRAIN_AUDIO, feature_extractor,\n",
    "    sampling_rate=config.SAMPLING_RATE,\n",
    "    max_duration=config.MAX_DURATION,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "print(\"Clean test dataset:\")\n",
    "clean_test_dataset = ViSpeechDataset(\n",
    "    clean_test_df, config.CLEAN_TEST_AUDIO, feature_extractor,\n",
    "    sampling_rate=config.SAMPLING_RATE,\n",
    "    max_duration=config.MAX_DURATION,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "print(\"Noisy test dataset:\")\n",
    "noisy_test_dataset = ViSpeechDataset(\n",
    "    noisy_test_df, config.NOISY_TEST_AUDIO, feature_extractor,\n",
    "    sampling_rate=config.SAMPLING_RATE,\n",
    "    max_duration=config.MAX_DURATION,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "print(\"\\nLoading model...\")\n",
    "model = MultiTaskSpeakerModel(\n",
    "    config.MODEL_NAME,\n",
    "    num_genders=2,\n",
    "    num_dialects=3,\n",
    "    freeze_encoder=False,\n",
    "    dropout=0.1,\n",
    "    head_hidden_dim=256\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "new_params = trainable_params - sum(p.numel() for p in model.wavlm.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel loaded: {config.MODEL_NAME}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"New parameters added: {new_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T09:27:49.845302Z",
     "iopub.status.busy": "2025-11-25T09:27:49.844778Z",
     "iopub.status.idle": "2025-11-25T09:27:49.874097Z",
     "shell.execute_reply": "2025-11-25T09:27:49.873372Z",
     "shell.execute_reply.started": "2025-11-25T09:27:49.845276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 11. INITIALIZE WANDB\n",
    "print(\"\\nINITIALIZING WANDB\")\n",
    "\n",
    "wandb.init(\n",
    "    project=config.WANDB_PROJECT,\n",
    "    name=\"wavlm-attentive-layernorm\",\n",
    "    config={\n",
    "        \"model_name\": config.MODEL_NAME,\n",
    "        \"architecture\": \"WavLM + Attentive Pooling + LayerNorm\",\n",
    "        \"sampling_rate\": config.SAMPLING_RATE,\n",
    "        \"max_duration\": config.MAX_DURATION,\n",
    "        \"batch_size\": config.BATCH_SIZE,\n",
    "        \"learning_rate\": config.LEARNING_RATE,\n",
    "        \"num_epochs\": config.NUM_EPOCHS,\n",
    "        \"warmup_ratio\": config.WARMUP_RATIO,\n",
    "        \"weight_decay\": config.WEIGHT_DECAY,\n",
    "        \"gradient_clip\": config.GRADIENT_CLIP,\n",
    "        \"augment_prob\": config.AUGMENT_PROB,\n",
    "        \"dialect_loss_weight\": config.DIALECT_LOSS_WEIGHT,\n",
    "        \"head_hidden_dim\": 256,\n",
    "        \"train_samples\": len(train_dataset),\n",
    "        \"val_samples\": len(val_dataset),\n",
    "        \"clean_test_samples\": len(clean_test_dataset),\n",
    "        \"noisy_test_samples\": len(noisy_test_dataset),\n",
    "    },\n",
    "    tags=[\"speaker-profiling\", \"wavlm\", \"multi-task\", \"vietnamese\"]\n",
    ")\n",
    "\n",
    "print(f\"WandB initialized: {wandb.run.url}\")\n",
    "\n",
    "\n",
    "# 12. TRAINING ARGUMENTS\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.OUTPUT_DIR,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    \n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    per_device_train_batch_size=config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "    num_train_epochs=config.NUM_EPOCHS,\n",
    "    weight_decay=config.WEIGHT_DECAY,\n",
    "    warmup_ratio=config.WARMUP_RATIO,\n",
    "    max_grad_norm=config.GRADIENT_CLIP,\n",
    "    \n",
    "    lr_scheduler_type='linear',\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='dialect_acc',\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    \n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    report_to='wandb',\n",
    "    \n",
    "    remove_unused_columns=False,\n",
    "    seed=config.SEED,\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.0025\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T09:27:55.195507Z",
     "iopub.status.busy": "2025-11-25T09:27:55.195227Z",
     "iopub.status.idle": "2025-11-25T09:52:16.934815Z",
     "shell.execute_reply": "2025-11-25T09:52:16.934068Z",
     "shell.execute_reply.started": "2025-11-25T09:27:55.195485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 13. TRAINER & TRAINING\n",
    "print(\"\\nTRAINING\")\n",
    "\n",
    "trainer = MultiTaskTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(f\"Architecture: WavLM + Attentive Pooling + LayerNorm\")\n",
    "print(f\"Epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"Steps per epoch: ~{len(train_dataset) // config.BATCH_SIZE}\")\n",
    "print(f\"Total steps: ~{(len(train_dataset) // config.BATCH_SIZE) * config.NUM_EPOCHS}\")\n",
    "print(f\"WandB tracking: {wandb.run.url}\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T09:56:38.226575Z",
     "iopub.status.busy": "2025-11-25T09:56:38.226207Z",
     "iopub.status.idle": "2025-11-25T09:57:33.182013Z",
     "shell.execute_reply": "2025-11-25T09:57:33.181292Z",
     "shell.execute_reply.started": "2025-11-25T09:56:38.226541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 14. EVALUATION\n",
    "def evaluate_and_report(trainer, dataset, dataset_name):\n",
    "    print(f\"\\nEVALUATING ON {dataset_name.upper()}\")\n",
    "    \n",
    "    results = trainer.predict(dataset)\n",
    "    gender_logits, dialect_logits = results.predictions\n",
    "    gender_labels, dialect_labels = results.label_ids\n",
    "    \n",
    "    gender_preds = np.argmax(gender_logits, axis=-1)\n",
    "    dialect_preds = np.argmax(dialect_logits, axis=-1)\n",
    "    \n",
    "    gender_acc = accuracy_score(gender_labels, gender_preds) * 100\n",
    "    dialect_acc = accuracy_score(dialect_labels, dialect_preds) * 100\n",
    "    gender_f1 = f1_score(gender_labels, gender_preds, average='weighted') * 100\n",
    "    dialect_f1 = f1_score(dialect_labels, dialect_preds, average='weighted') * 100\n",
    "    \n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"Gender -> Accuracy: {gender_acc:.2f}% | F1: {gender_f1:.2f}%\")\n",
    "    print(f\"Dialect -> Accuracy: {dialect_acc:.2f}% | F1: {dialect_f1:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nGender Classification Report:\")\n",
    "    print(classification_report(gender_labels, gender_preds,\n",
    "                               target_names=['Male', 'Female'],\n",
    "                               digits=4))\n",
    "    \n",
    "    print(f\"\\nDialect Classification Report:\")\n",
    "    print(classification_report(dialect_labels, dialect_preds,\n",
    "                               target_names=['North', 'Central', 'South'],\n",
    "                               digits=4))\n",
    "    \n",
    "    print(f\"\\nGender Confusion Matrix:\")\n",
    "    gender_cm = confusion_matrix(gender_labels, gender_preds)\n",
    "    print(gender_cm)\n",
    "    \n",
    "    print(f\"\\nDialect Confusion Matrix:\")\n",
    "    dialect_cm = confusion_matrix(dialect_labels, dialect_preds)\n",
    "    print(dialect_cm)\n",
    "    \n",
    "    wandb.log({\n",
    "        f\"{dataset_name}/gender_acc\": gender_acc,\n",
    "        f\"{dataset_name}/gender_f1\": gender_f1,\n",
    "        f\"{dataset_name}/dialect_acc\": dialect_acc,\n",
    "        f\"{dataset_name}/dialect_f1\": dialect_f1,\n",
    "        f\"{dataset_name}/combined_f1\": (gender_f1 + dialect_f1) / 2,\n",
    "    })\n",
    "    \n",
    "    wandb.log({\n",
    "        f\"{dataset_name}/gender_confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "            probs=None,\n",
    "            y_true=gender_labels,\n",
    "            preds=gender_preds,\n",
    "            class_names=['Male', 'Female']\n",
    "        ),\n",
    "        f\"{dataset_name}/dialect_confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "            probs=None,\n",
    "            y_true=dialect_labels,\n",
    "            preds=dialect_preds,\n",
    "            class_names=['North', 'Central', 'South']\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        'gender_acc': gender_acc,\n",
    "        'gender_f1': gender_f1,\n",
    "        'dialect_acc': dialect_acc,\n",
    "        'dialect_f1': dialect_f1\n",
    "    }\n",
    "\n",
    "clean_results = evaluate_and_report(trainer, clean_test_dataset, \"clean_test\")\n",
    "noisy_results = evaluate_and_report(trainer, noisy_test_dataset, \"noisy_test\")\n",
    "\n",
    "\n",
    "# 15. COMPARISON WITH BASELINE\n",
    "baseline_results = {\n",
    "    'gender': {'clean': 98.73, 'noisy': 98.14},\n",
    "    'dialect': {'clean': 81.47, 'noisy': 74.80}\n",
    "}\n",
    "\n",
    "our_results = {\n",
    "    'gender': {'clean': clean_results['gender_acc'], 'noisy': noisy_results['gender_acc']},\n",
    "    'dialect': {'clean': clean_results['dialect_acc'], 'noisy': noisy_results['dialect_acc']}\n",
    "}\n",
    "\n",
    "print(\"\\nCOMPARISON WITH BASELINE (PACLIC 2024 - ResNet34)\")\n",
    "print(f\"\\n{'Task':<15} {'Test Set':<12} {'Baseline':<12} {'Our Model':<12} {'Delta':<12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "comparison_data = []\n",
    "for task in ['gender', 'dialect']:\n",
    "    for test_set in ['clean', 'noisy']:\n",
    "        baseline_val = baseline_results[task][test_set]\n",
    "        our_val = our_results[task][test_set]\n",
    "        delta = our_val - baseline_val\n",
    "        delta_str = f\"{delta:+.2f}%\"\n",
    "        \n",
    "        print(f\"{task.capitalize():<15} {test_set.capitalize():<12} \"\n",
    "              f\"{baseline_val:<12.2f} {our_val:<12.2f} {delta_str:<12}\")\n",
    "        \n",
    "        comparison_data.append([task, test_set, baseline_val, our_val, delta])\n",
    "\n",
    "wandb.log({\n",
    "    \"baseline_comparison\": wandb.Table(\n",
    "        data=comparison_data,\n",
    "        columns=[\"Task\", \"Test Set\", \"Baseline\", \"Our Model\", \"Delta\"]\n",
    "    )\n",
    "})\n",
    "\n",
    "print(\"\\nSUMMARY:\")\n",
    "avg_improvement = sum([d[4] for d in comparison_data]) / len(comparison_data)\n",
    "print(f\"Average Improvement: {avg_improvement:+.2f}%\")\n",
    "\n",
    "wandb.log({\n",
    "    \"summary/avg_improvement\": avg_improvement,\n",
    "    \"summary/clean_gender_delta\": our_results['gender']['clean'] - baseline_results['gender']['clean'],\n",
    "    \"summary/clean_dialect_delta\": our_results['dialect']['clean'] - baseline_results['dialect']['clean'],\n",
    "    \"summary/noisy_gender_delta\": our_results['gender']['noisy'] - baseline_results['gender']['noisy'],\n",
    "    \"summary/noisy_dialect_delta\": our_results['dialect']['noisy'] - baseline_results['dialect']['noisy'],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T09:57:41.312530Z",
     "iopub.status.busy": "2025-11-25T09:57:41.311731Z",
     "iopub.status.idle": "2025-11-25T09:57:41.984534Z",
     "shell.execute_reply": "2025-11-25T09:57:41.983715Z",
     "shell.execute_reply.started": "2025-11-25T09:57:41.312499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 16. SAVE MODEL\n",
    "print(\"\\nSAVING MODEL\")\n",
    "\n",
    "output_dir = config.OUTPUT_DIR + '/best_model'\n",
    "trainer.save_model(output_dir)\n",
    "feature_extractor.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to: {output_dir}\")\n",
    "\n",
    "artifact = wandb.Artifact(\n",
    "    name=\"speaker-profiling-model\",\n",
    "    type=\"model\",\n",
    "    description=\"WavLM + Attentive Pooling + LayerNorm for Speaker Profiling\"\n",
    ")\n",
    "artifact.add_dir(output_dir)\n",
    "wandb.log_artifact(artifact)\n",
    "\n",
    "print(f\"Model artifact logged to WandB\")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\nPIPELINE COMPLETED\")\n",
    "print(f\"Architecture: WavLM + Attentive Pooling + LayerNorm\")\n",
    "print(\"Check WandB dashboard for detailed metrics!\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8828507,
     "sourceId": 13858485,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
