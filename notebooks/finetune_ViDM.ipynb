{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41358df9",
   "metadata": {},
   "source": [
    "Tôi sẽ sử dụng file này, sau đó import lên Kaggle và chạy để sử dụng GPU và Ram của kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/VuThanhLam124/Profiling_gender_dialect.git\n",
    "!apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075fad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Profiling_gender_dialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648edfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install -q transformers==4.44.0 accelerate==0.33.0 datasets==2.21.0\n",
    "!pip install -q librosa soundfile audiomentations==0.35.0 wandb safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f545b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINETUNE WITH ViMD DATASET\n",
    "# ============================================================\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"training\")\n",
    "\n",
    "ENCODER = \"vinai/PhoWhisper-base\"\n",
    "encoder_short = ENCODER.split(\"/\")[-1]\n",
    "WANDB_API_KEY = \"f05e29c3466ec288e97041e0e3d541c4087096a6\"\n",
    "\n",
    "vimd_config = f\"\"\"\n",
    "model:\n",
    "  name: \"{ENCODER}\"\n",
    "  num_genders: 2\n",
    "  num_dialects: 3\n",
    "  dropout: 0.25\n",
    "  head_hidden_dim: 512\n",
    "  freeze_encoder: false\n",
    "\n",
    "training:\n",
    "  batch_size: 32\n",
    "  gradient_accumulation_steps: 4\n",
    "  learning_rate: 2.5e-5\n",
    "  num_epochs: 30\n",
    "  warmup_ratio: 0.15\n",
    "  weight_decay: 0.015\n",
    "  gradient_clip: 0.5\n",
    "  lr_scheduler: \"cosine\"\n",
    "  fp16: true\n",
    "  dataloader_num_workers: 2\n",
    "\n",
    "loss:\n",
    "  dialect_weight: 3\n",
    "\n",
    "wandb:\n",
    "  enabled: true\n",
    "  api_key: \"{WANDB_API_KEY}\"\n",
    "  project: \"vimd-speaker-profiling\"\n",
    "  run_name: \"{encoder_short}\"\n",
    "\n",
    "data:\n",
    "  source: \"vimd\"\n",
    "  vimd_path: \"/kaggle/input/vimd-dataset\"\n",
    "\n",
    "audio:\n",
    "  sampling_rate: 16000\n",
    "  max_duration: 5\n",
    "\n",
    "augmentation:\n",
    "  enabled: true\n",
    "  prob: 0.75\n",
    "\n",
    "output:\n",
    "  dir: \"/kaggle/working/output_vimd\"\n",
    "  save_total_limit: 1\n",
    "  metric_for_best_model: \"dialect_acc\"\n",
    "\n",
    "early_stopping:\n",
    "  patience: 5\n",
    "  threshold: 0.001\n",
    "\n",
    "labels:\n",
    "  gender:\n",
    "    Male: 0\n",
    "    Female: 1\n",
    "    0: 0\n",
    "    1: 1\n",
    "  dialect:\n",
    "    North: 0\n",
    "    Central: 1\n",
    "    South: 2\n",
    "\n",
    "seed: 42\n",
    "\"\"\"\n",
    "\n",
    "config_path = \"configs/vimd_train.yaml\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    f.write(vimd_config)\n",
    "\n",
    "logger.info(f\"Config saved: {config_path}\")\n",
    "logger.info(f\"Encoder: {ENCODER}\")\n",
    "logger.info(f\"Batch size: 32, Gradient accumulation: 4 (effective batch: 32x4)\")\n",
    "logger.info(f\"WandB: enabled, project=vimd-speaker-profiling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# START TRAINING\n",
    "# ============================================================\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(f\"TRAINING: {ENCODER}\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "exit_code = os.system(f\"python finetune.py --config {config_path}\")\n",
    "\n",
    "if exit_code == 0:\n",
    "    logger.info(\"Training completed successfully\")\n",
    "else:\n",
    "    logger.error(f\"Training failed with exit code: {exit_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98bdb3d",
   "metadata": {},
   "source": [
    "Eval with ViDM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada7e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CHECK SAVED MODEL\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "model_dir = \"/kaggle/working/output_vimd/best_model\"\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"SAVED MODEL\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "if os.path.exists(model_dir):\n",
    "    total_size = 0\n",
    "    for f in sorted(os.listdir(model_dir)):\n",
    "        size = os.path.getsize(os.path.join(model_dir, f)) / 1024 / 1024\n",
    "        total_size += size\n",
    "        logger.info(f\"  {f}: {size:.1f} MB\")\n",
    "    logger.info(f\"  Total: {total_size:.1f} MB\")\n",
    "else:\n",
    "    logger.warning(\"Model not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64285c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE MODEL ON ViMD TEST SET\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "model_dir = \"/kaggle/working/output_vimd/best_model\"\n",
    "config_path = \"configs/vimd_train.yaml\"\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    logger.error(\"Model not found, skipping eval\")\n",
    "else:\n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(\"EVALUATING ON ViMD TEST SET\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    \n",
    "    exit_code = os.system(\n",
    "        f\"python eval.py --checkpoint {model_dir} --config {config_path} \"\n",
    "        f\"--test_name vimd_test --output_dir /kaggle/working/output_vimd/eval\"\n",
    "    )\n",
    "    \n",
    "    if exit_code == 0:\n",
    "        logger.info(\"Evaluation completed successfully\")\n",
    "    else:\n",
    "        logger.error(f\"Evaluation failed with exit code: {exit_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d4fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE MODEL TO KAGGLE OUTPUT\n",
    "# ============================================================\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/final_model_vimd\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "model_dir = \"/kaggle/working/output_vimd/best_model\"\n",
    "eval_dir = \"/kaggle/working/output_vimd/eval\"\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"COPYING MODEL TO OUTPUT\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "if os.path.exists(model_dir):\n",
    "    dst_dir = f\"{OUTPUT_DIR}/best_model\"\n",
    "    if os.path.exists(dst_dir):\n",
    "        shutil.rmtree(dst_dir)\n",
    "    shutil.copytree(model_dir, dst_dir)\n",
    "    logger.info(f\"Copied model to: {dst_dir}\")\n",
    "else:\n",
    "    logger.warning(\"Model not found\")\n",
    "\n",
    "if os.path.exists(eval_dir):\n",
    "    dst_eval = f\"{OUTPUT_DIR}/eval\"\n",
    "    if os.path.exists(dst_eval):\n",
    "        shutil.rmtree(dst_eval)\n",
    "    shutil.copytree(eval_dir, dst_eval)\n",
    "    logger.info(f\"Copied eval to: {dst_eval}\")\n",
    "\n",
    "logger.info(f\"All files saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05baade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LIST FINAL OUTPUT\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/final_model_vimd\"\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"FINAL OUTPUT STRUCTURE\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "        level = root.replace(OUTPUT_DIR, '').count(os.sep)\n",
    "        indent = '  ' * level\n",
    "        logger.info(f\"{indent}{os.path.basename(root)}/\")\n",
    "        sub_indent = '  ' * (level + 1)\n",
    "        for file in files:\n",
    "            size = os.path.getsize(os.path.join(root, file)) / 1024 / 1024\n",
    "            logger.info(f\"{sub_indent}{file} ({size:.1f} MB)\")\n",
    "else:\n",
    "    logger.warning(\"Output directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aacf2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESUME TRAINING (TRAIN THÊM)\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "# Checkpoint để resume\n",
    "CHECKPOINT_DIR = \"/kaggle/working/output_vimd/best_model\"\n",
    "\n",
    "# Số epoch train thêm\n",
    "ADDITIONAL_EPOCHS = 10\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"RESUME TRAINING\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    logger.error(f\"Checkpoint not found: {CHECKPOINT_DIR}\")\n",
    "else:\n",
    "    # Tạo config mới với resume\n",
    "    resume_config = f\"\"\"\n",
    "model:\n",
    "  name: \"{ENCODER}\"\n",
    "  num_genders: 2\n",
    "  num_dialects: 3\n",
    "  dropout: 0.25\n",
    "  head_hidden_dim: 512\n",
    "  freeze_encoder: false\n",
    "\n",
    "training:\n",
    "  batch_size: 32\n",
    "  gradient_accumulation_steps: 4\n",
    "  learning_rate: 1e-5  # Learning rate thấp hơn khi resume\n",
    "  num_epochs: {ADDITIONAL_EPOCHS}\n",
    "  warmup_ratio: 0.05  # Warmup ngắn hơn\n",
    "  weight_decay: 0.015\n",
    "  gradient_clip: 0.5\n",
    "  lr_scheduler: \"cosine\"\n",
    "  fp16: true\n",
    "  dataloader_num_workers: 2\n",
    "  resume_from_checkpoint: \"{CHECKPOINT_DIR}\"  # Resume từ checkpoint\n",
    "\n",
    "loss:\n",
    "  dialect_weight: 3\n",
    "\n",
    "wandb:\n",
    "  enabled: true\n",
    "  api_key: \"{WANDB_API_KEY}\"\n",
    "  project: \"vimd-speaker-profiling\"\n",
    "  run_name: \"{encoder_short}-resume\"\n",
    "\n",
    "data:\n",
    "  source: \"vimd\"\n",
    "  vimd_path: \"/kaggle/input/vimd-dataset\"\n",
    "\n",
    "audio:\n",
    "  sampling_rate: 16000\n",
    "  max_duration: 5\n",
    "\n",
    "augmentation:\n",
    "  enabled: true\n",
    "  prob: 0.75\n",
    "\n",
    "output:\n",
    "  dir: \"/kaggle/working/output_vimd_resume\"\n",
    "  save_total_limit: 1\n",
    "  metric_for_best_model: \"dialect_acc\"\n",
    "\n",
    "early_stopping:\n",
    "  patience: 5\n",
    "  threshold: 0.001\n",
    "\n",
    "labels:\n",
    "  gender:\n",
    "    Male: 0\n",
    "    Female: 1\n",
    "    0: 0\n",
    "    1: 1\n",
    "  dialect:\n",
    "    North: 0\n",
    "    Central: 1\n",
    "    South: 2\n",
    "\n",
    "seed: 42\n",
    "\"\"\"\n",
    "    \n",
    "    resume_config_path = \"configs/vimd_resume.yaml\"\n",
    "    with open(resume_config_path, \"w\") as f:\n",
    "        f.write(resume_config)\n",
    "    \n",
    "    logger.info(f\"Resume config saved: {resume_config_path}\")\n",
    "    logger.info(f\"Resume from: {CHECKPOINT_DIR}\")\n",
    "    logger.info(f\"Additional epochs: {ADDITIONAL_EPOCHS}\")\n",
    "    logger.info(f\"Learning rate: 1e-5 (reduced for fine-tuning)\")\n",
    "    \n",
    "    # Start resume training\n",
    "    exit_code = os.system(f\"python finetune.py --config {resume_config_path}\")\n",
    "    \n",
    "    if exit_code == 0:\n",
    "        logger.info(\"Resume training completed successfully!\")\n",
    "        # Copy new best model\n",
    "        new_model_dir = \"/kaggle/working/output_vimd_resume/best_model\"\n",
    "        if os.path.exists(new_model_dir):\n",
    "            import shutil\n",
    "            dst = \"/kaggle/working/final_model_vimd/best_model_resumed\"\n",
    "            if os.path.exists(dst):\n",
    "                shutil.rmtree(dst)\n",
    "            shutil.copytree(new_model_dir, dst)\n",
    "            logger.info(f\"Resumed model saved to: {dst}\")\n",
    "    else:\n",
    "        logger.error(f\"Resume training failed with exit code: {exit_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bebc654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE RESUMED MODEL\n",
    "# ============================================================\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Model sau khi train thêm\n",
    "resumed_model_dir = \"/kaggle/working/output_vimd_resume/best_model\"\n",
    "config_path = \"configs/vimd_resume.yaml\"\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"EVALUATING RESUMED MODEL ON ViMD TEST SET\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "if not os.path.exists(resumed_model_dir):\n",
    "    logger.error(f\"Resumed model not found: {resumed_model_dir}\")\n",
    "else:\n",
    "    eval_output_dir = \"/kaggle/working/output_vimd_resume/eval\"\n",
    "    \n",
    "    exit_code = os.system(\n",
    "        f\"python eval.py --checkpoint {resumed_model_dir} --config {config_path} \"\n",
    "        f\"--test_name vimd_test --output_dir {eval_output_dir}\"\n",
    "    )\n",
    "    \n",
    "    if exit_code == 0:\n",
    "        logger.info(\"Evaluation completed successfully!\")\n",
    "        \n",
    "        # Copy eval results to final output\n",
    "        dst_eval = \"/kaggle/working/final_model_vimd/eval_resumed\"\n",
    "        if os.path.exists(dst_eval):\n",
    "            shutil.rmtree(dst_eval)\n",
    "        shutil.copytree(eval_output_dir, dst_eval)\n",
    "        logger.info(f\"Eval results saved to: {dst_eval}\")\n",
    "        \n",
    "        # Print evaluation results\n",
    "        eval_file = os.path.join(eval_output_dir, \"evaluation_results.json\")\n",
    "        if os.path.exists(eval_file):\n",
    "            import json\n",
    "            with open(eval_file, \"r\") as f:\n",
    "                results = json.load(f)\n",
    "            logger.info(\"=\" * 50)\n",
    "            logger.info(\"RESULTS:\")\n",
    "            logger.info(f\"  Gender Accuracy: {results.get('gender_accuracy', 'N/A'):.4f}\")\n",
    "            logger.info(f\"  Dialect Accuracy: {results.get('dialect_accuracy', 'N/A'):.4f}\")\n",
    "            logger.info(\"=\" * 50)\n",
    "    else:\n",
    "        logger.error(f\"Evaluation failed with exit code: {exit_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD AND PREPARE LSVSC DATASET\n",
    "# ============================================================\n",
    "import json\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"LOADING LSVSC DATASET\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(\"doof-ferb/LSVSC\")\n",
    "logger.info(f\"Dataset splits: {ds.keys()}\")\n",
    "logger.info(f\"Dataset info: {ds}\")\n",
    "\n",
    "# Use test split or validation\n",
    "test_split = ds.get(\"test\", ds.get(\"validation\", None))\n",
    "\n",
    "if test_split is None:\n",
    "    logger.error(\"No test or validation split found\")\n",
    "else:\n",
    "    logger.info(f\"Using split with {len(test_split)} samples\")\n",
    "    \n",
    "    # Show sample structure\n",
    "    if len(test_split) > 0:\n",
    "        sample = test_split[0]\n",
    "        logger.info(f\"Sample keys: {sample.keys()}\")\n",
    "        for key in sample.keys():\n",
    "            val = sample[key]\n",
    "            if isinstance(val, (list, np.ndarray)):\n",
    "                logger.info(f\"  {key}: {type(val).__name__} with shape {len(val)}\")\n",
    "            else:\n",
    "                logger.info(f\"  {key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6443206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE ON LSVSC DATASET\n",
    "# ============================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import librosa\n",
    "import os\n",
    "import json\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"EVALUATING ON LSVSC DATASET\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "ENCODER = \"vinai/PhoWhisper-base\"\n",
    "# Model checkpoint path\n",
    "model_checkpoint = \"/kaggle/input/fine-tune-vimd/final_model_vimd/best_model_resumed\"\n",
    "\n",
    "if not os.path.exists(model_checkpoint):\n",
    "    logger.error(f\"Model not found: {model_checkpoint}\")\n",
    "else:\n",
    "    from src.models import MultiTaskSpeakerModel\n",
    "    from safetensors.torch import load_file\n",
    "    \n",
    "    # Load model with correct architecture (head_hidden_dim=512 from training config)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MultiTaskSpeakerModel(ENCODER, head_hidden_dim=512)\n",
    "    \n",
    "    safetensors_path = os.path.join(model_checkpoint, \"model.safetensors\")\n",
    "    checkpoint = load_file(safetensors_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    logger.info(f\"Model loaded successfully from: {model_checkpoint}\")\n",
    "    \n",
    "    # Get processor\n",
    "    if \"whisper\" in ENCODER.lower():\n",
    "        from transformers import WhisperFeatureExtractor\n",
    "        processor = WhisperFeatureExtractor.from_pretrained(ENCODER)\n",
    "    else:\n",
    "        from transformers import Wav2Vec2FeatureExtractor\n",
    "        processor = Wav2Vec2FeatureExtractor.from_pretrained(ENCODER)\n",
    "    \n",
    "    # Prepare LSVSC data\n",
    "    is_whisper = \"whisper\" in ENCODER.lower()\n",
    "    sampling_rate = 16000\n",
    "    \n",
    "    gender_preds = []\n",
    "    dialect_preds = []\n",
    "    gender_gts = []\n",
    "    dialect_gts = []\n",
    "    \n",
    "    logger.info(f\"Processing {len(test_split)} samples...\")\n",
    "    \n",
    "    # Process each sample\n",
    "    for idx, sample in enumerate(test_split):\n",
    "        if idx % 500 == 0:\n",
    "            logger.info(f\"  Processing sample {idx}/{len(test_split)}\")\n",
    "        \n",
    "        try:\n",
    "            # Get audio - LSVSC has 'audio' field with dict structure\n",
    "            audio_dict = sample[\"audio\"]\n",
    "            waveform = np.array(audio_dict[\"array\"], dtype=np.float32)\n",
    "            sr = audio_dict[\"sampling_rate\"]\n",
    "            \n",
    "            # Resample if needed\n",
    "            if sr != sampling_rate:\n",
    "                waveform = librosa.resample(waveform, orig_sr=sr, target_sr=sampling_rate)\n",
    "            \n",
    "            # Process audio based on model type\n",
    "            if is_whisper:\n",
    "                # Pad to 30 seconds\n",
    "                whisper_length = sampling_rate * 30\n",
    "                if len(waveform) < whisper_length:\n",
    "                    waveform = np.pad(waveform, (0, whisper_length - len(waveform)))\n",
    "                else:\n",
    "                    waveform = waveform[:whisper_length]\n",
    "                \n",
    "                inputs = processor(waveform, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "                input_tensor = inputs.input_features.to(device)\n",
    "            else:\n",
    "                inputs = processor(waveform, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "                input_tensor = inputs.input_values.to(device)\n",
    "            \n",
    "            # Inference\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_tensor)\n",
    "                gender_logits = outputs['gender_logits']\n",
    "                dialect_logits = outputs['dialect_logits']\n",
    "            \n",
    "            gender_pred = gender_logits.argmax(dim=-1).item()\n",
    "            dialect_pred = dialect_logits.argmax(dim=-1).item()\n",
    "            \n",
    "            # Get ground truth - LSVSC format: \"female\" / \"male\", \"northern dialect\" / \"central dialect\" / \"southern dialect\"\n",
    "            gender_label = sample[\"gender\"].lower()\n",
    "            gender_gt = 0 if \"female\" in gender_label else 1\n",
    "            \n",
    "            dialect_label = sample[\"dialect\"].lower()\n",
    "            if \"northern\" in dialect_label:\n",
    "                dialect_gt = 0\n",
    "            elif \"central\" in dialect_label:\n",
    "                dialect_gt = 1\n",
    "            else:  # southern\n",
    "                dialect_gt = 2\n",
    "            \n",
    "            # Only append if inference succeeded\n",
    "            gender_preds.append(gender_pred)\n",
    "            dialect_preds.append(dialect_pred)\n",
    "            gender_gts.append(gender_gt)\n",
    "            dialect_gts.append(dialect_gt)\n",
    "        \n",
    "        except Exception as e:\n",
    "            if idx < 5:  # Only log first few errors\n",
    "                logger.warning(f\"Error processing sample {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(\"LSVSC EVALUATION RESULTS\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    \n",
    "    if gender_gts and gender_preds:\n",
    "        gender_acc = accuracy_score(gender_gts, gender_preds)\n",
    "        logger.info(f\"\\nGender Classification (Female=0, Male=1):\")\n",
    "        logger.info(f\"  Accuracy: {gender_acc:.4f}\")\n",
    "        logger.info(f\"  Samples: {len(gender_gts)}\")\n",
    "        cm_gender = confusion_matrix(gender_gts, gender_preds)\n",
    "        logger.info(f\"  Confusion Matrix:\\n{cm_gender}\")\n",
    "        logger.info(f\"\\n{classification_report(gender_gts, gender_preds, target_names=['Female', 'Male'])}\")\n",
    "    else:\n",
    "        logger.warning(\"No gender ground truth data\")\n",
    "    \n",
    "    if dialect_gts and dialect_preds:\n",
    "        dialect_acc = accuracy_score(dialect_gts, dialect_preds)\n",
    "        logger.info(f\"\\nDialect Classification (North=0, Central=1, South=2):\")\n",
    "        logger.info(f\"  Accuracy: {dialect_acc:.4f}\")\n",
    "        logger.info(f\"  Samples: {len(dialect_gts)}\")\n",
    "        cm_dialect = confusion_matrix(dialect_gts, dialect_preds)\n",
    "        logger.info(f\"  Confusion Matrix:\\n{cm_dialect}\")\n",
    "        logger.info(f\"\\n{classification_report(dialect_gts, dialect_preds, target_names=['North', 'Central', 'South'])}\")\n",
    "    else:\n",
    "        logger.warning(\"No dialect ground truth data\")\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        \"dataset\": \"LSVSC\",\n",
    "        \"model_checkpoint\": model_checkpoint,\n",
    "        \"encoder\": ENCODER,\n",
    "        \"num_samples_total\": len(test_split),\n",
    "        \"num_samples_processed\": len(gender_gts),\n",
    "        \"num_samples_failed\": len(test_split) - len(gender_gts),\n",
    "        \"gender_accuracy\": float(accuracy_score(gender_gts, gender_preds)) if gender_gts and gender_preds else None,\n",
    "        \"dialect_accuracy\": float(accuracy_score(dialect_gts, dialect_preds)) if dialect_gts and dialect_preds else None,\n",
    "        \"gender_confusion_matrix\": cm_gender.tolist() if 'cm_gender' in locals() else None,\n",
    "        \"dialect_confusion_matrix\": cm_dialect.tolist() if 'cm_dialect' in locals() else None,\n",
    "    }\n",
    "    \n",
    "    results_file = \"/kaggle/working/final_model_vimd/lsvsc_evaluation.json\"\n",
    "    os.makedirs(os.path.dirname(results_file), exist_ok=True)\n",
    "    with open(results_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"\\nResults saved to: {results_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
