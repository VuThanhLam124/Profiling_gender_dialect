{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31722c8f",
   "metadata": {},
   "source": [
    "Tôi sẽ sử dụng file này, sau đó import lên Kaggle và chạy để sử dụng GPU và Ram của kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b3c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/VuThanhLam124/Profiling_gender_dialect.git\n",
    "!apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931ef746",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Profiling_gender_dialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b171c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install -q transformers==4.44.0 accelerate==0.33.0 datasets==2.21.0\n",
    "!pip install -q librosa soundfile audiomentations==0.35.0 wandb safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9094d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FIX: Patch models.py for attention mask size mismatch\n",
    "# ============================================================\n",
    "fix_code = '''\n",
    "        # Get hidden states from either raw audio or pre-extracted features\n",
    "        if input_features is not None:\n",
    "            # Use pre-extracted features directly\n",
    "            hidden_states = input_features\n",
    "        elif input_values is not None:\n",
    "            # Extract features from encoder\n",
    "            hidden_states = self._encode(input_values, attention_mask)\n",
    "        else:\n",
    "            raise ValueError(\"Either input_values or input_features must be provided\")\n",
    "        \n",
    "        # Create proper attention mask for hidden states (encoder downsamples audio)\n",
    "        # Hidden states have different sequence length than input audio\n",
    "        if attention_mask is not None and hidden_states.shape[1] != attention_mask.shape[1]:\n",
    "            # Create new mask based on hidden states length\n",
    "            batch_size, seq_len, _ = hidden_states.shape\n",
    "            pooled_mask = torch.ones(batch_size, seq_len, device=hidden_states.device)\n",
    "        else:\n",
    "            pooled_mask = attention_mask\n",
    "        \n",
    "        # Attentive pooling\n",
    "        pooled, attn_weights = self.attentive_pooling(hidden_states, pooled_mask)\n",
    "'''\n",
    "\n",
    "old_code = '''\n",
    "        # Get hidden states from either raw audio or pre-extracted features\n",
    "        if input_features is not None:\n",
    "            # Use pre-extracted features directly\n",
    "            hidden_states = input_features\n",
    "        elif input_values is not None:\n",
    "            # Extract features from encoder\n",
    "            hidden_states = self._encode(input_values, attention_mask)\n",
    "        else:\n",
    "            raise ValueError(\"Either input_values or input_features must be provided\")\n",
    "        \n",
    "        # Attentive pooling\n",
    "        pooled, attn_weights = self.attentive_pooling(hidden_states, attention_mask)\n",
    "'''\n",
    "\n",
    "with open(\"src/models.py\", \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "if old_code in content:\n",
    "    content = content.replace(old_code, fix_code)\n",
    "    with open(\"src/models.py\", \"w\") as f:\n",
    "        f.write(content)\n",
    "    logger.info(\"Patched src/models.py - Fixed attention mask size mismatch\")\n",
    "else:\n",
    "    logger.info(\"src/models.py already patched or different version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Train 3 ENCODER TYPES (20 epochs each)\n",
    "# ============================================================\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"training\")\n",
    "\n",
    "ENCODERS_TO_TEST = [\n",
    "    \"microsoft/wavlm-base-plus\",\n",
    "    \"facebook/hubert-base-ls960\",\n",
    "    \"facebook/wav2vec2-base\",\n",
    "]\n",
    "\n",
    "base_config = \"\"\"\n",
    "model:\n",
    "  name: \"{encoder_name}\"\n",
    "  num_genders: 2\n",
    "  num_dialects: 3\n",
    "  dropout: 0.15\n",
    "  head_hidden_dim: 256\n",
    "  freeze_encoder: false \n",
    "\n",
    "training:\n",
    "  batch_size: 32\n",
    "  gradient_accumulation_steps: 4\n",
    "  learning_rate: 3.5e-5\n",
    "  num_epochs: 20  \n",
    "  warmup_ratio: 0.1\n",
    "  weight_decay: 0.01\n",
    "  gradient_clip: 1.0\n",
    "  lr_scheduler: \"linear\"\n",
    "  fp16: true\n",
    "  dataloader_num_workers: 2\n",
    "\n",
    "loss:\n",
    "  dialect_weight: 2.5\n",
    "\n",
    "wandb:\n",
    "  enabled: false\n",
    "\n",
    "data:\n",
    "  source: \"vispeech\"\n",
    "  vispeech_root: \"/kaggle/input/vispeech\"\n",
    "  train_meta: \"/kaggle/input/vispeech/metadata/trainset.csv\"\n",
    "  train_audio: \"/kaggle/input/vispeech/trainset\"\n",
    "  clean_test_meta: \"/kaggle/input/vispeech/metadata/clean_testset.csv\"\n",
    "  clean_test_audio: \"/kaggle/input/vispeech/clean_testset\"\n",
    "  noisy_test_meta: \"/kaggle/input/vispeech/metadata/noisy_testset.csv\"\n",
    "  noisy_test_audio: \"/kaggle/input/vispeech/noisy_testset\"\n",
    "  val_split: 0.15\n",
    "\n",
    "audio:\n",
    "  sampling_rate: 16000\n",
    "  max_duration: 5\n",
    "\n",
    "augmentation:\n",
    "  enabled: true\n",
    "  prob: 0.8\n",
    "\n",
    "output:\n",
    "  dir: \"/kaggle/working/output_{encoder_short}\"\n",
    "  save_total_limit: 1\n",
    "  metric_for_best_model: \"dialect_acc\"\n",
    "\n",
    "early_stopping:\n",
    "  patience: 3\n",
    "  threshold: 0.00001\n",
    "\n",
    "labels:\n",
    "  gender:\n",
    "    Male: 0\n",
    "    Female: 1\n",
    "  dialect:\n",
    "    North: 0\n",
    "    Central: 1\n",
    "    South: 2\n",
    "\n",
    "seed: 42\n",
    "\"\"\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "for encoder in ENCODERS_TO_TEST:\n",
    "    encoder_short = encoder.split(\"/\")[-1]\n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(f\"TRAINING: {encoder}\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    \n",
    "    config_content = base_config.format(\n",
    "        encoder_name=encoder,\n",
    "        encoder_short=encoder_short\n",
    "    )\n",
    "    \n",
    "    config_path = f\"configs/train_{encoder_short}.yaml\"\n",
    "    with open(config_path, \"w\") as f:\n",
    "        f.write(config_content)\n",
    "    \n",
    "    logger.info(f\"Config: {config_path}\")\n",
    "    logger.info(f\"Output: /kaggle/working/output_{encoder_short}\")\n",
    "    \n",
    "    exit_code = os.system(f\"python finetune.py --config {config_path}\")\n",
    "    \n",
    "    if exit_code == 0:\n",
    "        results[encoder] = \"SUCCESS\"\n",
    "        logger.info(f\"{encoder_short}: Training completed\")\n",
    "    else:\n",
    "        results[encoder] = f\"FAILED (exit code: {exit_code})\"\n",
    "        logger.error(f\"{encoder_short}: Training failed\")\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"TRAINING SUMMARY\")\n",
    "logger.info(\"=\" * 70)\n",
    "for encoder, status in results.items():\n",
    "    logger.info(f\"  {encoder}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eceb55",
   "metadata": {},
   "source": [
    "Eval with ViSpeech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c28347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CHECK SAVED MODELS\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "ENCODERS = [\"wavlm-base-plus\", \"hubert-base-ls960\", \"wav2vec2-base\"]\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"SAVED MODELS\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "for encoder in ENCODERS:\n",
    "    model_dir = f\"/kaggle/working/output_{encoder}/best_model\"\n",
    "    if os.path.exists(model_dir):\n",
    "        logger.info(f\"{encoder}:\")\n",
    "        total_size = 0\n",
    "        for f in sorted(os.listdir(model_dir)):\n",
    "            size = os.path.getsize(os.path.join(model_dir, f)) / 1024 / 1024\n",
    "            total_size += size\n",
    "            logger.info(f\"  {f}: {size:.1f} MB\")\n",
    "        logger.info(f\"  Total: {total_size:.1f} MB\")\n",
    "    else:\n",
    "        logger.warning(f\"{encoder}: Model not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defeb5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE ALL MODELS ON CLEAN & NOISY TEST SETS\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "ENCODERS = [\"wavlm-base-plus\", \"hubert-base-ls960\", \"wav2vec2-base\"]\n",
    "eval_results = {}\n",
    "\n",
    "for encoder in ENCODERS:\n",
    "    model_dir = f\"/kaggle/working/output_{encoder}/best_model\"\n",
    "    config_path = f\"configs/train_{encoder}.yaml\"\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        logger.warning(f\"{encoder}: Model not found, skipping eval\")\n",
    "        continue\n",
    "    \n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(f\"EVALUATING: {encoder}\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    \n",
    "    # Evaluate on clean test\n",
    "    logger.info(\"Clean test set...\")\n",
    "    exit_code = os.system(\n",
    "        f\"python eval.py --checkpoint {model_dir} --config {config_path} \"\n",
    "        f\"--test_name clean_test --output_dir /kaggle/working/output_{encoder}/eval\"\n",
    "    )\n",
    "    \n",
    "    # Evaluate on noisy test\n",
    "    logger.info(\"Noisy test set...\")\n",
    "    exit_code = os.system(\n",
    "        f\"python eval.py --checkpoint {model_dir} --config {config_path} \"\n",
    "        f\"--test_name noisy_test --output_dir /kaggle/working/output_{encoder}/eval\"\n",
    "    )\n",
    "    \n",
    "    if exit_code == 0:\n",
    "        eval_results[encoder] = \"SUCCESS\"\n",
    "    else:\n",
    "        eval_results[encoder] = \"FAILED\"\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"EVALUATION SUMMARY\")\n",
    "logger.info(\"=\" * 70)\n",
    "for encoder, status in eval_results.items():\n",
    "    logger.info(f\"  {encoder}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95388a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE MODELS TO KAGGLE OUTPUT\n",
    "# ============================================================\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/final_models\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "ENCODERS = [\"wavlm-base-plus\", \"hubert-base-ls960\", \"wav2vec2-base\"]\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"COPYING MODELS TO OUTPUT\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "for encoder in ENCODERS:\n",
    "    src_dir = f\"/kaggle/working/output_{encoder}/best_model\"\n",
    "    dst_dir = f\"{OUTPUT_DIR}/{encoder}\"\n",
    "    \n",
    "    if os.path.exists(src_dir):\n",
    "        if os.path.exists(dst_dir):\n",
    "            shutil.rmtree(dst_dir)\n",
    "        shutil.copytree(src_dir, dst_dir)\n",
    "        logger.info(f\"Copied: {encoder}\")\n",
    "    else:\n",
    "        logger.warning(f\"Not found: {encoder}\")\n",
    "\n",
    "# Also copy eval results\n",
    "for encoder in ENCODERS:\n",
    "    eval_dir = f\"/kaggle/working/output_{encoder}/eval\"\n",
    "    if os.path.exists(eval_dir):\n",
    "        dst_eval = f\"{OUTPUT_DIR}/{encoder}/eval\"\n",
    "        if os.path.exists(dst_eval):\n",
    "            shutil.rmtree(dst_eval)\n",
    "        shutil.copytree(eval_dir, dst_eval)\n",
    "        logger.info(f\"Copied eval: {encoder}\")\n",
    "\n",
    "logger.info(f\"All models saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e86533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LIST FINAL OUTPUT\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/final_models\"\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"FINAL OUTPUT STRUCTURE\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "    level = root.replace(OUTPUT_DIR, '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    logger.info(f\"{indent}{os.path.basename(root)}/\")\n",
    "    sub_indent = '  ' * (level + 1)\n",
    "    for file in files:\n",
    "        size = os.path.getsize(os.path.join(root, file)) / 1024 / 1024\n",
    "        logger.info(f\"{sub_indent}{file} ({size:.1f} MB)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
