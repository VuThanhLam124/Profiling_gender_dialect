{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31722c8f",
   "metadata": {},
   "source": [
    "Tôi sẽ sử dụng file này, sau đó import lên Kaggle và chạy để sử dụng GPU và Ram của kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b3c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/VuThanhLam124/Profiling_gender_dialect.git\n",
    "!apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931ef746",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Profiling_gender_dialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b171c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install -q transformers==4.44.0 accelerate==0.33.0 datasets==2.21.0\n",
    "!pip install -q librosa soundfile audiomentations==0.35.0 wandb safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Train 3 ENCODER TYPES (20 epochs each)\n",
    "# ============================================================\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"training\")\n",
    "\n",
    "ENCODERS_TO_TEST = [\n",
    "    \"microsoft/wavlm-base-plus\",\n",
    "    \"facebook/hubert-base-ls960\",\n",
    "    \"facebook/wav2vec2-base\",\n",
    "]\n",
    "\n",
    "WANDB_API_KEY = \"f05e29c3466ec288e97041e0e3d541c4087096a6\"\n",
    "\n",
    "base_config = \"\"\"\n",
    "model:\n",
    "  name: \"{encoder_name}\"\n",
    "  num_genders: 2\n",
    "  num_dialects: 3\n",
    "  dropout: 0.25\n",
    "  head_hidden_dim: 256\n",
    "  freeze_encoder: false \n",
    "\n",
    "training:\n",
    "  batch_size: 32\n",
    "  gradient_accumulation_steps: 4\n",
    "  learning_rate: 2e-5\n",
    "  num_epochs: 20  \n",
    "  warmup_ratio: 0.1\n",
    "  weight_decay: 0.01\n",
    "  gradient_clip: 0.5\n",
    "  lr_scheduler: \"cosine\"\n",
    "  fp16: true\n",
    "  dataloader_num_workers: 2\n",
    "\n",
    "loss:\n",
    "  dialect_weight: 3\n",
    "\n",
    "wandb:\n",
    "  enabled: true\n",
    "  api_key: \"{wandb_key}\"\n",
    "  project: \"vispeech-speaker-profiling\"\n",
    "  run_name: \"{encoder_short}\"\n",
    "\n",
    "data:\n",
    "  source: \"vispeech\"\n",
    "  vispeech_root: \"/kaggle/input/vispeech\"\n",
    "  train_meta: \"/kaggle/input/vispeech/metadata/trainset.csv\"\n",
    "  train_audio: \"/kaggle/input/vispeech/trainset\"\n",
    "  clean_test_meta: \"/kaggle/input/vispeech/metadata/clean_testset.csv\"\n",
    "  clean_test_audio: \"/kaggle/input/vispeech/clean_testset\"\n",
    "  noisy_test_meta: \"/kaggle/input/vispeech/metadata/noisy_testset.csv\"\n",
    "  noisy_test_audio: \"/kaggle/input/vispeech/noisy_testset\"\n",
    "  val_split: 0.15\n",
    "\n",
    "audio:\n",
    "  sampling_rate: 16000\n",
    "  max_duration: 5\n",
    "\n",
    "augmentation:\n",
    "  enabled: true\n",
    "  prob: 0.8\n",
    "\n",
    "output:\n",
    "  dir: \"/kaggle/working/output_{encoder_short}\"\n",
    "  save_total_limit: 1\n",
    "  metric_for_best_model: \"dialect_acc\"\n",
    "\n",
    "early_stopping:\n",
    "  patience: 5\n",
    "  threshold: 0.001\n",
    "\n",
    "labels:\n",
    "  gender:\n",
    "    Male: 0\n",
    "    Female: 1\n",
    "  dialect:\n",
    "    North: 0\n",
    "    Central: 1\n",
    "    South: 2\n",
    "\n",
    "seed: 42\n",
    "\"\"\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "for encoder in ENCODERS_TO_TEST:\n",
    "    encoder_short = encoder.split(\"/\")[-1]\n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(f\"TRAINING: {encoder}\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    \n",
    "    config_content = base_config.format(\n",
    "        encoder_name=encoder,\n",
    "        encoder_short=encoder_short,\n",
    "        wandb_key=WANDB_API_KEY\n",
    "    )\n",
    "    \n",
    "    config_path = f\"configs/train_{encoder_short}.yaml\"\n",
    "    with open(config_path, \"w\") as f:\n",
    "        f.write(config_content)\n",
    "    \n",
    "    logger.info(f\"Config: {config_path}\")\n",
    "    logger.info(f\"Output: /kaggle/working/output_{encoder_short}\")\n",
    "    logger.info(f\"WandB: enabled, project=vispeech-speaker-profiling\")\n",
    "    \n",
    "    exit_code = os.system(f\"python finetune.py --config {config_path}\")\n",
    "    \n",
    "    if exit_code == 0:\n",
    "        results[encoder] = \"SUCCESS\"\n",
    "        logger.info(f\"{encoder_short}: Training completed\")\n",
    "    else:\n",
    "        results[encoder] = f\"FAILED (exit code: {exit_code})\"\n",
    "        logger.error(f\"{encoder_short}: Training failed\")\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"TRAINING SUMMARY\")\n",
    "logger.info(\"=\" * 70)\n",
    "for encoder, status in results.items():\n",
    "    logger.info(f\"  {encoder}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeae78d",
   "metadata": {},
   "source": [
    "## Evaluate BEFORE Training (Pretrained Models - No Finetuning)\n",
    "Đánh giá tất cả pretrained models (chưa finetune) trên clean_testset để có baseline so sánh với sau khi train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE PRETRAINED MODELS (BEFORE FINETUNING)\n",
    "# ============================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import AutoFeatureExtractor\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "from src.models import MultiTaskSpeakerModel\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"EVALUATING PRETRAINED MODELS (BEFORE FINETUNING)\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "# Load test metadata\n",
    "test_meta_path = \"/kaggle/input/vispeech/metadata/clean_testset.csv\"\n",
    "test_audio_dir = \"/kaggle/input/vispeech/clean_testset\"\n",
    "test_df = pd.read_csv(test_meta_path)\n",
    "\n",
    "logger.info(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Label mappings\n",
    "gender_map = {'Male': 0, 'Female': 1}\n",
    "dialect_map = {'North': 0, 'Central': 1, 'South': 2}\n",
    "\n",
    "sampling_rate = 16000\n",
    "max_duration = 5\n",
    "max_length = sampling_rate * max_duration\n",
    "\n",
    "# Store results for all encoders\n",
    "pretrained_results = {}\n",
    "\n",
    "for encoder in ENCODERS_TO_TEST:\n",
    "    encoder_short = encoder.split(\"/\")[-1]\n",
    "    \n",
    "    logger.info(\"-\" * 50)\n",
    "    logger.info(f\"Evaluating: {encoder}\")\n",
    "    \n",
    "    # Initialize pretrained model (random classification heads)\n",
    "    model = MultiTaskSpeakerModel(\n",
    "        model_name=encoder,\n",
    "        num_genders=2,\n",
    "        num_dialects=3,\n",
    "        dropout=0.25,\n",
    "        head_hidden_dim=256\n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load feature extractor\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(encoder)\n",
    "    \n",
    "    # Evaluate\n",
    "    gender_preds, gender_labels = [], []\n",
    "    dialect_preds, dialect_labels = [], []\n",
    "    \n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=encoder_short):\n",
    "        try:\n",
    "            audio_path = os.path.join(test_audio_dir, row['audio_name'])\n",
    "            \n",
    "            # Load audio\n",
    "            audio, _ = librosa.load(audio_path, sr=sampling_rate, mono=True)\n",
    "            \n",
    "            # Trim silence\n",
    "            audio, _ = librosa.effects.trim(audio, top_db=20)\n",
    "            \n",
    "            # Normalize\n",
    "            if len(audio) > 0:\n",
    "                audio = audio / (np.max(np.abs(audio)) + 1e-8)\n",
    "            \n",
    "            # Pad or truncate\n",
    "            if len(audio) < max_length:\n",
    "                audio = np.pad(audio, (0, max_length - len(audio)))\n",
    "            else:\n",
    "                start = (len(audio) - max_length) // 2\n",
    "                audio = audio[start:start + max_length]\n",
    "            \n",
    "            # Extract features\n",
    "            inputs = feature_extractor(\n",
    "                audio,\n",
    "                sampling_rate=sampling_rate,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            # Predict\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs.input_values.to(device))\n",
    "                g_pred = torch.argmax(outputs['gender_logits'], dim=-1).cpu().item()\n",
    "                d_pred = torch.argmax(outputs['dialect_logits'], dim=-1).cpu().item()\n",
    "            \n",
    "            # Get labels\n",
    "            g_label = gender_map.get(row['gender'], 0)\n",
    "            d_label = dialect_map.get(row['dialect'], 0)\n",
    "            \n",
    "            gender_preds.append(g_pred)\n",
    "            gender_labels.append(g_label)\n",
    "            dialect_preds.append(d_pred)\n",
    "            dialect_labels.append(d_label)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    gender_acc = accuracy_score(gender_labels, gender_preds)\n",
    "    gender_f1 = f1_score(gender_labels, gender_preds, average='weighted')\n",
    "    dialect_acc = accuracy_score(dialect_labels, dialect_preds)\n",
    "    dialect_f1 = f1_score(dialect_labels, dialect_preds, average='weighted')\n",
    "    \n",
    "    pretrained_results[encoder_short] = {\n",
    "        'gender_acc': gender_acc,\n",
    "        'gender_f1': gender_f1,\n",
    "        'dialect_acc': dialect_acc,\n",
    "        'dialect_f1': dialect_f1\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"  Gender Acc: {gender_acc*100:.2f}% | Dialect Acc: {dialect_acc*100:.2f}%\")\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Summary table\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"PRETRAINED MODELS BASELINE (BEFORE FINETUNING)\")\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(f\"{'Encoder':<25} {'Gender Acc':>12} {'Dialect Acc':>12}\")\n",
    "logger.info(\"-\" * 51)\n",
    "\n",
    "for encoder_short, metrics in pretrained_results.items():\n",
    "    logger.info(f\"{encoder_short:<25} {metrics['gender_acc']*100:>11.2f}% {metrics['dialect_acc']*100:>11.2f}%\")\n",
    "\n",
    "logger.info(\"-\" * 51)\n",
    "logger.info(\"(Random heads = ~50% gender, ~33% dialect expected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eceb55",
   "metadata": {},
   "source": [
    "Eval with ViSpeech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c28347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CHECK SAVED MODELS\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "ENCODERS = [\"wavlm-base-plus\", \"hubert-base-ls960\", \"wav2vec2-base\"]\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"SAVED MODELS\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "for encoder in ENCODERS:\n",
    "    model_dir = f\"/kaggle/working/output_{encoder}/best_model\"\n",
    "    if os.path.exists(model_dir):\n",
    "        logger.info(f\"{encoder}:\")\n",
    "        total_size = 0\n",
    "        for f in sorted(os.listdir(model_dir)):\n",
    "            size = os.path.getsize(os.path.join(model_dir, f)) / 1024 / 1024\n",
    "            total_size += size\n",
    "            logger.info(f\"  {f}: {size:.1f} MB\")\n",
    "        logger.info(f\"  Total: {total_size:.1f} MB\")\n",
    "    else:\n",
    "        logger.warning(f\"{encoder}: Model not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defeb5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE ALL MODELS ON CLEAN & NOISY TEST SETS\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "ENCODERS = [\"wavlm-base-plus\", \"hubert-base-ls960\", \"wav2vec2-base\"]\n",
    "eval_results = {}\n",
    "\n",
    "for encoder in ENCODERS:\n",
    "    model_dir = f\"/kaggle/working/output_{encoder}/best_model\"\n",
    "    config_path = f\"configs/train_{encoder}.yaml\"\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        logger.warning(f\"{encoder}: Model not found, skipping eval\")\n",
    "        continue\n",
    "    \n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(f\"EVALUATING: {encoder}\")\n",
    "    logger.info(\"=\" * 70)\n",
    "    \n",
    "    # Evaluate on clean test\n",
    "    logger.info(\"Clean test set...\")\n",
    "    exit_code = os.system(\n",
    "        f\"python eval.py --checkpoint {model_dir} --config {config_path} \"\n",
    "        f\"--test_name clean_test --output_dir /kaggle/working/output_{encoder}/eval\"\n",
    "    )\n",
    "    \n",
    "    # Evaluate on noisy test\n",
    "    logger.info(\"Noisy test set...\")\n",
    "    exit_code = os.system(\n",
    "        f\"python eval.py --checkpoint {model_dir} --config {config_path} \"\n",
    "        f\"--test_name noisy_test --output_dir /kaggle/working/output_{encoder}/eval\"\n",
    "    )\n",
    "    \n",
    "    if exit_code == 0:\n",
    "        eval_results[encoder] = \"SUCCESS\"\n",
    "    else:\n",
    "        eval_results[encoder] = \"FAILED\"\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"EVALUATION SUMMARY\")\n",
    "logger.info(\"=\" * 70)\n",
    "for encoder, status in eval_results.items():\n",
    "    logger.info(f\"  {encoder}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990e58e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPARE: BEFORE vs AFTER FINETUNING (ALL ENCODERS)\n",
    "# ============================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import AutoFeatureExtractor\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "from src.models import MultiTaskSpeakerModel\n",
    "from src.utils import load_model_checkpoint\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"EVALUATING FINETUNED MODELS (AFTER TRAINING)\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "# Load test metadata\n",
    "test_meta_path = \"/kaggle/input/vispeech/metadata/clean_testset.csv\"\n",
    "test_audio_dir = \"/kaggle/input/vispeech/clean_testset\"\n",
    "test_df = pd.read_csv(test_meta_path)\n",
    "\n",
    "# Label mappings\n",
    "gender_map = {'Male': 0, 'Female': 1}\n",
    "dialect_map = {'North': 0, 'Central': 1, 'South': 2}\n",
    "\n",
    "sampling_rate = 16000\n",
    "max_length = sampling_rate * 5\n",
    "\n",
    "# Store finetuned results\n",
    "finetuned_results = {}\n",
    "\n",
    "ENCODERS = [\"wavlm-base-plus\", \"hubert-base-ls960\", \"wav2vec2-base\"]\n",
    "ENCODER_FULL_NAMES = {\n",
    "    \"wavlm-base-plus\": \"microsoft/wavlm-base-plus\",\n",
    "    \"hubert-base-ls960\": \"facebook/hubert-base-ls960\",\n",
    "    \"wav2vec2-base\": \"facebook/wav2vec2-base\"\n",
    "}\n",
    "\n",
    "for encoder_short in ENCODERS:\n",
    "    model_dir = f\"/kaggle/working/output_{encoder_short}/best_model\"\n",
    "    encoder_full = ENCODER_FULL_NAMES[encoder_short]\n",
    "    \n",
    "    if not os.path.exists(model_dir):\n",
    "        logger.warning(f\"{encoder_short}: Model not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    logger.info(\"-\" * 50)\n",
    "    logger.info(f\"Evaluating finetuned: {encoder_short}\")\n",
    "    \n",
    "    # Load finetuned model\n",
    "    model = MultiTaskSpeakerModel(\n",
    "        model_name=encoder_full,\n",
    "        num_genders=2,\n",
    "        num_dialects=3,\n",
    "        dropout=0.25,\n",
    "        head_hidden_dim=256\n",
    "    )\n",
    "    model = load_model_checkpoint(model, model_dir, \"cuda\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load feature extractor from saved model\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_dir)\n",
    "    \n",
    "    # Evaluate\n",
    "    gender_preds, gender_labels = [], []\n",
    "    dialect_preds, dialect_labels = [], []\n",
    "    \n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=encoder_short):\n",
    "        try:\n",
    "            audio_path = os.path.join(test_audio_dir, row['audio_name'])\n",
    "            audio, _ = librosa.load(audio_path, sr=sampling_rate, mono=True)\n",
    "            audio, _ = librosa.effects.trim(audio, top_db=20)\n",
    "            \n",
    "            if len(audio) > 0:\n",
    "                audio = audio / (np.max(np.abs(audio)) + 1e-8)\n",
    "            \n",
    "            if len(audio) < max_length:\n",
    "                audio = np.pad(audio, (0, max_length - len(audio)))\n",
    "            else:\n",
    "                start = (len(audio) - max_length) // 2\n",
    "                audio = audio[start:start + max_length]\n",
    "            \n",
    "            inputs = feature_extractor(\n",
    "                audio, sampling_rate=sampling_rate,\n",
    "                return_tensors=\"pt\", padding=True\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs.input_values.to(device))\n",
    "                g_pred = torch.argmax(outputs['gender_logits'], dim=-1).cpu().item()\n",
    "                d_pred = torch.argmax(outputs['dialect_logits'], dim=-1).cpu().item()\n",
    "            \n",
    "            g_label = gender_map.get(row['gender'], 0)\n",
    "            d_label = dialect_map.get(row['dialect'], 0)\n",
    "            \n",
    "            gender_preds.append(g_pred)\n",
    "            gender_labels.append(g_label)\n",
    "            dialect_preds.append(d_pred)\n",
    "            dialect_labels.append(d_label)\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    finetuned_results[encoder_short] = {\n",
    "        'gender_acc': accuracy_score(gender_labels, gender_preds),\n",
    "        'gender_f1': f1_score(gender_labels, gender_preds, average='weighted'),\n",
    "        'dialect_acc': accuracy_score(dialect_labels, dialect_preds),\n",
    "        'dialect_f1': f1_score(dialect_labels, dialect_preds, average='weighted')\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"  Gender Acc: {finetuned_results[encoder_short]['gender_acc']*100:.2f}% | Dialect Acc: {finetuned_results[encoder_short]['dialect_acc']*100:.2f}%\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ============================================================\n",
    "# COMPARISON TABLE: BEFORE vs AFTER\n",
    "# ============================================================\n",
    "logger.info(\"\")\n",
    "logger.info(\"=\" * 90)\n",
    "logger.info(\"COMPARISON: PRETRAINED vs FINETUNED (All Encoders)\")\n",
    "logger.info(\"=\" * 90)\n",
    "\n",
    "logger.info(f\"{'Encoder':<20} | {'--- Gender Acc ---':^25} | {'--- Dialect Acc ---':^25}\")\n",
    "logger.info(f\"{'':<20} | {'Before':>8} {'After':>8} {'Δ':>7} | {'Before':>8} {'After':>8} {'Δ':>7}\")\n",
    "logger.info(\"-\" * 90)\n",
    "\n",
    "for encoder_short in ENCODERS:\n",
    "    if encoder_short not in finetuned_results:\n",
    "        continue\n",
    "    \n",
    "    before = pretrained_results.get(encoder_short, {'gender_acc': 0, 'dialect_acc': 0})\n",
    "    after = finetuned_results[encoder_short]\n",
    "    \n",
    "    g_before = before['gender_acc'] * 100\n",
    "    g_after = after['gender_acc'] * 100\n",
    "    g_delta = g_after - g_before\n",
    "    \n",
    "    d_before = before['dialect_acc'] * 100\n",
    "    d_after = after['dialect_acc'] * 100\n",
    "    d_delta = d_after - d_before\n",
    "    \n",
    "    logger.info(f\"{encoder_short:<20} | {g_before:>7.2f}% {g_after:>7.2f}% {'+' if g_delta > 0 else ''}{g_delta:>6.2f}% | {d_before:>7.2f}% {d_after:>7.2f}% {'+' if d_delta > 0 else ''}{d_delta:>6.2f}%\")\n",
    "\n",
    "logger.info(\"-\" * 90)\n",
    "\n",
    "# Find best model\n",
    "best_encoder = max(finetuned_results.keys(), key=lambda x: finetuned_results[x]['dialect_acc'])\n",
    "best_metrics = finetuned_results[best_encoder]\n",
    "\n",
    "logger.info(f\"BEST MODEL: {best_encoder}\")\n",
    "logger.info(f\"  Gender Accuracy:  {best_metrics['gender_acc']*100:.2f}%\")\n",
    "logger.info(f\"  Dialect Accuracy: {best_metrics['dialect_acc']*100:.2f}%\")\n",
    "logger.info(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95388a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE MODELS TO KAGGLE OUTPUT\n",
    "# ============================================================\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/final_models\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "ENCODERS = [\"wavlm-base-plus\", \"hubert-base-ls960\", \"wav2vec2-base\"]\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"COPYING MODELS TO OUTPUT\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "for encoder in ENCODERS:\n",
    "    src_dir = f\"/kaggle/working/output_{encoder}/best_model\"\n",
    "    dst_dir = f\"{OUTPUT_DIR}/{encoder}\"\n",
    "    \n",
    "    if os.path.exists(src_dir):\n",
    "        if os.path.exists(dst_dir):\n",
    "            shutil.rmtree(dst_dir)\n",
    "        shutil.copytree(src_dir, dst_dir)\n",
    "        logger.info(f\"Copied: {encoder}\")\n",
    "    else:\n",
    "        logger.warning(f\"Not found: {encoder}\")\n",
    "\n",
    "# Also copy eval results\n",
    "for encoder in ENCODERS:\n",
    "    eval_dir = f\"/kaggle/working/output_{encoder}/eval\"\n",
    "    if os.path.exists(eval_dir):\n",
    "        dst_eval = f\"{OUTPUT_DIR}/{encoder}/eval\"\n",
    "        if os.path.exists(dst_eval):\n",
    "            shutil.rmtree(dst_eval)\n",
    "        shutil.copytree(eval_dir, dst_eval)\n",
    "        logger.info(f\"Copied eval: {encoder}\")\n",
    "\n",
    "logger.info(f\"All models saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e86533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LIST FINAL OUTPUT\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/final_models\"\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"FINAL OUTPUT STRUCTURE\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "    level = root.replace(OUTPUT_DIR, '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    logger.info(f\"{indent}{os.path.basename(root)}/\")\n",
    "    sub_indent = '  ' * (level + 1)\n",
    "    for file in files:\n",
    "        size = os.path.getsize(os.path.join(root, file)) / 1024 / 1024\n",
    "        logger.info(f\"{sub_indent}{file} ({size:.1f} MB)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
