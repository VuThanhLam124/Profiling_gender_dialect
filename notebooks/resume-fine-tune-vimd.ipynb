{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df775df",
   "metadata": {
    "papermill": {
     "duration": 0.003333,
     "end_time": "2025-12-09T02:42:56.099770",
     "exception": false,
     "start_time": "2025-12-09T02:42:56.096437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Tôi sẽ sử dụng file này, sau đó import lên Kaggle và chạy để sử dụng GPU và Ram của kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f6e80b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T02:42:56.106571Z",
     "iopub.status.busy": "2025-12-09T02:42:56.105818Z",
     "iopub.status.idle": "2025-12-09T02:42:59.999610Z",
     "shell.execute_reply": "2025-12-09T02:42:59.998707Z"
    },
    "papermill": {
     "duration": 3.899034,
     "end_time": "2025-12-09T02:43:00.001393",
     "exception": false,
     "start_time": "2025-12-09T02:42:56.102359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Profiling_gender_dialect'...\r\n",
      "remote: Enumerating objects: 267, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (267/267), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (176/176), done.\u001b[K\r\n",
      "remote: Total 267 (delta 162), reused 194 (delta 89), pack-reused 0 (from 0)\u001b[K\r\n",
      "Receiving objects: 100% (267/267), 212.97 KiB | 7.10 MiB/s, done.\r\n",
      "Resolving deltas: 100% (162/162), done.\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\r\n",
      "0 upgraded, 0 newly installed, 0 to remove and 165 not upgraded.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/VuThanhLam124/Profiling_gender_dialect.git\n",
    "!apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5554d58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T02:43:00.012088Z",
     "iopub.status.busy": "2025-12-09T02:43:00.011361Z",
     "iopub.status.idle": "2025-12-09T02:43:00.017750Z",
     "shell.execute_reply": "2025-12-09T02:43:00.017062Z"
    },
    "papermill": {
     "duration": 0.012446,
     "end_time": "2025-12-09T02:43:00.018800",
     "exception": false,
     "start_time": "2025-12-09T02:43:00.006354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Profiling_gender_dialect\n"
     ]
    }
   ],
   "source": [
    "cd Profiling_gender_dialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4cbbfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T02:43:00.028519Z",
     "iopub.status.busy": "2025-12-09T02:43:00.028275Z",
     "iopub.status.idle": "2025-12-09T02:44:48.036066Z",
     "shell.execute_reply": "2025-12-09T02:44:48.034744Z"
    },
    "papermill": {
     "duration": 108.015461,
     "end_time": "2025-12-09T02:44:48.038379",
     "exception": false,
     "start_time": "2025-12-09T02:43:00.022918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: torchaudio>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.6.0+cu124)\r\n",
      "Collecting transformers==4.44.0 (from -r requirements.txt (line 3))\r\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.11.0)\r\n",
      "Requirement already satisfied: soundfile>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.13.1)\r\n",
      "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.26.4)\r\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2.2.3)\r\n",
      "Collecting scikit-learn>=1.3.0 (from -r requirements.txt (line 8))\r\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\r\n",
      "Collecting audiomentations==0.35.0 (from -r requirements.txt (line 9))\r\n",
      "  Downloading audiomentations-0.35.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: omegaconf>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (2.3.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (0.5.3)\r\n",
      "Collecting accelerate==0.33.0 (from -r requirements.txt (line 12))\r\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\r\n",
      "Requirement already satisfied: gradio>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (5.38.1)\r\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (4.67.1)\r\n",
      "Requirement already satisfied: wandb>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (0.21.0)\r\n",
      "Collecting datasets==2.21.0 (from -r requirements.txt (line 16))\r\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\r\n",
      "Collecting speechbrain>=1.0.0 (from -r requirements.txt (line 18))\r\n",
      "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.0->-r requirements.txt (line 3)) (3.20.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.0->-r requirements.txt (line 3)) (0.36.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.0->-r requirements.txt (line 3)) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.0->-r requirements.txt (line 3)) (6.0.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.0->-r requirements.txt (line 3)) (2025.11.3)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.0->-r requirements.txt (line 3)) (2.32.5)\r\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.44.0->-r requirements.txt (line 3))\r\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\n",
      "Collecting librosa>=0.10.0 (from -r requirements.txt (line 4))\r\n",
      "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Requirement already satisfied: scipy<2,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from audiomentations==0.35.0->-r requirements.txt (line 9)) (1.15.3)\r\n",
      "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from audiomentations==0.35.0->-r requirements.txt (line 9)) (0.5.0.post1)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.33.0->-r requirements.txt (line 12)) (7.1.3)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0->-r requirements.txt (line 16)) (19.0.1)\r\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.21.0->-r requirements.txt (line 16))\r\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0->-r requirements.txt (line 16)) (3.6.0)\r\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0->-r requirements.txt (line 16)) (0.70.18)\r\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==2.21.0->-r requirements.txt (line 16))\r\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0->-r requirements.txt (line 16)) (3.13.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (4.15.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.1.6)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->-r requirements.txt (line 1))\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->-r requirements.txt (line 1))\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->-r requirements.txt (line 1))\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->-r requirements.txt (line 1))\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->-r requirements.txt (line 1))\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->-r requirements.txt (line 1))\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->-r requirements.txt (line 1))\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->-r requirements.txt (line 1))\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->-r requirements.txt (line 1))\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->-r requirements.txt (line 1))\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->-r requirements.txt (line 1)) (1.3.0)\r\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 4)) (3.0.1)\r\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 4)) (1.5.2)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 4)) (4.4.2)\r\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 4)) (0.60.0)\r\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 4)) (1.8.2)\r\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 4)) (0.4)\r\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->-r requirements.txt (line 4)) (1.1.2)\r\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.0->-r requirements.txt (line 5)) (2.0.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r requirements.txt (line 6)) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r requirements.txt (line 6)) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r requirements.txt (line 6)) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r requirements.txt (line 6)) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r requirements.txt (line 6)) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0->-r requirements.txt (line 6)) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2025.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 8)) (3.6.0)\r\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf>=2.3.0->-r requirements.txt (line 10)) (4.9.3)\r\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (22.1.0)\r\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (4.11.0)\r\n",
      "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (1.1.0)\r\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (0.116.1)\r\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (0.6.1)\r\n",
      "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (1.11.0)\r\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (0.1.2)\r\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (0.28.1)\r\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (3.0.3)\r\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (3.11.0)\r\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (11.3.0)\r\n",
      "Collecting pydantic<2.12,>=2.0 (from gradio>=4.0.0->-r requirements.txt (line 13))\r\n",
      "  Downloading pydantic-2.11.10-py3-none-any.whl.metadata (68 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (0.25.1)\r\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (0.0.20)\r\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (0.12.5)\r\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (0.1.6)\r\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (2.10.0)\r\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (0.47.2)\r\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (0.13.3)\r\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (0.16.0)\r\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio>=4.0.0->-r requirements.txt (line 13)) (0.35.0)\r\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio>=4.0.0->-r requirements.txt (line 13)) (15.0.1)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.0->-r requirements.txt (line 15)) (8.3.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.0->-r requirements.txt (line 15)) (3.1.45)\r\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.0->-r requirements.txt (line 15)) (4.5.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.0->-r requirements.txt (line 15)) (6.33.0)\r\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.16.0->-r requirements.txt (line 15)) (2.33.2)\r\n",
      "Collecting hyperpyyaml (from speechbrain>=1.0.0->-r requirements.txt (line 18))\r\n",
      "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\r\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->-r requirements.txt (line 18)) (0.2.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 13)) (3.11)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 13)) (1.3.1)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.0->-r requirements.txt (line 5)) (2.23)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 16)) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 16)) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 16)) (25.4.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 16)) (1.8.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 16)) (6.7.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 16)) (0.4.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.21.0->-r requirements.txt (line 16)) (1.22.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.16.0->-r requirements.txt (line 15)) (4.0.12)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 13)) (2025.10.5)\r\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 13)) (1.0.9)\r\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 13)) (0.16.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.0->-r requirements.txt (line 3)) (1.2.0)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa>=0.10.0->-r requirements.txt (line 4)) (0.43.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio>=4.0.0->-r requirements.txt (line 13)) (0.7.0)\r\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<2.12,>=2.0->gradio>=4.0.0->-r requirements.txt (line 13))\r\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio>=4.0.0->-r requirements.txt (line 13)) (0.4.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 7)) (1.17.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.0->-r requirements.txt (line 3)) (3.4.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.0->-r requirements.txt (line 3)) (2.5.0)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 13)) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 13)) (14.2.0)\r\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.11/dist-packages (from hyperpyyaml->speechbrain>=1.0.0->-r requirements.txt (line 18)) (0.18.16)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->-r requirements.txt (line 6)) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->-r requirements.txt (line 6)) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0->-r requirements.txt (line 6)) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0->-r requirements.txt (line 6)) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0->-r requirements.txt (line 6)) (2024.2.0)\r\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting multiprocess (from datasets==2.21.0->-r requirements.txt (line 16))\r\n",
      "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\r\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.16.0->-r requirements.txt (line 15)) (5.0.2)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0->-r requirements.txt (line 6)) (2024.2.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 13)) (4.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 13)) (2.19.2)\r\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->-r requirements.txt (line 18)) (0.2.14)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 13)) (0.1.2)\r\n",
      "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading audiomentations-0.35.0-py3-none-any.whl (82 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pydantic-2.11.10-py3-none-any.whl (444 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\r\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pydantic-core, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, hyperpyyaml, tokenizers, nvidia-cusolver-cu12, scikit-learn, librosa, transformers, speechbrain, datasets, audiomentations, accelerate\r\n",
      "  Attempting uninstall: pydantic-core\r\n",
      "    Found existing installation: pydantic_core 2.41.5\r\n",
      "    Uninstalling pydantic_core-2.41.5:\r\n",
      "      Successfully uninstalled pydantic_core-2.41.5\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2025.10.0\r\n",
      "    Uninstalling fsspec-2025.10.0:\r\n",
      "      Successfully uninstalled fsspec-2025.10.0\r\n",
      "  Attempting uninstall: dill\r\n",
      "    Found existing installation: dill 0.4.0\r\n",
      "    Uninstalling dill-0.4.0:\r\n",
      "      Successfully uninstalled dill-0.4.0\r\n",
      "  Attempting uninstall: pydantic\r\n",
      "    Found existing installation: pydantic 2.12.4\r\n",
      "    Uninstalling pydantic-2.12.4:\r\n",
      "      Successfully uninstalled pydantic-2.12.4\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: multiprocess\r\n",
      "    Found existing installation: multiprocess 0.70.18\r\n",
      "    Uninstalling multiprocess-0.70.18:\r\n",
      "      Successfully uninstalled multiprocess-0.70.18\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.21.2\r\n",
      "    Uninstalling tokenizers-0.21.2:\r\n",
      "      Successfully uninstalled tokenizers-0.21.2\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "  Attempting uninstall: librosa\r\n",
      "    Found existing installation: librosa 0.11.0\r\n",
      "    Uninstalling librosa-0.11.0:\r\n",
      "      Successfully uninstalled librosa-0.11.0\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.53.3\r\n",
      "    Uninstalling transformers-4.53.3:\r\n",
      "      Successfully uninstalled transformers-4.53.3\r\n",
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 4.4.1\r\n",
      "    Uninstalling datasets-4.4.1:\r\n",
      "      Successfully uninstalled datasets-4.4.1\r\n",
      "  Attempting uninstall: accelerate\r\n",
      "    Found existing installation: accelerate 1.9.0\r\n",
      "    Uninstalling accelerate-1.9.0:\r\n",
      "      Successfully uninstalled accelerate-1.9.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "s3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2024.6.1 which is incompatible.\r\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\r\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed accelerate-0.33.0 audiomentations-0.35.0 datasets-2.21.0 dill-0.3.8 fsspec-2024.6.1 hyperpyyaml-1.2.2 librosa-0.10.2.post1 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydantic-2.11.10 pydantic-core-2.33.2 scikit-learn-1.7.2 speechbrain-1.0.3 tokenizers-0.19.1 transformers-4.44.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install -q transformers==4.44.0 accelerate==0.33.0 datasets==2.21.0\n",
    "!pip install -q librosa soundfile audiomentations==0.35.0 wandb safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e48e43e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T02:44:48.104187Z",
     "iopub.status.busy": "2025-12-09T02:44:48.103432Z",
     "iopub.status.idle": "2025-12-09T02:44:48.108642Z",
     "shell.execute_reply": "2025-12-09T02:44:48.107903Z"
    },
    "papermill": {
     "duration": 0.039944,
     "end_time": "2025-12-09T02:44:48.110012",
     "exception": false,
     "start_time": "2025-12-09T02:44:48.070068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # FINETUNE WITH ViMD DATASET\n",
    "# # ============================================================\n",
    "# import os\n",
    "# import logging\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# logger = logging.getLogger(\"training\")\n",
    "\n",
    "# ENCODER = \"vinai/PhoWhisper-base\"\n",
    "# encoder_short = ENCODER.split(\"/\")[-1]\n",
    "# WANDB_API_KEY = \"f05e29c3466ec288e97041e0e3d541c4087096a6\"\n",
    "\n",
    "# vimd_config = f\"\"\"\n",
    "# model:\n",
    "#   name: \"{ENCODER}\"\n",
    "#   num_genders: 2\n",
    "#   num_dialects: 3\n",
    "#   dropout: 0.25\n",
    "#   head_hidden_dim: 512\n",
    "#   freeze_encoder: false\n",
    "\n",
    "# training:\n",
    "#   batch_size: 32\n",
    "#   gradient_accumulation_steps: 4\n",
    "#   learning_rate: 2.5e-5\n",
    "#   num_epochs: 30\n",
    "#   warmup_ratio: 0.15\n",
    "#   weight_decay: 0.015\n",
    "#   gradient_clip: 0.5\n",
    "#   lr_scheduler: \"cosine\"\n",
    "#   fp16: true\n",
    "#   dataloader_num_workers: 2\n",
    "\n",
    "# loss:\n",
    "#   dialect_weight: 3\n",
    "\n",
    "# wandb:\n",
    "#   enabled: true\n",
    "#   api_key: \"{WANDB_API_KEY}\"\n",
    "#   project: \"vimd-speaker-profiling\"\n",
    "#   run_name: \"{encoder_short}\"\n",
    "\n",
    "# data:\n",
    "#   source: \"vimd\"\n",
    "#   vimd_path: \"/kaggle/input/vimd-dataset\"\n",
    "\n",
    "# audio:\n",
    "#   sampling_rate: 16000\n",
    "#   max_duration: 5\n",
    "\n",
    "# augmentation:\n",
    "#   enabled: true\n",
    "#   prob: 0.75\n",
    "\n",
    "# output:\n",
    "#   dir: \"/kaggle/working/output_vimd\"\n",
    "#   save_total_limit: 1\n",
    "#   metric_for_best_model: \"dialect_acc\"\n",
    "\n",
    "# early_stopping:\n",
    "#   patience: 5\n",
    "#   threshold: 0.001\n",
    "\n",
    "# labels:\n",
    "#   gender:\n",
    "#     Male: 0\n",
    "#     Female: 1\n",
    "#     0: 0\n",
    "#     1: 1\n",
    "#   dialect:\n",
    "#     North: 0\n",
    "#     Central: 1\n",
    "#     South: 2\n",
    "\n",
    "# seed: 42\n",
    "# \"\"\"\n",
    "\n",
    "# config_path = \"configs/vimd_train.yaml\"\n",
    "# with open(config_path, \"w\") as f:\n",
    "#     f.write(vimd_config)\n",
    "\n",
    "# logger.info(f\"Config saved: {config_path}\")\n",
    "# logger.info(f\"Encoder: {ENCODER}\")\n",
    "# logger.info(f\"Batch size: 32, Gradient accumulation: 4 (effective batch: 32x4)\")\n",
    "# logger.info(f\"WandB: enabled, project=vimd-speaker-profiling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a7f77b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T02:44:48.173647Z",
     "iopub.status.busy": "2025-12-09T02:44:48.172956Z",
     "iopub.status.idle": "2025-12-09T02:44:48.176804Z",
     "shell.execute_reply": "2025-12-09T02:44:48.176048Z"
    },
    "papermill": {
     "duration": 0.03712,
     "end_time": "2025-12-09T02:44:48.178072",
     "exception": false,
     "start_time": "2025-12-09T02:44:48.140952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # START TRAINING\n",
    "# # ============================================================\n",
    "# logger.info(\"=\" * 70)\n",
    "# logger.info(f\"TRAINING: {ENCODER}\")\n",
    "# logger.info(\"=\" * 70)\n",
    "\n",
    "# exit_code = os.system(f\"python finetune.py --config {config_path}\")\n",
    "\n",
    "# if exit_code == 0:\n",
    "#     logger.info(\"Training completed successfully\")\n",
    "# else:\n",
    "#     logger.error(f\"Training failed with exit code: {exit_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550327a1",
   "metadata": {
    "papermill": {
     "duration": 0.030843,
     "end_time": "2025-12-09T02:44:48.239954",
     "exception": false,
     "start_time": "2025-12-09T02:44:48.209111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Eval with ViDM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe579cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T02:44:48.304324Z",
     "iopub.status.busy": "2025-12-09T02:44:48.303814Z",
     "iopub.status.idle": "2025-12-09T07:43:10.792133Z",
     "shell.execute_reply": "2025-12-09T07:43:10.790987Z"
    },
    "papermill": {
     "duration": 17902.522625,
     "end_time": "2025-12-09T07:43:10.793900",
     "exception": false,
     "start_time": "2025-12-09T02:44:48.271275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 02:44:48,310 - INFO - ======================================================================\n",
      "2025-12-09 02:44:48,312 - INFO - RESUME TRAINING\n",
      "2025-12-09 02:44:48,312 - INFO - ======================================================================\n",
      "2025-12-09 02:44:48,318 - INFO - Resume config saved: configs/vimd_resume.yaml\n",
      "2025-12-09 02:44:48,319 - INFO - Resume from: /kaggle/input/phowhisper-vimd/output_vimd_resume/checkpoint-8460\n",
      "2025-12-09 02:44:48,320 - INFO - Additional epochs: 7\n",
      "2025-12-09 02:44:48,320 - INFO - Learning rate: 1e-5 (reduced for fine-tuning)\n",
      "2025-12-09 02:45:02.080513: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765248302.277203      95 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765248302.332768      95 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "2025-12-09 02:45:19 | INFO | ============================================================\n",
      "2025-12-09 02:45:19 | INFO | SPEAKER PROFILING TRAINING (Full Finetune)\n",
      "2025-12-09 02:45:19 | INFO | ============================================================\n",
      "2025-12-09 02:45:19 | INFO | Data source: vimd\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: No netrc file found, creating one.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "wandb: Currently logged in as: vuthanhlam848 (vuthanhlam848-vnpost) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.21.0\n",
      "wandb: Run data is saved locally in /kaggle/working/Profiling_gender_dialect/wandb/run-20251209_024519-nplfjz1y\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run PhoWhisper-base-resume\n",
      "wandb: ⭐️ View project at https://wandb.ai/vuthanhlam848-vnpost/vimd-speaker-profiling\n",
      "wandb: 🚀 View run at https://wandb.ai/vuthanhlam848-vnpost/vimd-speaker-profiling/runs/nplfjz1y\n",
      "2025-12-09 02:45:21 | INFO | WandB project: vimd-speaker-profiling\n",
      "2025-12-09 02:45:21 | INFO | Model: vinai/PhoWhisper-base\n",
      "2025-12-09 02:45:21 | INFO | Batch Size: 32\n",
      "2025-12-09 02:45:21 | INFO | Learning Rate: 2.5e-05\n",
      "2025-12-09 02:45:21 | INFO | Epochs: 7\n",
      "2025-12-09 02:45:21 | INFO | Dialect Loss Weight: 5x\n",
      "2025-12-09 02:45:21 | INFO | ------------------------------------------------------------\n",
      "2025-12-09 02:45:21 | INFO | Loading feature extractor for vinai/PhoWhisper-base...\n",
      "2025-12-09 02:45:21 | INFO | Loading ViMD dataset from /kaggle/input/vimd-dataset...\n",
      "Downloading data: 100%|██████████| 87/87 [00:00<00:00, 14100.95files/s]\n",
      "Generating train split: 15023 examples [06:05, 41.15 examples/s]\n",
      "Generating validation split: 1900 examples [00:42, 44.78 examples/s]\n",
      "Generating test split: 2026 examples [00:44, 45.28 examples/s]\n",
      "2025-12-09 02:52:57 | INFO | Available splits: ['train', 'validation', 'test']\n",
      "2025-12-09 02:52:57 | INFO | ViMD Train: 15,023 samples\n",
      "2025-12-09 02:52:57 | INFO | ViMD Validation: 1,900 samples\n",
      "2025-12-09 02:52:57 | INFO | ViMD Test: 2,026 samples\n",
      "2025-12-09 02:52:57 | INFO | Available columns: ['region', 'province_code', 'province_name', 'filename', 'text', 'speakerID', 'gender', 'audio']\n",
      "2025-12-09 02:52:57 | INFO | Creating ViMD datasets...\n",
      "2025-12-09 02:52:57 | INFO | Whisper mode: audio will be padded/truncated to 30 seconds (480000 samples)\n",
      "2025-12-09 02:52:57 | INFO | Augmentation ENABLED (prob=0.75)\n",
      "2025-12-09 02:52:57 | INFO | Whisper mode: audio will be padded/truncated to 30 seconds (480000 samples)\n",
      "2025-12-09 02:52:57 | INFO | Train samples: 15,023\n",
      "2025-12-09 02:52:57 | INFO | Validation samples: 1,900\n",
      "2025-12-09 02:52:57 | INFO | Loading model...\n",
      "2025-12-09 02:52:57 | INFO | Loading encoder: vinai/PhoWhisper-base\n",
      "2025-12-09 02:52:57 | INFO | Encoder class: WhisperModel\n",
      "2025-12-09 03:00:02 | INFO | Hidden size: 512\n",
      "2025-12-09 03:00:02 | INFO | Architecture: vinai/PhoWhisper-base + Attentive Pooling + LayerNorm\n",
      "2025-12-09 03:00:02 | INFO | Hidden size: 512\n",
      "2025-12-09 03:00:02 | INFO | Head hidden dim: 512\n",
      "2025-12-09 03:00:02 | INFO | Dropout: 0.25\n",
      "2025-12-09 03:00:02 | INFO | Total parameters: 73,516,549\n",
      "2025-12-09 03:00:02 | INFO | Trainable parameters: 72,748,549\n",
      "2025-12-09 03:00:02 | INFO | Starting training...\n",
      "2025-12-09 03:00:02 | INFO | Steps per epoch: ~469\n",
      " 14%|█▍        | 470/3290 [34:47<2:20:12,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1133, 'grad_norm': 8.748444557189941, 'learning_rate': 3.0120481927710845e-07, 'epoch': 0.0}\n",
      "{'loss': 6.0902, 'grad_norm': 8.716727256774902, 'learning_rate': 1.4759036144578315e-05, 'epoch': 0.11}\n",
      "{'loss': 4.7438, 'grad_norm': 46.04225158691406, 'learning_rate': 2.4998650550779706e-05, 'epoch': 0.21}\n",
      "{'loss': 3.6931, 'grad_norm': 35.67607879638672, 'learning_rate': 2.4974668446939085e-05, 'epoch': 0.32}\n",
      "{'loss': 3.404, 'grad_norm': 71.1805648803711, 'learning_rate': 2.492213538377007e-05, 'epoch': 0.43}\n",
      "{'loss': 3.0658, 'grad_norm': 49.94474792480469, 'learning_rate': 2.4839033596862587e-05, 'epoch': 0.53}\n",
      "{'loss': 2.9879, 'grad_norm': 37.24433135986328, 'learning_rate': 2.4726335596276483e-05, 'epoch': 0.64}\n",
      "{'loss': 2.9229, 'grad_norm': 57.79120635986328, 'learning_rate': 2.4584311697679535e-05, 'epoch': 0.74}\n",
      "{'loss': 2.6167, 'grad_norm': 78.41902160644531, 'learning_rate': 2.441330255738144e-05, 'epoch': 0.85}\n",
      "{'loss': 2.6876, 'grad_norm': 49.31616973876953, 'learning_rate': 2.4213718355240903e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 2/60 [00:05<02:35,  2.68s/it]\u001b[A\n",
      "  5%|▌         | 3/60 [00:05<01:42,  1.81s/it]\u001b[A\n",
      "  7%|▋         | 4/60 [00:11<03:04,  3.29s/it]\u001b[A\n",
      "  8%|▊         | 5/60 [00:12<02:08,  2.33s/it]\u001b[A\n",
      " 10%|█         | 6/60 [00:18<03:10,  3.52s/it]\u001b[A\n",
      " 12%|█▏        | 7/60 [00:18<02:15,  2.56s/it]\u001b[A\n",
      " 13%|█▎        | 8/60 [00:25<03:14,  3.74s/it]\u001b[A\n",
      " 15%|█▌        | 9/60 [00:25<02:19,  2.74s/it]\u001b[A\n",
      " 17%|█▋        | 10/60 [00:30<02:55,  3.52s/it]\u001b[A\n",
      " 18%|█▊        | 11/60 [00:31<02:07,  2.60s/it]\u001b[A\n",
      " 20%|██        | 12/60 [00:36<02:47,  3.48s/it]\u001b[A\n",
      " 22%|██▏       | 13/60 [00:37<02:01,  2.59s/it]\u001b[A\n",
      " 23%|██▎       | 14/60 [00:42<02:38,  3.45s/it]\u001b[A\n",
      " 25%|██▌       | 15/60 [00:43<01:55,  2.57s/it]\u001b[A\n",
      " 27%|██▋       | 16/60 [00:47<02:18,  3.14s/it]\u001b[A\n",
      " 28%|██▊       | 17/60 [00:49<01:57,  2.72s/it]\u001b[A\n",
      " 30%|███       | 18/60 [00:54<02:24,  3.43s/it]\u001b[A\n",
      " 32%|███▏      | 19/60 [00:55<01:45,  2.57s/it]\u001b[A\n",
      " 33%|███▎      | 20/60 [01:00<02:15,  3.39s/it]\u001b[A\n",
      " 35%|███▌      | 21/60 [01:00<01:38,  2.53s/it]\u001b[A\n",
      " 37%|███▋      | 22/60 [01:05<02:00,  3.16s/it]\u001b[A\n",
      " 38%|███▊      | 23/60 [01:06<01:36,  2.60s/it]\u001b[A\n",
      " 40%|████      | 24/60 [01:11<01:58,  3.30s/it]\u001b[A\n",
      " 42%|████▏     | 25/60 [01:13<01:34,  2.71s/it]\u001b[A\n",
      " 43%|████▎     | 26/60 [01:17<01:47,  3.16s/it]\u001b[A\n",
      " 45%|████▌     | 27/60 [01:19<01:32,  2.80s/it]\u001b[A\n",
      " 47%|████▋     | 28/60 [01:23<01:44,  3.26s/it]\u001b[A\n",
      " 48%|████▊     | 29/60 [01:25<01:25,  2.76s/it]\u001b[A\n",
      " 50%|█████     | 30/60 [01:30<01:44,  3.49s/it]\u001b[A\n",
      " 52%|█████▏    | 31/60 [01:32<01:28,  3.03s/it]\u001b[A\n",
      " 53%|█████▎    | 32/60 [01:36<01:35,  3.40s/it]\u001b[A\n",
      " 55%|█████▌    | 33/60 [01:38<01:16,  2.85s/it]\u001b[A\n",
      " 57%|█████▋    | 34/60 [01:43<01:32,  3.54s/it]\u001b[A\n",
      " 58%|█████▊    | 35/60 [01:44<01:12,  2.91s/it]\u001b[A\n",
      " 60%|██████    | 36/60 [01:49<01:23,  3.47s/it]\u001b[A\n",
      " 62%|██████▏   | 37/60 [01:51<01:05,  2.85s/it]\u001b[A\n",
      " 63%|██████▎   | 38/60 [01:55<01:10,  3.20s/it]\u001b[A\n",
      " 65%|██████▌   | 39/60 [01:56<00:55,  2.62s/it]\u001b[A\n",
      " 67%|██████▋   | 40/60 [02:01<01:05,  3.26s/it]\u001b[A\n",
      " 68%|██████▊   | 41/60 [02:02<00:50,  2.64s/it]\u001b[A\n",
      " 70%|███████   | 42/60 [02:05<00:53,  2.95s/it]\u001b[A\n",
      " 72%|███████▏  | 43/60 [02:07<00:41,  2.43s/it]\u001b[A\n",
      " 73%|███████▎  | 44/60 [02:11<00:50,  3.14s/it]\u001b[A\n",
      " 75%|███████▌  | 45/60 [02:13<00:41,  2.79s/it]\u001b[A\n",
      " 77%|███████▋  | 46/60 [02:17<00:44,  3.15s/it]\u001b[A\n",
      " 78%|███████▊  | 47/60 [02:20<00:40,  3.10s/it]\u001b[A\n",
      " 80%|████████  | 48/60 [02:25<00:40,  3.40s/it]\u001b[A\n",
      " 82%|████████▏ | 49/60 [02:26<00:32,  2.96s/it]\u001b[A\n",
      " 83%|████████▎ | 50/60 [02:30<00:31,  3.17s/it]\u001b[A\n",
      " 85%|████████▌ | 51/60 [02:34<00:29,  3.29s/it]\u001b[A\n",
      " 87%|████████▋ | 52/60 [02:37<00:26,  3.30s/it]\u001b[A\n",
      " 88%|████████▊ | 53/60 [02:41<00:23,  3.43s/it]\u001b[A\n",
      " 90%|█████████ | 54/60 [02:43<00:19,  3.24s/it]\u001b[A\n",
      " 92%|█████████▏| 55/60 [02:48<00:17,  3.53s/it]\u001b[A\n",
      " 93%|█████████▎| 56/60 [02:49<00:11,  2.99s/it]\u001b[A\n",
      " 95%|█████████▌| 57/60 [02:54<00:10,  3.46s/it]\u001b[A\n",
      " 97%|█████████▋| 58/60 [02:56<00:05,  2.98s/it]\u001b[A\n",
      " 98%|█████████▊| 59/60 [02:56<00:02,  2.24s/it]\u001b[A\n",
      "                                                    \r\n",
      " 14%|█▍        | 470/3290 [37:53<2:20:12,  2.98s/it]\n",
      "100%|██████████| 60/60 [02:57<00:00,  1.71s/it]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0768673419952393, 'eval_gender_acc': 0.9736842105263158, 'eval_gender_f1': 0.9736694666616827, 'eval_dialect_acc': 0.858421052631579, 'eval_dialect_f1': 0.8552395823285741, 'eval_combined_f1': 0.9144545244951284, 'eval_runtime': 185.9514, 'eval_samples_per_second': 10.218, 'eval_steps_per_second': 0.323, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 940/3290 [1:12:49<1:58:09,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3393, 'grad_norm': 39.56296157836914, 'learning_rate': 2.3986037810814606e-05, 'epoch': 1.06}\n",
      "{'loss': 2.456, 'grad_norm': 43.52970504760742, 'learning_rate': 2.3730807035107956e-05, 'epoch': 1.17}\n",
      "{'loss': 2.4044, 'grad_norm': 48.946380615234375, 'learning_rate': 2.3448638220681727e-05, 'epoch': 1.28}\n",
      "{'loss': 2.2339, 'grad_norm': 99.77123260498047, 'learning_rate': 2.3140208173256535e-05, 'epoch': 1.38}\n",
      "{'loss': 2.2635, 'grad_norm': 47.62440872192383, 'learning_rate': 2.280625668833718e-05, 'epoch': 1.49}\n",
      "{'loss': 2.3296, 'grad_norm': 50.39411163330078, 'learning_rate': 2.2447584776750668e-05, 'epoch': 1.6}\n",
      "{'loss': 2.4667, 'grad_norm': 41.63428497314453, 'learning_rate': 2.2065052743354155e-05, 'epoch': 1.7}\n",
      "{'loss': 2.3304, 'grad_norm': 41.143882751464844, 'learning_rate': 2.1659578123521075e-05, 'epoch': 1.81}\n",
      "{'loss': 2.2939, 'grad_norm': 74.77558898925781, 'learning_rate': 2.1232133482355153e-05, 'epoch': 1.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 2/60 [00:05<02:36,  2.70s/it]\u001b[A\n",
      "  5%|▌         | 3/60 [00:05<01:43,  1.81s/it]\u001b[A\n",
      "  7%|▋         | 4/60 [00:11<03:01,  3.24s/it]\u001b[A\n",
      "  8%|▊         | 5/60 [00:12<02:06,  2.29s/it]\u001b[A\n",
      " 10%|█         | 6/60 [00:18<03:11,  3.55s/it]\u001b[A\n",
      " 12%|█▏        | 7/60 [00:18<02:16,  2.58s/it]\u001b[A\n",
      " 13%|█▎        | 8/60 [00:25<03:20,  3.85s/it]\u001b[A\n",
      " 15%|█▌        | 9/60 [00:25<02:23,  2.82s/it]\u001b[A\n",
      " 17%|█▋        | 10/60 [00:31<03:02,  3.64s/it]\u001b[A\n",
      " 18%|█▊        | 11/60 [00:31<02:11,  2.69s/it]\u001b[A\n",
      " 20%|██        | 12/60 [00:37<02:51,  3.57s/it]\u001b[A\n",
      " 22%|██▏       | 13/60 [00:38<02:04,  2.65s/it]\u001b[A\n",
      " 23%|██▎       | 14/60 [00:43<02:43,  3.56s/it]\u001b[A\n",
      " 25%|██▌       | 15/60 [00:44<01:58,  2.64s/it]\u001b[A\n",
      " 27%|██▋       | 16/60 [00:48<02:20,  3.20s/it]\u001b[A\n",
      " 28%|██▊       | 17/60 [00:50<01:54,  2.66s/it]\u001b[A\n",
      " 30%|███       | 18/60 [00:55<02:27,  3.50s/it]\u001b[A\n",
      " 32%|███▏      | 19/60 [00:56<01:46,  2.61s/it]\u001b[A\n",
      " 33%|███▎      | 20/60 [01:01<02:16,  3.41s/it]\u001b[A\n",
      " 35%|███▌      | 21/60 [01:01<01:39,  2.54s/it]\u001b[A\n",
      " 37%|███▋      | 22/60 [01:06<02:01,  3.21s/it]\u001b[A\n",
      " 38%|███▊      | 23/60 [01:07<01:29,  2.43s/it]\u001b[A\n",
      " 40%|████      | 24/60 [01:12<02:01,  3.37s/it]\u001b[A\n",
      " 42%|████▏     | 25/60 [01:13<01:29,  2.54s/it]\u001b[A\n",
      " 43%|████▎     | 26/60 [01:18<01:50,  3.26s/it]\u001b[A\n",
      " 45%|████▌     | 27/60 [01:19<01:26,  2.62s/it]\u001b[A\n",
      " 47%|████▋     | 28/60 [01:24<01:48,  3.39s/it]\u001b[A\n",
      " 48%|████▊     | 29/60 [01:25<01:21,  2.63s/it]\u001b[A\n",
      " 50%|█████     | 30/60 [01:31<01:48,  3.63s/it]\u001b[A\n",
      " 52%|█████▏    | 31/60 [01:32<01:23,  2.89s/it]\u001b[A\n",
      " 53%|█████▎    | 32/60 [01:37<01:38,  3.52s/it]\u001b[A\n",
      " 55%|█████▌    | 33/60 [01:38<01:13,  2.71s/it]\u001b[A\n",
      " 57%|█████▋    | 34/60 [01:44<01:35,  3.68s/it]\u001b[A\n",
      " 58%|█████▊    | 35/60 [01:44<01:08,  2.74s/it]\u001b[A\n",
      " 60%|██████    | 36/60 [01:50<01:26,  3.59s/it]\u001b[A\n",
      " 62%|██████▏   | 37/60 [01:51<01:01,  2.67s/it]\u001b[A\n",
      " 63%|██████▎   | 38/60 [01:55<01:13,  3.33s/it]\u001b[A\n",
      " 65%|██████▌   | 39/60 [01:56<00:52,  2.49s/it]\u001b[A\n",
      " 67%|██████▋   | 40/60 [02:01<01:08,  3.41s/it]\u001b[A\n",
      " 68%|██████▊   | 41/60 [02:02<00:48,  2.54s/it]\u001b[A\n",
      " 70%|███████   | 42/60 [02:06<00:56,  3.13s/it]\u001b[A\n",
      " 72%|███████▏  | 43/60 [02:07<00:39,  2.34s/it]\u001b[A\n",
      " 73%|███████▎  | 44/60 [02:12<00:52,  3.25s/it]\u001b[A\n",
      " 75%|███████▌  | 45/60 [02:14<00:40,  2.69s/it]\u001b[A\n",
      " 77%|███████▋  | 46/60 [02:18<00:45,  3.26s/it]\u001b[A\n",
      " 78%|███████▊  | 47/60 [02:21<00:38,  2.98s/it]\u001b[A\n",
      " 80%|████████  | 48/60 [02:25<00:41,  3.47s/it]\u001b[A\n",
      " 82%|████████▏ | 49/60 [02:27<00:31,  2.85s/it]\u001b[A\n",
      " 83%|████████▎ | 50/60 [02:30<00:31,  3.13s/it]\u001b[A\n",
      " 85%|████████▌ | 51/60 [02:34<00:28,  3.13s/it]\u001b[A\n",
      " 87%|████████▋ | 52/60 [02:37<00:25,  3.23s/it]\u001b[A\n",
      " 88%|████████▊ | 53/60 [02:40<00:22,  3.21s/it]\u001b[A\n",
      " 90%|█████████ | 54/60 [02:43<00:19,  3.17s/it]\u001b[A\n",
      " 92%|█████████▏| 55/60 [02:47<00:16,  3.34s/it]\u001b[A\n",
      " 93%|█████████▎| 56/60 [02:49<00:11,  2.93s/it]\u001b[A\n",
      " 95%|█████████▌| 57/60 [02:53<00:10,  3.35s/it]\u001b[A\n",
      " 97%|█████████▋| 58/60 [02:55<00:05,  2.95s/it]\u001b[A\n",
      " 98%|█████████▊| 59/60 [02:56<00:02,  2.22s/it]\u001b[A\n",
      "                                                      \r\n",
      " 29%|██▊       | 940/3290 [1:15:55<1:58:09,  3.02s/it]\n",
      "100%|██████████| 60/60 [02:56<00:00,  1.69s/it]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.684180736541748, 'eval_gender_acc': 0.9842105263157894, 'eval_gender_f1': 0.9843118714732904, 'eval_dialect_acc': 0.8821052631578947, 'eval_dialect_f1': 0.8808847660122511, 'eval_combined_f1': 0.9325983187427707, 'eval_runtime': 185.5666, 'eval_samples_per_second': 10.239, 'eval_steps_per_second': 0.323, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1410/3290 [1:52:38<1:44:39,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1112, 'grad_norm': 65.75873565673828, 'learning_rate': 2.0783744081910857e-05, 'epoch': 2.02}\n",
      "{'loss': 1.9745, 'grad_norm': 78.1759033203125, 'learning_rate': 2.0315485422015834e-05, 'epoch': 2.13}\n",
      "{'loss': 1.9717, 'grad_norm': 51.36994171142578, 'learning_rate': 1.982848066059379e-05, 'epoch': 2.23}\n",
      "{'loss': 1.8775, 'grad_norm': 86.5916976928711, 'learning_rate': 1.9323897919675382e-05, 'epoch': 2.34}\n",
      "{'loss': 1.9339, 'grad_norm': 34.30186462402344, 'learning_rate': 1.8802947483558937e-05, 'epoch': 2.45}\n",
      "{'loss': 1.9601, 'grad_norm': 41.82738494873047, 'learning_rate': 1.8266878895841374e-05, 'epoch': 2.55}\n",
      "{'loss': 1.8985, 'grad_norm': 56.59455108642578, 'learning_rate': 1.771697796228245e-05, 'epoch': 2.66}\n",
      "{'loss': 2.0172, 'grad_norm': 82.27117919921875, 'learning_rate': 1.715456366669105e-05, 'epoch': 2.77}\n",
      "{'loss': 2.0014, 'grad_norm': 48.33686447143555, 'learning_rate': 1.658098500723122e-05, 'epoch': 2.87}\n",
      "{'loss': 1.8216, 'grad_norm': 35.508365631103516, 'learning_rate': 1.599761776073618e-05, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 2/60 [00:05<02:39,  2.74s/it]\u001b[A\n",
      "  5%|▌         | 3/60 [00:06<01:46,  1.87s/it]\u001b[A\n",
      "  7%|▋         | 4/60 [00:11<03:05,  3.32s/it]\u001b[A\n",
      "  8%|▊         | 5/60 [00:12<02:08,  2.35s/it]\u001b[A\n",
      " 10%|█         | 6/60 [00:18<03:16,  3.64s/it]\u001b[A\n",
      " 12%|█▏        | 7/60 [00:19<02:19,  2.64s/it]\u001b[A\n",
      " 13%|█▎        | 8/60 [00:25<03:22,  3.90s/it]\u001b[A\n",
      " 15%|█▌        | 9/60 [00:26<02:25,  2.85s/it]\u001b[A\n",
      " 17%|█▋        | 10/60 [00:31<03:03,  3.67s/it]\u001b[A\n",
      " 18%|█▊        | 11/60 [00:32<02:12,  2.71s/it]\u001b[A\n",
      " 20%|██        | 12/60 [00:38<02:54,  3.63s/it]\u001b[A\n",
      " 22%|██▏       | 13/60 [00:38<02:06,  2.69s/it]\u001b[A\n",
      " 23%|██▎       | 14/60 [00:44<02:45,  3.59s/it]\u001b[A\n",
      " 25%|██▌       | 15/60 [00:44<02:00,  2.67s/it]\u001b[A\n",
      " 27%|██▋       | 16/60 [00:49<02:25,  3.30s/it]\u001b[A\n",
      " 28%|██▊       | 17/60 [00:51<02:01,  2.84s/it]\u001b[A\n",
      " 30%|███       | 18/60 [00:56<02:29,  3.57s/it]\u001b[A\n",
      " 32%|███▏      | 19/60 [00:57<01:48,  2.65s/it]\u001b[A\n",
      " 33%|███▎      | 20/60 [01:02<02:19,  3.48s/it]\u001b[A\n",
      " 35%|███▌      | 21/60 [01:03<01:41,  2.59s/it]\u001b[A\n",
      " 37%|███▋      | 22/60 [01:08<02:05,  3.30s/it]\u001b[A\n",
      " 38%|███▊      | 23/60 [01:09<01:39,  2.70s/it]\u001b[A\n",
      " 40%|████      | 24/60 [01:14<02:01,  3.38s/it]\u001b[A\n",
      " 42%|████▏     | 25/60 [01:15<01:35,  2.74s/it]\u001b[A\n",
      " 43%|████▎     | 26/60 [01:19<01:48,  3.20s/it]\u001b[A\n",
      " 45%|████▌     | 27/60 [01:21<01:33,  2.84s/it]\u001b[A\n",
      " 47%|████▋     | 28/60 [01:26<01:46,  3.34s/it]\u001b[A\n",
      " 48%|████▊     | 29/60 [01:27<01:26,  2.78s/it]\u001b[A\n",
      " 50%|█████     | 30/60 [01:33<01:48,  3.61s/it]\u001b[A\n",
      " 52%|█████▏    | 31/60 [01:35<01:30,  3.11s/it]\u001b[A\n",
      " 53%|█████▎    | 32/60 [01:39<01:37,  3.48s/it]\u001b[A\n",
      " 55%|█████▌    | 33/60 [01:41<01:19,  2.96s/it]\u001b[A\n",
      " 57%|█████▋    | 34/60 [01:46<01:34,  3.65s/it]\u001b[A\n",
      " 58%|█████▊    | 35/60 [01:48<01:14,  2.96s/it]\u001b[A\n",
      " 60%|██████    | 36/60 [01:53<01:26,  3.61s/it]\u001b[A\n",
      " 62%|██████▏   | 37/60 [01:54<01:05,  2.84s/it]\u001b[A\n",
      " 63%|██████▎   | 38/60 [01:58<01:12,  3.31s/it]\u001b[A\n",
      " 65%|██████▌   | 39/60 [01:59<00:54,  2.59s/it]\u001b[A\n",
      " 67%|██████▋   | 40/60 [02:04<01:07,  3.35s/it]\u001b[A\n",
      " 68%|██████▊   | 41/60 [02:05<00:50,  2.64s/it]\u001b[A\n",
      " 70%|███████   | 42/60 [02:09<00:54,  3.03s/it]\u001b[A\n",
      " 72%|███████▏  | 43/60 [02:10<00:41,  2.45s/it]\u001b[A\n",
      " 73%|███████▎  | 44/60 [02:15<00:52,  3.27s/it]\u001b[A\n",
      " 75%|███████▌  | 45/60 [02:17<00:42,  2.82s/it]\u001b[A\n",
      " 77%|███████▋  | 46/60 [02:21<00:46,  3.30s/it]\u001b[A\n",
      " 78%|███████▊  | 47/60 [02:24<00:40,  3.12s/it]\u001b[A\n",
      " 80%|████████  | 48/60 [02:29<00:41,  3.48s/it]\u001b[A\n",
      " 82%|████████▏ | 49/60 [02:30<00:32,  2.97s/it]\u001b[A\n",
      " 83%|████████▎ | 50/60 [02:34<00:31,  3.15s/it]\u001b[A\n",
      " 85%|████████▌ | 51/60 [02:37<00:29,  3.25s/it]\u001b[A\n",
      " 87%|████████▋ | 52/60 [02:41<00:26,  3.29s/it]\u001b[A\n",
      " 88%|████████▊ | 53/60 [02:44<00:23,  3.30s/it]\u001b[A\n",
      " 90%|█████████ | 54/60 [02:47<00:19,  3.29s/it]\u001b[A\n",
      " 92%|█████████▏| 55/60 [02:51<00:17,  3.44s/it]\u001b[A\n",
      " 93%|█████████▎| 56/60 [02:53<00:12,  3.07s/it]\u001b[A\n",
      " 95%|█████████▌| 57/60 [02:57<00:10,  3.37s/it]\u001b[A\n",
      " 97%|█████████▋| 58/60 [03:00<00:06,  3.13s/it]\u001b[A\n",
      " 98%|█████████▊| 59/60 [03:00<00:02,  2.35s/it]\u001b[A\n",
      "                                                       \r\n",
      " 43%|████▎     | 1410/3290 [1:55:48<1:44:39,  3.34s/it]\n",
      "100%|██████████| 60/60 [03:01<00:00,  1.78s/it]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.512199878692627, 'eval_gender_acc': 0.9852631578947368, 'eval_gender_f1': 0.9853501753668443, 'eval_dialect_acc': 0.8936842105263157, 'eval_dialect_f1': 0.8923155854925037, 'eval_combined_f1': 0.9388328804296739, 'eval_runtime': 190.0232, 'eval_samples_per_second': 9.999, 'eval_steps_per_second': 0.316, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 1880/3290 [2:32:55<1:12:19,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7104, 'grad_norm': 32.14581298828125, 'learning_rate': 1.5405861182791456e-05, 'epoch': 3.09}\n",
      "{'loss': 1.6012, 'grad_norm': 44.917972564697266, 'learning_rate': 1.4807134651502233e-05, 'epoch': 3.19}\n",
      "{'loss': 1.6336, 'grad_norm': 61.033695220947266, 'learning_rate': 1.4202874262995166e-05, 'epoch': 3.3}\n",
      "{'loss': 1.6759, 'grad_norm': 63.176734924316406, 'learning_rate': 1.3594529386820585e-05, 'epoch': 3.4}\n",
      "{'loss': 1.7203, 'grad_norm': 38.591773986816406, 'learning_rate': 1.2983559189517283e-05, 'epoch': 3.51}\n",
      "{'loss': 1.5679, 'grad_norm': 25.86754608154297, 'learning_rate': 1.2371429134678385e-05, 'epoch': 3.62}\n",
      "{'loss': 1.7218, 'grad_norm': 61.36302185058594, 'learning_rate': 1.1759607467913244e-05, 'epoch': 3.72}\n",
      "{'loss': 1.6331, 'grad_norm': 51.337738037109375, 'learning_rate': 1.114956169513637e-05, 'epoch': 3.83}\n",
      "{'loss': 1.5765, 'grad_norm': 25.356748580932617, 'learning_rate': 1.0542755062630635e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 2/60 [00:05<02:50,  2.94s/it]\u001b[A\n",
      "  5%|▌         | 3/60 [00:06<01:50,  1.94s/it]\u001b[A\n",
      "  7%|▋         | 4/60 [00:12<03:18,  3.55s/it]\u001b[A\n",
      "  8%|▊         | 5/60 [00:13<02:17,  2.49s/it]\u001b[A\n",
      " 10%|█         | 6/60 [00:19<03:29,  3.88s/it]\u001b[A\n",
      " 12%|█▏        | 7/60 [00:20<02:28,  2.80s/it]\u001b[A\n",
      " 13%|█▎        | 8/60 [00:27<03:36,  4.16s/it]\u001b[A\n",
      " 15%|█▌        | 9/60 [00:28<02:34,  3.03s/it]\u001b[A\n",
      " 17%|█▋        | 10/60 [00:33<03:16,  3.93s/it]\u001b[A\n",
      " 18%|█▊        | 11/60 [00:34<02:21,  2.89s/it]\u001b[A\n",
      " 20%|██        | 12/60 [00:40<03:04,  3.85s/it]\u001b[A\n",
      " 22%|██▏       | 13/60 [00:41<02:13,  2.84s/it]\u001b[A\n",
      " 23%|██▎       | 14/60 [00:47<02:55,  3.83s/it]\u001b[A\n",
      " 25%|██▌       | 15/60 [00:47<02:07,  2.83s/it]\u001b[A\n",
      " 27%|██▋       | 16/60 [00:52<02:32,  3.48s/it]\u001b[A\n",
      " 28%|██▊       | 17/60 [00:53<01:54,  2.66s/it]\u001b[A\n",
      " 30%|███       | 18/60 [00:59<02:40,  3.83s/it]\u001b[A\n",
      " 32%|███▏      | 19/60 [01:00<01:56,  2.84s/it]\u001b[A\n",
      " 33%|███▎      | 20/60 [01:06<02:28,  3.70s/it]\u001b[A\n",
      " 35%|███▌      | 21/60 [01:06<01:47,  2.75s/it]\u001b[A\n",
      " 37%|███▋      | 22/60 [01:11<02:11,  3.46s/it]\u001b[A\n",
      " 38%|███▊      | 23/60 [01:12<01:35,  2.58s/it]\u001b[A\n",
      " 40%|████      | 24/60 [01:18<02:11,  3.65s/it]\u001b[A\n",
      " 42%|████▏     | 25/60 [01:19<01:34,  2.71s/it]\u001b[A\n",
      " 43%|████▎     | 26/60 [01:24<01:57,  3.46s/it]\u001b[A\n",
      " 45%|████▌     | 27/60 [01:25<01:27,  2.65s/it]\u001b[A\n",
      " 47%|████▋     | 28/60 [01:31<01:57,  3.66s/it]\u001b[A\n",
      " 48%|████▊     | 29/60 [01:31<01:24,  2.72s/it]\u001b[A\n",
      " 50%|█████     | 30/60 [01:38<01:56,  3.87s/it]\u001b[A\n",
      " 52%|█████▏    | 31/60 [01:38<01:23,  2.89s/it]\u001b[A\n",
      " 53%|█████▎    | 32/60 [01:44<01:49,  3.90s/it]\u001b[A\n",
      " 55%|█████▌    | 33/60 [01:45<01:17,  2.89s/it]\u001b[A\n",
      " 57%|█████▋    | 34/60 [01:52<01:44,  4.01s/it]\u001b[A\n",
      " 58%|█████▊    | 35/60 [01:52<01:14,  2.96s/it]\u001b[A\n",
      " 60%|██████    | 36/60 [01:58<01:33,  3.90s/it]\u001b[A\n",
      " 62%|██████▏   | 37/60 [01:59<01:06,  2.89s/it]\u001b[A\n",
      " 63%|██████▎   | 38/60 [02:04<01:18,  3.55s/it]\u001b[A\n",
      " 65%|██████▌   | 39/60 [02:04<00:55,  2.64s/it]\u001b[A\n",
      " 67%|██████▋   | 40/60 [02:10<01:12,  3.60s/it]\u001b[A\n",
      " 68%|██████▊   | 41/60 [02:11<00:50,  2.68s/it]\u001b[A\n",
      " 70%|███████   | 42/60 [02:15<00:58,  3.24s/it]\u001b[A\n",
      " 72%|███████▏  | 43/60 [02:16<00:41,  2.43s/it]\u001b[A\n",
      " 73%|███████▎  | 44/60 [02:22<00:54,  3.42s/it]\u001b[A\n",
      " 75%|███████▌  | 45/60 [02:22<00:38,  2.55s/it]\u001b[A\n",
      " 77%|███████▋  | 46/60 [02:28<00:49,  3.52s/it]\u001b[A\n",
      " 78%|███████▊  | 47/60 [02:29<00:37,  2.88s/it]\u001b[A\n",
      " 80%|████████  | 48/60 [02:35<00:45,  3.79s/it]\u001b[A\n",
      " 82%|████████▏ | 49/60 [02:36<00:30,  2.81s/it]\u001b[A\n",
      " 83%|████████▎ | 50/60 [02:41<00:35,  3.52s/it]\u001b[A\n",
      " 85%|████████▌ | 51/60 [02:43<00:27,  3.03s/it]\u001b[A\n",
      " 87%|████████▋ | 52/60 [02:48<00:29,  3.73s/it]\u001b[A\n",
      " 88%|████████▊ | 53/60 [02:50<00:22,  3.15s/it]\u001b[A\n",
      " 90%|█████████ | 54/60 [02:55<00:22,  3.70s/it]\u001b[A\n",
      " 92%|█████████▏| 55/60 [02:57<00:16,  3.27s/it]\u001b[A\n",
      " 93%|█████████▎| 56/60 [03:01<00:13,  3.48s/it]\u001b[A\n",
      " 95%|█████████▌| 57/60 [03:04<00:09,  3.20s/it]\u001b[A\n",
      " 97%|█████████▋| 58/60 [03:08<00:07,  3.51s/it]\u001b[A\n",
      " 98%|█████████▊| 59/60 [03:08<00:02,  2.61s/it]\u001b[A\n",
      "                                                       \r\n",
      " 57%|█████▋    | 1880/3290 [2:36:14<1:12:19,  3.08s/it]\n",
      "100%|██████████| 60/60 [03:09<00:00,  1.98s/it]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6811233758926392, 'eval_gender_acc': 0.9863157894736843, 'eval_gender_f1': 0.9863679494889995, 'eval_dialect_acc': 0.8836842105263157, 'eval_dialect_f1': 0.8831696531585408, 'eval_combined_f1': 0.9347688013237702, 'eval_runtime': 198.3679, 'eval_samples_per_second': 9.578, 'eval_steps_per_second': 0.302, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 2350/3290 [3:14:19<45:06,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6101, 'grad_norm': 59.60865020751953, 'learning_rate': 9.940643047327543e-06, 'epoch': 4.04}\n",
      "{'loss': 1.4636, 'grad_norm': 70.97894287109375, 'learning_rate': 9.34466986572293e-06, 'epoch': 4.15}\n",
      "{'loss': 1.5151, 'grad_norm': 85.99413299560547, 'learning_rate': 8.75626500980182e-06, 'epoch': 4.26}\n",
      "{'loss': 1.5369, 'grad_norm': 37.5247917175293, 'learning_rate': 8.176839818281184e-06, 'epoch': 4.36}\n",
      "{'loss': 1.3764, 'grad_norm': 56.61595916748047, 'learning_rate': 7.6077840913949285e-06, 'epoch': 4.47}\n",
      "{'loss': 1.5098, 'grad_norm': 81.54926300048828, 'learning_rate': 7.050462757340689e-06, 'epoch': 4.57}\n",
      "{'loss': 1.4424, 'grad_norm': 31.634504318237305, 'learning_rate': 6.506212598384339e-06, 'epoch': 4.68}\n",
      "{'loss': 1.3335, 'grad_norm': 51.3825798034668, 'learning_rate': 5.976339044474887e-06, 'epoch': 4.79}\n",
      "{'loss': 1.416, 'grad_norm': 33.12735366821289, 'learning_rate': 5.462113042060532e-06, 'epoch': 4.89}\n",
      "{'loss': 1.3081, 'grad_norm': 17.4018497467041, 'learning_rate': 4.974541687417292e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 2/60 [00:05<02:41,  2.78s/it]\u001b[A\n",
      "  5%|▌         | 3/60 [00:06<01:44,  1.84s/it]\u001b[A\n",
      "  7%|▋         | 4/60 [00:12<03:11,  3.42s/it]\u001b[A\n",
      "  8%|▊         | 5/60 [00:12<02:12,  2.41s/it]\u001b[A\n",
      " 10%|█         | 6/60 [00:19<03:22,  3.76s/it]\u001b[A\n",
      " 12%|█▏        | 7/60 [00:19<02:24,  2.72s/it]\u001b[A\n",
      " 13%|█▎        | 8/60 [00:26<03:29,  4.02s/it]\u001b[A\n",
      " 15%|█▌        | 9/60 [00:27<02:29,  2.94s/it]\u001b[A\n",
      " 17%|█▋        | 10/60 [00:32<03:08,  3.76s/it]\u001b[A\n",
      " 18%|█▊        | 11/60 [00:33<02:15,  2.77s/it]\u001b[A\n",
      " 20%|██        | 12/60 [00:39<02:58,  3.72s/it]\u001b[A\n",
      " 22%|██▏       | 13/60 [00:39<02:09,  2.75s/it]\u001b[A\n",
      " 23%|██▎       | 14/60 [00:45<02:49,  3.69s/it]\u001b[A\n",
      " 25%|██▌       | 15/60 [00:46<02:03,  2.74s/it]\u001b[A\n",
      " 27%|██▋       | 16/60 [00:50<02:30,  3.41s/it]\u001b[A\n",
      " 28%|██▊       | 17/60 [00:52<01:56,  2.70s/it]\u001b[A\n",
      " 30%|███       | 18/60 [00:58<02:38,  3.77s/it]\u001b[A\n",
      " 32%|███▏      | 19/60 [00:58<01:54,  2.79s/it]\u001b[A\n",
      " 33%|███▎      | 20/60 [01:04<02:26,  3.66s/it]\u001b[A\n",
      " 35%|███▌      | 21/60 [01:05<01:45,  2.72s/it]\u001b[A\n",
      " 37%|███▋      | 22/60 [01:10<02:10,  3.43s/it]\u001b[A\n",
      " 38%|███▊      | 23/60 [01:10<01:34,  2.56s/it]\u001b[A\n",
      " 40%|████      | 24/60 [01:16<02:10,  3.62s/it]\u001b[A\n",
      " 42%|████▏     | 25/60 [01:17<01:34,  2.69s/it]\u001b[A\n",
      " 43%|████▎     | 26/60 [01:22<01:56,  3.42s/it]\u001b[A\n",
      " 45%|████▌     | 27/60 [01:23<01:26,  2.63s/it]\u001b[A\n",
      " 47%|████▋     | 28/60 [01:29<01:56,  3.64s/it]\u001b[A\n",
      " 48%|████▊     | 29/60 [01:29<01:23,  2.71s/it]\u001b[A\n",
      " 50%|█████     | 30/60 [01:36<01:57,  3.93s/it]\u001b[A\n",
      " 52%|█████▏    | 31/60 [01:36<01:24,  2.90s/it]\u001b[A\n",
      " 53%|█████▎    | 32/60 [01:43<01:48,  3.87s/it]\u001b[A\n",
      " 55%|█████▌    | 33/60 [01:43<01:17,  2.87s/it]\u001b[A\n",
      " 57%|█████▋    | 34/60 [01:50<01:44,  4.03s/it]\u001b[A\n",
      " 58%|█████▊    | 35/60 [01:50<01:14,  2.98s/it]\u001b[A\n",
      " 60%|██████    | 36/60 [01:56<01:33,  3.89s/it]\u001b[A\n",
      " 62%|██████▏   | 37/60 [01:57<01:06,  2.88s/it]\u001b[A\n",
      " 63%|██████▎   | 38/60 [02:02<01:18,  3.56s/it]\u001b[A\n",
      " 65%|██████▌   | 39/60 [02:03<00:55,  2.65s/it]\u001b[A\n",
      " 67%|██████▋   | 40/60 [02:08<01:12,  3.60s/it]\u001b[A\n",
      " 68%|██████▊   | 41/60 [02:09<00:50,  2.68s/it]\u001b[A\n",
      " 70%|███████   | 42/60 [02:14<00:58,  3.27s/it]\u001b[A\n",
      " 72%|███████▏  | 43/60 [02:14<00:41,  2.45s/it]\u001b[A\n",
      " 73%|███████▎  | 44/60 [02:20<00:55,  3.45s/it]\u001b[A\n",
      " 75%|███████▌  | 45/60 [02:20<00:38,  2.57s/it]\u001b[A\n",
      " 77%|███████▋  | 46/60 [02:26<00:49,  3.54s/it]\u001b[A\n",
      " 78%|███████▊  | 47/60 [02:27<00:34,  2.68s/it]\u001b[A\n",
      " 80%|████████  | 48/60 [02:34<00:46,  3.88s/it]\u001b[A\n",
      " 82%|████████▏ | 49/60 [02:34<00:31,  2.87s/it]\u001b[A\n",
      " 83%|████████▎ | 50/60 [02:39<00:35,  3.54s/it]\u001b[A\n",
      " 85%|████████▌ | 51/60 [02:40<00:25,  2.85s/it]\u001b[A\n",
      " 87%|████████▋ | 52/60 [02:46<00:30,  3.77s/it]\u001b[A\n",
      " 88%|████████▊ | 53/60 [02:47<00:20,  2.96s/it]\u001b[A\n",
      " 90%|█████████ | 54/60 [02:53<00:22,  3.78s/it]\u001b[A\n",
      " 92%|█████████▏| 55/60 [02:55<00:15,  3.11s/it]\u001b[A\n",
      " 93%|█████████▎| 56/60 [02:59<00:14,  3.57s/it]\u001b[A\n",
      " 95%|█████████▌| 57/60 [03:01<00:09,  3.04s/it]\u001b[A\n",
      " 97%|█████████▋| 58/60 [03:06<00:07,  3.59s/it]\u001b[A\n",
      " 98%|█████████▊| 59/60 [03:07<00:02,  2.67s/it]\u001b[A\n",
      "                                                     \r\n",
      " 71%|███████▏  | 2350/3290 [3:17:35<45:06,  2.88s/it]\n",
      "100%|██████████| 60/60 [03:07<00:00,  2.01s/it]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5483909845352173, 'eval_gender_acc': 0.9847368421052631, 'eval_gender_f1': 0.9848387083946571, 'eval_dialect_acc': 0.8973684210526316, 'eval_dialect_f1': 0.8965157373232197, 'eval_combined_f1': 0.9406772228589384, 'eval_runtime': 196.4919, 'eval_samples_per_second': 9.67, 'eval_steps_per_second': 0.305, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 2820/3290 [3:55:36<23:25,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2709, 'grad_norm': 49.86878967285156, 'learning_rate': 4.4948976483200335e-06, 'epoch': 5.11}\n",
      "{'loss': 1.3889, 'grad_norm': 39.893123626708984, 'learning_rate': 4.034454522962057e-06, 'epoch': 5.21}\n",
      "{'loss': 1.3146, 'grad_norm': 30.031240463256836, 'learning_rate': 3.5943167230477483e-06, 'epoch': 5.32}\n",
      "{'loss': 1.3762, 'grad_norm': 51.34257125854492, 'learning_rate': 3.1755399562443165e-06, 'epoch': 5.43}\n",
      "{'loss': 1.2938, 'grad_norm': 106.0869140625, 'learning_rate': 2.779128693977874e-06, 'epoch': 5.53}\n",
      "{'loss': 1.2768, 'grad_norm': 34.181495666503906, 'learning_rate': 2.4060337621239423e-06, 'epoch': 5.64}\n",
      "{'loss': 1.3098, 'grad_norm': 44.564456939697266, 'learning_rate': 2.057150060371282e-06, 'epoch': 5.74}\n",
      "{'loss': 1.237, 'grad_norm': 48.92115783691406, 'learning_rate': 1.7333144157294826e-06, 'epoch': 5.85}\n",
      "{'loss': 1.3611, 'grad_norm': 56.328147888183594, 'learning_rate': 1.4353035753287267e-06, 'epoch': 5.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 2/60 [00:05<02:42,  2.80s/it]\u001b[A\n",
      "  5%|▌         | 3/60 [00:06<01:45,  1.85s/it]\u001b[A\n",
      "  7%|▋         | 4/60 [00:12<03:10,  3.41s/it]\u001b[A\n",
      "  8%|▊         | 5/60 [00:12<02:12,  2.40s/it]\u001b[A\n",
      " 10%|█         | 6/60 [00:19<03:21,  3.74s/it]\u001b[A\n",
      " 12%|█▏        | 7/60 [00:19<02:23,  2.70s/it]\u001b[A\n",
      " 13%|█▎        | 8/60 [00:26<03:25,  3.95s/it]\u001b[A\n",
      " 15%|█▌        | 9/60 [00:26<02:27,  2.89s/it]\u001b[A\n",
      " 17%|█▋        | 10/60 [00:32<03:10,  3.80s/it]\u001b[A\n",
      " 18%|█▊        | 11/60 [00:33<02:17,  2.80s/it]\u001b[A\n",
      " 20%|██        | 12/60 [00:39<03:03,  3.82s/it]\u001b[A\n",
      " 22%|██▏       | 13/60 [00:39<02:12,  2.82s/it]\u001b[A\n",
      " 23%|██▎       | 14/60 [00:45<02:52,  3.76s/it]\u001b[A\n",
      " 25%|██▌       | 15/60 [00:46<02:05,  2.78s/it]\u001b[A\n",
      " 27%|██▋       | 16/60 [00:51<02:29,  3.40s/it]\u001b[A\n",
      " 28%|██▊       | 17/60 [00:52<01:57,  2.74s/it]\u001b[A\n",
      " 30%|███       | 18/60 [00:58<02:37,  3.74s/it]\u001b[A\n",
      " 32%|███▏      | 19/60 [00:58<01:53,  2.77s/it]\u001b[A\n",
      " 33%|███▎      | 20/60 [01:04<02:24,  3.62s/it]\u001b[A\n",
      " 35%|███▌      | 21/60 [01:05<01:44,  2.69s/it]\u001b[A\n",
      " 37%|███▋      | 22/60 [01:09<02:07,  3.36s/it]\u001b[A\n",
      " 38%|███▊      | 23/60 [01:10<01:32,  2.51s/it]\u001b[A\n",
      " 40%|████      | 24/60 [01:16<02:07,  3.53s/it]\u001b[A\n",
      " 42%|████▏     | 25/60 [01:16<01:32,  2.63s/it]\u001b[A\n",
      " 43%|████▎     | 26/60 [01:22<01:54,  3.37s/it]\u001b[A\n",
      " 45%|████▌     | 27/60 [01:23<01:29,  2.71s/it]\u001b[A\n",
      " 47%|████▋     | 28/60 [01:28<01:54,  3.58s/it]\u001b[A\n",
      " 48%|████▊     | 29/60 [01:29<01:23,  2.68s/it]\u001b[A\n",
      " 50%|█████     | 30/60 [01:35<01:54,  3.81s/it]\u001b[A\n",
      " 52%|█████▏    | 31/60 [01:36<01:26,  3.00s/it]\u001b[A\n",
      " 53%|█████▎    | 32/60 [01:42<01:44,  3.75s/it]\u001b[A\n",
      " 55%|█████▌    | 33/60 [01:42<01:15,  2.78s/it]\u001b[A\n",
      " 57%|█████▋    | 34/60 [01:49<01:41,  3.91s/it]\u001b[A\n",
      " 58%|█████▊    | 35/60 [01:50<01:12,  2.89s/it]\u001b[A\n",
      " 60%|██████    | 36/60 [01:56<01:32,  3.83s/it]\u001b[A\n",
      " 62%|██████▏   | 37/60 [01:56<01:05,  2.84s/it]\u001b[A\n",
      " 63%|██████▎   | 38/60 [02:01<01:17,  3.51s/it]\u001b[A\n",
      " 65%|██████▌   | 39/60 [02:02<00:54,  2.61s/it]\u001b[A\n",
      " 67%|██████▋   | 40/60 [02:07<01:11,  3.57s/it]\u001b[A\n",
      " 68%|██████▊   | 41/60 [02:08<00:50,  2.66s/it]\u001b[A\n",
      " 70%|███████   | 42/60 [02:12<00:57,  3.20s/it]\u001b[A\n",
      " 72%|███████▏  | 43/60 [02:13<00:40,  2.39s/it]\u001b[A\n",
      " 73%|███████▎  | 44/60 [02:19<00:53,  3.36s/it]\u001b[A\n",
      " 75%|███████▌  | 45/60 [02:20<00:40,  2.70s/it]\u001b[A\n",
      " 77%|███████▋  | 46/60 [02:25<00:47,  3.40s/it]\u001b[A\n",
      " 78%|███████▊  | 47/60 [02:27<00:39,  3.05s/it]\u001b[A\n",
      " 80%|████████  | 48/60 [02:32<00:43,  3.62s/it]\u001b[A\n",
      " 82%|████████▏ | 49/60 [02:33<00:32,  2.95s/it]\u001b[A\n",
      " 83%|████████▎ | 50/60 [02:38<00:33,  3.34s/it]\u001b[A\n",
      " 85%|████████▌ | 51/60 [02:41<00:29,  3.23s/it]\u001b[A\n",
      " 87%|████████▋ | 52/60 [02:44<00:27,  3.40s/it]\u001b[A\n",
      " 88%|████████▊ | 53/60 [02:48<00:23,  3.35s/it]\u001b[A\n",
      " 90%|█████████ | 54/60 [02:51<00:20,  3.38s/it]\u001b[A\n",
      " 92%|█████████▏| 55/60 [02:55<00:17,  3.43s/it]\u001b[A\n",
      " 93%|█████████▎| 56/60 [02:57<00:12,  3.11s/it]\u001b[A\n",
      " 95%|█████████▌| 57/60 [03:01<00:10,  3.38s/it]\u001b[A\n",
      " 97%|█████████▋| 58/60 [03:03<00:06,  3.11s/it]\u001b[A\n",
      " 98%|█████████▊| 59/60 [03:04<00:02,  2.33s/it]\u001b[A\n",
      "                                                     \r\n",
      " 86%|████████▌ | 2820/3290 [3:58:50<23:25,  2.99s/it]\n",
      "100%|██████████| 60/60 [03:05<00:00,  1.78s/it]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4843757152557373, 'eval_gender_acc': 0.9868421052631579, 'eval_gender_f1': 0.9869026809296335, 'eval_dialect_acc': 0.9036842105263158, 'eval_dialect_f1': 0.9028211502301782, 'eval_combined_f1': 0.9448619155799058, 'eval_runtime': 193.5662, 'eval_samples_per_second': 9.816, 'eval_steps_per_second': 0.31, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3290/3290 [4:36:34<00:00,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2814, 'grad_norm': 40.28841781616211, 'learning_rate': 1.163832343326228e-06, 'epoch': 6.06}\n",
      "{'loss': 1.2363, 'grad_norm': 77.25232696533203, 'learning_rate': 9.195518663881228e-07, 'epoch': 6.17}\n",
      "{'loss': 1.2474, 'grad_norm': 60.532569885253906, 'learning_rate': 7.030480718592274e-07, 'epoch': 6.28}\n",
      "{'loss': 1.1666, 'grad_norm': 57.27878189086914, 'learning_rate': 5.148402623668308e-07, 'epoch': 6.38}\n",
      "{'loss': 1.2726, 'grad_norm': 68.025146484375, 'learning_rate': 3.553798702295205e-07, 'epoch': 6.49}\n",
      "{'loss': 1.292, 'grad_norm': 55.2930908203125, 'learning_rate': 2.2504937465864795e-07, 'epoch': 6.6}\n",
      "{'loss': 1.1934, 'grad_norm': 81.15133666992188, 'learning_rate': 1.241613843496553e-07, 'epoch': 6.7}\n",
      "{'loss': 1.3061, 'grad_norm': 67.3716049194336, 'learning_rate': 5.295788766373627e-08, 'epoch': 6.81}\n",
      "{'loss': 0.9671, 'grad_norm': 56.189754486083984, 'learning_rate': 1.1609672198317767e-08, 'epoch': 6.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 2/60 [00:05<02:52,  2.98s/it]\u001b[A\n",
      "  5%|▌         | 3/60 [00:06<01:53,  2.00s/it]\u001b[A\n",
      "  7%|▋         | 4/60 [00:13<03:25,  3.66s/it]\u001b[A\n",
      "  8%|▊         | 5/60 [00:13<02:21,  2.57s/it]\u001b[A\n",
      " 10%|█         | 6/60 [00:20<03:32,  3.93s/it]\u001b[A\n",
      " 12%|█▏        | 7/60 [00:20<02:30,  2.84s/it]\u001b[A\n",
      " 13%|█▎        | 8/60 [00:27<03:35,  4.14s/it]\u001b[A\n",
      " 15%|█▌        | 9/60 [00:28<02:34,  3.02s/it]\u001b[A\n",
      " 17%|█▋        | 10/60 [00:34<03:12,  3.85s/it]\u001b[A\n",
      " 18%|█▊        | 11/60 [00:34<02:19,  2.84s/it]\u001b[A\n",
      " 20%|██        | 12/60 [00:40<03:03,  3.81s/it]\u001b[A\n",
      " 22%|██▏       | 13/60 [00:41<02:12,  2.82s/it]\u001b[A\n",
      " 23%|██▎       | 14/60 [00:46<02:51,  3.74s/it]\u001b[A\n",
      " 25%|██▌       | 15/60 [00:47<02:04,  2.77s/it]\u001b[A\n",
      " 27%|██▋       | 16/60 [00:52<02:31,  3.44s/it]\u001b[A\n",
      " 28%|██▊       | 17/60 [00:53<02:01,  2.83s/it]\u001b[A\n",
      " 30%|███       | 18/60 [00:59<02:37,  3.75s/it]\u001b[A\n",
      " 32%|███▏      | 19/60 [01:00<01:54,  2.78s/it]\u001b[A\n",
      " 33%|███▎      | 20/60 [01:06<02:27,  3.70s/it]\u001b[A\n",
      " 35%|███▌      | 21/60 [01:06<01:47,  2.75s/it]\u001b[A\n",
      " 37%|███▋      | 22/60 [01:11<02:08,  3.39s/it]\u001b[A\n",
      " 38%|███▊      | 23/60 [01:12<01:35,  2.59s/it]\u001b[A\n",
      " 40%|████      | 24/60 [01:18<02:10,  3.62s/it]\u001b[A\n",
      " 42%|████▏     | 25/60 [01:18<01:34,  2.69s/it]\u001b[A\n",
      " 43%|████▎     | 26/60 [01:24<01:58,  3.47s/it]\u001b[A\n",
      " 45%|████▌     | 27/60 [01:25<01:32,  2.82s/it]\u001b[A\n",
      " 47%|████▋     | 28/60 [01:30<01:55,  3.61s/it]\u001b[A\n",
      " 48%|████▊     | 29/60 [01:31<01:25,  2.76s/it]\u001b[A\n",
      " 50%|█████     | 30/60 [01:38<01:55,  3.85s/it]\u001b[A\n",
      " 52%|█████▏    | 31/60 [01:39<01:30,  3.11s/it]\u001b[A\n",
      " 53%|█████▎    | 32/60 [01:44<01:44,  3.72s/it]\u001b[A\n",
      " 55%|█████▌    | 33/60 [01:45<01:19,  2.95s/it]\u001b[A\n",
      " 57%|█████▋    | 34/60 [01:51<01:39,  3.84s/it]\u001b[A\n",
      " 58%|█████▊    | 35/60 [01:52<01:15,  3.01s/it]\u001b[A\n",
      " 60%|██████    | 36/60 [01:58<01:29,  3.75s/it]\u001b[A\n",
      " 62%|██████▏   | 37/60 [01:59<01:08,  2.98s/it]\u001b[A\n",
      " 63%|██████▎   | 38/60 [02:03<01:14,  3.40s/it]\u001b[A\n",
      " 65%|██████▌   | 39/60 [02:04<00:57,  2.73s/it]\u001b[A\n",
      " 67%|██████▋   | 40/60 [02:10<01:08,  3.45s/it]\u001b[A\n",
      " 68%|██████▊   | 41/60 [02:11<00:53,  2.83s/it]\u001b[A\n",
      " 70%|███████   | 42/60 [02:15<00:55,  3.06s/it]\u001b[A\n",
      " 72%|███████▏  | 43/60 [02:16<00:44,  2.64s/it]\u001b[A\n",
      " 73%|███████▎  | 44/60 [02:21<00:52,  3.25s/it]\u001b[A\n",
      " 75%|███████▌  | 45/60 [02:23<00:45,  3.04s/it]\u001b[A\n",
      " 77%|███████▋  | 46/60 [02:27<00:45,  3.26s/it]\u001b[A\n",
      " 78%|███████▊  | 47/60 [02:31<00:43,  3.37s/it]\u001b[A\n",
      " 80%|████████  | 48/60 [02:35<00:41,  3.48s/it]\u001b[A\n",
      " 82%|████████▏ | 49/60 [02:37<00:35,  3.27s/it]\u001b[A\n",
      " 83%|████████▎ | 50/60 [02:40<00:31,  3.15s/it]\u001b[A\n",
      " 85%|████████▌ | 51/60 [02:45<00:31,  3.54s/it]\u001b[A\n",
      " 87%|████████▋ | 52/60 [02:47<00:26,  3.27s/it]\u001b[A\n",
      " 88%|████████▊ | 53/60 [02:52<00:25,  3.60s/it]\u001b[A\n",
      " 90%|█████████ | 54/60 [02:54<00:19,  3.20s/it]\u001b[A\n",
      " 92%|█████████▏| 55/60 [02:59<00:18,  3.77s/it]\u001b[A\n",
      " 93%|█████████▎| 56/60 [03:00<00:11,  2.95s/it]\u001b[A\n",
      " 95%|█████████▌| 57/60 [03:06<00:11,  3.72s/it]\u001b[A\n",
      " 97%|█████████▋| 58/60 [03:07<00:05,  2.95s/it]\u001b[A\n",
      " 98%|█████████▊| 59/60 [03:08<00:02,  2.31s/it]\u001b[A\n",
      "                                                     \r\n",
      "100%|██████████| 3290/3290 [4:39:52<00:00,  3.54s/it]\n",
      "100%|██████████| 60/60 [03:08<00:00,  1.76s/it]\u001b[A\n",
      "100%|██████████| 3290/3290 [4:39:56<00:00,  5.11s/it]\n",
      "2025-12-09 07:40:01 | INFO | Saving model to /kaggle/working/output_vimd_resume/best_model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5123103857040405, 'eval_gender_acc': 0.9857894736842105, 'eval_gender_f1': 0.9858474053363222, 'eval_dialect_acc': 0.8994736842105263, 'eval_dialect_f1': 0.8992039402777319, 'eval_combined_f1': 0.942525672807027, 'eval_runtime': 197.404, 'eval_samples_per_second': 9.625, 'eval_steps_per_second': 0.304, 'epoch': 7.0}\n",
      "{'train_runtime': 16796.2074, 'train_samples_per_second': 6.261, 'train_steps_per_second': 0.196, 'train_loss': 1.914709313807154, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [02:47<00:00,  2.79s/it]\n",
      "wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "wandb: WARNING Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n",
      "2025-12-09 07:42:57 | INFO | WandB run: https://wandb.ai/vuthanhlam848-vnpost/vimd-speaker-profiling/runs/nplfjz1y\n",
      "2025-12-09 07:42:57 | INFO | Training completed!\n",
      "wandb: uploading best_model/model.safetensors\n",
      "wandb:                                                                                \n",
      "wandb: \n",
      "wandb: Run history:\n",
      "wandb:                         epoch ▁▁▁▁▂▂▂▂▂▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇████\n",
      "wandb:              eval_combined_f1 ▁▅▇▆▇██\n",
      "wandb:              eval_dialect_acc ▁▅▆▅▇██\n",
      "wandb:               eval_dialect_f1 ▁▅▆▅▇██\n",
      "wandb:                    eval_epoch ▁▂▃▅▆▇█\n",
      "wandb:               eval_gender_acc ▁▇▇█▇██\n",
      "wandb:                eval_gender_f1 ▁▇▇█▇██\n",
      "wandb:                     eval_loss █▃▁▃▂▁▁\n",
      "wandb:                  eval_runtime ▄▄▅█▇▇▁\n",
      "wandb:       eval_samples_per_second ▅▅▃▁▂▂█\n",
      "wandb:         eval_steps_per_second ▅▅▃▁▂▂█\n",
      "wandb:                   final_epoch ▁\n",
      "wandb:        final_eval_combined_f1 ▁\n",
      "wandb:        final_eval_dialect_acc ▁\n",
      "wandb:         final_eval_dialect_f1 ▁\n",
      "wandb:         final_eval_gender_acc ▁\n",
      "wandb:          final_eval_gender_f1 ▁\n",
      "wandb:               final_eval_loss ▁\n",
      "wandb:            final_eval_runtime ▁\n",
      "wandb: final_eval_samples_per_second ▁\n",
      "wandb:   final_eval_steps_per_second ▁\n",
      "wandb:                     grad_norm ▁▁▄▃▆▄▄▅▆▄▃▃▄▆▃▅▅▃▂▅▂▅▅▇▃▆▃▄▃▂▃▃█▄▄▆▅▄▄▄\n",
      "wandb:                 learning_rate ▁▅█████▇▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁\n",
      "wandb:                          loss ██▅▄▃▃▂▂▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "wandb:                    total_flos ▁\n",
      "wandb:                    train_loss ▁\n",
      "wandb:                 train_runtime ▁\n",
      "wandb:      train_samples_per_second ▁\n",
      "wandb:        train_steps_per_second ▁\n",
      "wandb: \n",
      "wandb: Run summary:\n",
      "wandb:                         epoch 7\n",
      "wandb:              eval_combined_f1 0.94486\n",
      "wandb:              eval_dialect_acc 0.90368\n",
      "wandb:               eval_dialect_f1 0.90282\n",
      "wandb:                    eval_epoch 7\n",
      "wandb:               eval_gender_acc 0.98684\n",
      "wandb:                eval_gender_f1 0.9869\n",
      "wandb:                     eval_loss 1.48438\n",
      "wandb:                  eval_runtime 175.3657\n",
      "wandb:       eval_samples_per_second 10.834\n",
      "wandb:         eval_steps_per_second 0.342\n",
      "wandb:                   final_epoch 7\n",
      "wandb:        final_eval_combined_f1 0.94486\n",
      "wandb:        final_eval_dialect_acc 0.90368\n",
      "wandb:         final_eval_dialect_f1 0.90282\n",
      "wandb:         final_eval_gender_acc 0.98684\n",
      "wandb:          final_eval_gender_f1 0.9869\n",
      "wandb:               final_eval_loss 1.48438\n",
      "wandb:            final_eval_runtime 175.3657\n",
      "wandb: final_eval_samples_per_second 10.834\n",
      "wandb:   final_eval_steps_per_second 0.342\n",
      "wandb:                     grad_norm 56.18975\n",
      "wandb:                 learning_rate 0.0\n",
      "wandb:                          loss 0.9671\n",
      "wandb:                    total_flos 0\n",
      "wandb:                    train_loss 1.91471\n",
      "wandb:                 train_runtime 16796.2074\n",
      "wandb:      train_samples_per_second 6.261\n",
      "wandb:        train_steps_per_second 0.196\n",
      "wandb: \n",
      "wandb: 🚀 View run PhoWhisper-base-resume at: https://wandb.ai/vuthanhlam848-vnpost/vimd-speaker-profiling/runs/nplfjz1y\n",
      "wandb: ⭐️ View project at: https://wandb.ai/vuthanhlam848-vnpost/vimd-speaker-profiling\n",
      "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
      "wandb: Find logs at: ./wandb/run-20251209_024519-nplfjz1y/logs\n",
      "2025-12-09 07:43:10,578 - INFO - Resume training completed successfully!\n",
      "2025-12-09 07:43:10,788 - INFO - Resumed model saved to: /kaggle/working/final_model_vimd/best_model_resumed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# RESUME TRAINING (TRAIN THÊM)\n",
    "# ============================================================\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"training\")\n",
    "\n",
    "ENCODER = \"vinai/PhoWhisper-base\"\n",
    "encoder_short = ENCODER.split(\"/\")[-1]\n",
    "WANDB_API_KEY = \"f05e29c3466ec288e97041e0e3d541c4087096a6\"\n",
    "\n",
    "# Checkpoint để resume\n",
    "CHECKPOINT_DIR = \"/kaggle/input/phowhisper-vimd/output_vimd_resume/checkpoint-8460\"\n",
    "\n",
    "# Số epoch train thêm\n",
    "ADDITIONAL_EPOCHS = 7\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"RESUME TRAINING\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    logger.error(f\"Checkpoint not found: {CHECKPOINT_DIR}\")\n",
    "else:\n",
    "    # Tạo config mới với resume\n",
    "    resume_config = f\"\"\"\n",
    "model:\n",
    "  name: \"{ENCODER}\"\n",
    "  num_genders: 2\n",
    "  num_dialects: 3\n",
    "  dropout: 0.25\n",
    "  head_hidden_dim: 512\n",
    "  freeze_encoder: false\n",
    "\n",
    "training:\n",
    "  batch_size: 32\n",
    "  gradient_accumulation_steps: 4\n",
    "  learning_rate: 2.5e-5\n",
    "  num_epochs: {ADDITIONAL_EPOCHS}\n",
    "  warmup_ratio: 0.025  # Warmup ngắn hơn\n",
    "  weight_decay: 0.0125\n",
    "  gradient_clip: 0.5\n",
    "  lr_scheduler: \"cosine\"\n",
    "  fp16: true\n",
    "  dataloader_num_workers: 2\n",
    "  resume_from_checkpoint: \"{CHECKPOINT_DIR}\"  # Resume từ checkpoint\n",
    "\n",
    "loss:\n",
    "  dialect_weight: 5\n",
    "\n",
    "wandb:\n",
    "  enabled: true\n",
    "  api_key: \"{WANDB_API_KEY}\"\n",
    "  project: \"vimd-speaker-profiling\"\n",
    "  run_name: \"{encoder_short}-resume\"\n",
    "\n",
    "data:\n",
    "  source: \"vimd\"\n",
    "  vimd_path: \"/kaggle/input/vimd-dataset\"\n",
    "\n",
    "audio:\n",
    "  sampling_rate: 16000\n",
    "  max_duration: 5\n",
    "\n",
    "augmentation:\n",
    "  enabled: true\n",
    "  prob: 0.75\n",
    "\n",
    "output:\n",
    "  dir: \"/kaggle/working/output_vimd_resume\"\n",
    "  save_total_limit: 1\n",
    "  metric_for_best_model: \"dialect_acc\"\n",
    "\n",
    "early_stopping:\n",
    "  patience: 5\n",
    "  threshold: 0.001\n",
    "\n",
    "labels:\n",
    "  gender:\n",
    "    Male: 0\n",
    "    Female: 1\n",
    "    0: 0\n",
    "    1: 1\n",
    "  dialect:\n",
    "    North: 0\n",
    "    Central: 1\n",
    "    South: 2\n",
    "\n",
    "seed: 42\n",
    "\"\"\"\n",
    "    \n",
    "    resume_config_path = \"configs/vimd_resume.yaml\"\n",
    "    with open(resume_config_path, \"w\") as f:\n",
    "        f.write(resume_config)\n",
    "    \n",
    "    logger.info(f\"Resume config saved: {resume_config_path}\")\n",
    "    logger.info(f\"Resume from: {CHECKPOINT_DIR}\")\n",
    "    logger.info(f\"Additional epochs: {ADDITIONAL_EPOCHS}\")\n",
    "    logger.info(f\"Learning rate: 1e-5 (reduced for fine-tuning)\")\n",
    "    \n",
    "    # Start resume training\n",
    "    exit_code = os.system(f\"python finetune.py --config {resume_config_path}\")\n",
    "    \n",
    "    if exit_code == 0:\n",
    "        logger.info(\"Resume training completed successfully!\")\n",
    "        # Copy new best model\n",
    "        new_model_dir = \"/kaggle/working/output_vimd_resume/best_model\"\n",
    "        if os.path.exists(new_model_dir):\n",
    "            import shutil\n",
    "            dst = \"/kaggle/working/final_model_vimd/best_model_resumed\"\n",
    "            if os.path.exists(dst):\n",
    "                shutil.rmtree(dst)\n",
    "            shutil.copytree(new_model_dir, dst)\n",
    "            logger.info(f\"Resumed model saved to: {dst}\")\n",
    "    else:\n",
    "        logger.error(f\"Resume training failed with exit code: {exit_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c0781ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:43:11.256296Z",
     "iopub.status.busy": "2025-12-09T07:43:11.255938Z",
     "iopub.status.idle": "2025-12-09T07:47:08.767411Z",
     "shell.execute_reply": "2025-12-09T07:47:08.766417Z"
    },
    "papermill": {
     "duration": 237.746152,
     "end_time": "2025-12-09T07:47:08.768942",
     "exception": false,
     "start_time": "2025-12-09T07:43:11.022790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 07:43:11,260 - INFO - ======================================================================\n",
      "2025-12-09 07:43:11,261 - INFO - EVALUATING RESUMED MODEL ON ViMD TEST SET\n",
      "2025-12-09 07:43:11,262 - INFO - ======================================================================\n",
      "2025-12-09 07:43:20 | INFO | ============================================================\n",
      "2025-12-09 07:43:20 | INFO | SPEAKER PROFILING EVALUATION\n",
      "2025-12-09 07:43:20 | INFO | ============================================================\n",
      "2025-12-09 07:43:20 | INFO | Device: cuda\n",
      "2025-12-09 07:43:20 | INFO | Checkpoint: /kaggle/working/output_vimd_resume/best_model\n",
      "2025-12-09 07:43:20 | INFO | Test set: vimd_test\n",
      "2025-12-09 07:43:20 | INFO | \n",
      "2025-12-09 07:43:20 | INFO | Loading model...\n",
      "2025-12-09 07:43:20 | INFO | Loading encoder: vinai/PhoWhisper-base\n",
      "2025-12-09 07:43:20 | INFO | Encoder class: WhisperModel\n",
      "2025-12-09 07:43:21 | INFO | Hidden size: 512\n",
      "2025-12-09 07:43:22 | INFO | Model loaded successfully!\n",
      "2025-12-09 07:43:22 | INFO | \n",
      "2025-12-09 07:43:22 | INFO | Loading test data: vimd_test\n",
      "2025-12-09 07:43:23 | INFO | Loaded ViMD test: 2026 samples\n",
      "2025-12-09 07:43:23 | INFO | Loaded 2026 samples\n",
      "/kaggle/working/Profiling_gender_dialect/eval.py:183: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(audio_data['path'], sr=self.sr, mono=True)\n",
      "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "/kaggle/working/Profiling_gender_dialect/eval.py:183: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(audio_data['path'], sr=self.sr, mono=True)\n",
      "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "2025-12-09 07:43:26.737348: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-09 07:43:26.737348: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765266206.878060     722 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765266206.878040     723 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765266206.923881     722 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1765266206.923912     723 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "/kaggle/working/Profiling_gender_dialect/eval.py:183: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(audio_data['path'], sr=self.sr, mono=True)\n",
      "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "/kaggle/working/Profiling_gender_dialect/eval.py:183: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(audio_data['path'], sr=self.sr, mono=True)\n",
      "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "2025-12-09 07:47:03 | INFO | \n",
      "2025-12-09 07:47:03 | INFO | ======================================================================\n",
      "2025-12-09 07:47:03 | INFO | RESULTS ON VIMD TEST SET\n",
      "2025-12-09 07:47:03 | INFO | ======================================================================\n",
      "2025-12-09 07:47:03 | INFO | Gender  - Accuracy: 98.57%  |  F1: 98.57%\n",
      "2025-12-09 07:47:03 | INFO | Dialect - Accuracy: 90.42%  |  F1: 90.30%\n",
      "2025-12-09 07:47:03 | INFO | \n",
      "2025-12-09 07:47:03 | INFO | ----------------------------------------------------------------------\n",
      "2025-12-09 07:47:03 | INFO | GENDER CLASSIFICATION REPORT\n",
      "2025-12-09 07:47:03 | INFO | ----------------------------------------------------------------------\n",
      "2025-12-09 07:47:03 | INFO |               precision    recall  f1-score   support\n",
      "2025-12-09 07:47:03 | INFO | \n",
      "2025-12-09 07:47:03 | INFO |         Male     0.9671    0.9849    0.9759       596\n",
      "2025-12-09 07:47:03 | INFO |       Female     0.9937    0.9860    0.9898      1430\n",
      "2025-12-09 07:47:03 | INFO | \n",
      "2025-12-09 07:47:03 | INFO |     accuracy                         0.9857      2026\n",
      "2025-12-09 07:47:03 | INFO |    macro avg     0.9804    0.9855    0.9829      2026\n",
      "2025-12-09 07:47:03 | INFO | weighted avg     0.9858    0.9857    0.9857      2026\n",
      "2025-12-09 07:47:03 | INFO | \n",
      "2025-12-09 07:47:03 | INFO | \n",
      "2025-12-09 07:47:03 | INFO | Gender Confusion Matrix:\n",
      "2025-12-09 07:47:03 | INFO |               Pred_Male  Pred_Female\n",
      "2025-12-09 07:47:03 | INFO | True_Male          587           9\n",
      "2025-12-09 07:47:03 | INFO | True_Female         20        1410\n",
      "2025-12-09 07:47:03 | INFO | \n",
      "2025-12-09 07:47:03 | INFO | ----------------------------------------------------------------------\n",
      "2025-12-09 07:47:03 | INFO | DIALECT CLASSIFICATION REPORT\n",
      "2025-12-09 07:47:03 | INFO | ----------------------------------------------------------------------\n",
      "2025-12-09 07:47:03 | INFO |               precision    recall  f1-score   support\n",
      "2025-12-09 07:47:03 | INFO | \n",
      "2025-12-09 07:47:03 | INFO |        North     0.8999    0.9757    0.9363       783\n",
      "2025-12-09 07:47:03 | INFO |      Central     0.9138    0.8170    0.8627       623\n",
      "2025-12-09 07:47:03 | INFO |        South     0.9016    0.9016    0.9016       620\n",
      "2025-12-09 07:47:03 | INFO | \n",
      "2025-12-09 07:47:03 | INFO |     accuracy                         0.9042      2026\n",
      "2025-12-09 07:47:03 | INFO |    macro avg     0.9051    0.8981    0.9002      2026\n",
      "2025-12-09 07:47:03 | INFO | weighted avg     0.9047    0.9042    0.9030      2026\n",
      "2025-12-09 07:47:03 | INFO | \n",
      "2025-12-09 07:47:03 | INFO | \n",
      "2025-12-09 07:47:03 | INFO | Dialect Confusion Matrix:\n",
      "2025-12-09 07:47:03 | INFO |               Pred_North  Pred_Central  Pred_South\n",
      "2025-12-09 07:47:03 | INFO | True_North          764            12            7\n",
      "2025-12-09 07:47:03 | INFO | True_Central         60           509           54\n",
      "2025-12-09 07:47:03 | INFO | True_South           25            36          559\n",
      "2025-12-09 07:47:05 | WARNING | Failed to save confusion matrix plot: [Errno 2] No such file or directory: '/kaggle/working/output_vimd_resume/eval/confusion_matrix_vimd_test_set.png'\n",
      "2025-12-09 07:47:05 | INFO | Results saved to /kaggle/working/output_vimd_resume/eval/results.json\n",
      "2025-12-09 07:47:05 | INFO | \n",
      "2025-12-09 07:47:05 | INFO | ============================================================\n",
      "2025-12-09 07:47:05 | INFO | EVALUATION COMPLETED!\n",
      "2025-12-09 07:47:05 | INFO | ============================================================\n",
      "2025-12-09 07:47:08,761 - INFO - Evaluation completed successfully!\n",
      "2025-12-09 07:47:08,763 - INFO - Eval results saved to: /kaggle/working/final_model_vimd/eval_resumed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EVALUATE RESUMED MODEL\n",
    "# ============================================================\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Model sau khi train thêm\n",
    "resumed_model_dir = \"/kaggle/working/output_vimd_resume/best_model\"\n",
    "config_path = \"configs/vimd_resume.yaml\"\n",
    "\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"EVALUATING RESUMED MODEL ON ViMD TEST SET\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "if not os.path.exists(resumed_model_dir):\n",
    "    logger.error(f\"Resumed model not found: {resumed_model_dir}\")\n",
    "else:\n",
    "    eval_output_dir = \"/kaggle/working/output_vimd_resume/eval\"\n",
    "    \n",
    "    exit_code = os.system(\n",
    "        f\"python eval.py --checkpoint {resumed_model_dir} --config {config_path} \"\n",
    "        f\"--test_name vimd_test --output_dir {eval_output_dir}\"\n",
    "    )\n",
    "    \n",
    "    if exit_code == 0:\n",
    "        logger.info(\"Evaluation completed successfully!\")\n",
    "        \n",
    "        # Copy eval results to final output\n",
    "        dst_eval = \"/kaggle/working/final_model_vimd/eval_resumed\"\n",
    "        if os.path.exists(dst_eval):\n",
    "            shutil.rmtree(dst_eval)\n",
    "        shutil.copytree(eval_output_dir, dst_eval)\n",
    "        logger.info(f\"Eval results saved to: {dst_eval}\")\n",
    "        \n",
    "        # Print evaluation results\n",
    "        eval_file = os.path.join(eval_output_dir, \"evaluation_results.json\")\n",
    "        if os.path.exists(eval_file):\n",
    "            import json\n",
    "            with open(eval_file, \"r\") as f:\n",
    "                results = json.load(f)\n",
    "            logger.info(\"=\" * 50)\n",
    "            logger.info(\"RESULTS:\")\n",
    "            logger.info(f\"  Gender Accuracy: {results.get('gender_accuracy', 'N/A'):.4f}\")\n",
    "            logger.info(f\"  Dialect Accuracy: {results.get('dialect_accuracy', 'N/A'):.4f}\")\n",
    "            logger.info(\"=\" * 50)\n",
    "    else:\n",
    "        logger.error(f\"Evaluation failed with exit code: {exit_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f36f4961",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:47:09.357253Z",
     "iopub.status.busy": "2025-12-09T07:47:09.356391Z",
     "iopub.status.idle": "2025-12-09T07:47:09.360662Z",
     "shell.execute_reply": "2025-12-09T07:47:09.359938Z"
    },
    "papermill": {
     "duration": 0.359556,
     "end_time": "2025-12-09T07:47:09.361976",
     "exception": false,
     "start_time": "2025-12-09T07:47:09.002420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # CHECK SAVED MODEL\n",
    "# # ============================================================\n",
    "# import os\n",
    "\n",
    "# model_dir = \"/kaggle/working/output_vimd/best_model\"\n",
    "\n",
    "# logger.info(\"=\" * 70)\n",
    "# logger.info(\"SAVED MODEL\")\n",
    "# logger.info(\"=\" * 70)\n",
    "\n",
    "# if os.path.exists(model_dir):\n",
    "#     total_size = 0\n",
    "#     for f in sorted(os.listdir(model_dir)):\n",
    "#         size = os.path.getsize(os.path.join(model_dir, f)) / 1024 / 1024\n",
    "#         total_size += size\n",
    "#         logger.info(f\"  {f}: {size:.1f} MB\")\n",
    "#     logger.info(f\"  Total: {total_size:.1f} MB\")\n",
    "# else:\n",
    "#     logger.warning(\"Model not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "100526c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:47:09.825994Z",
     "iopub.status.busy": "2025-12-09T07:47:09.825386Z",
     "iopub.status.idle": "2025-12-09T07:47:09.829261Z",
     "shell.execute_reply": "2025-12-09T07:47:09.828585Z"
    },
    "papermill": {
     "duration": 0.237601,
     "end_time": "2025-12-09T07:47:09.830548",
     "exception": false,
     "start_time": "2025-12-09T07:47:09.592947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # EVALUATE MODEL ON ViMD TEST SET\n",
    "# # ============================================================\n",
    "# import os\n",
    "\n",
    "# model_dir = \"/kaggle/working/output_vimd/best_model\"\n",
    "# config_path = \"configs/vimd_train.yaml\"\n",
    "\n",
    "# if not os.path.exists(model_dir):\n",
    "#     logger.error(\"Model not found, skipping eval\")\n",
    "# else:\n",
    "#     logger.info(\"=\" * 70)\n",
    "#     logger.info(\"EVALUATING ON ViMD TEST SET\")\n",
    "#     logger.info(\"=\" * 70)\n",
    "    \n",
    "#     exit_code = os.system(\n",
    "#         f\"python eval.py --checkpoint {model_dir} --config {config_path} \"\n",
    "#         f\"--test_name vimd_test --output_dir /kaggle/working/output_vimd/eval\"\n",
    "#     )\n",
    "    \n",
    "#     if exit_code == 0:\n",
    "#         logger.info(\"Evaluation completed successfully\")\n",
    "#     else:\n",
    "#         logger.error(f\"Evaluation failed with exit code: {exit_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49d618cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:47:10.288331Z",
     "iopub.status.busy": "2025-12-09T07:47:10.287926Z",
     "iopub.status.idle": "2025-12-09T07:47:10.292028Z",
     "shell.execute_reply": "2025-12-09T07:47:10.291353Z"
    },
    "papermill": {
     "duration": 0.234538,
     "end_time": "2025-12-09T07:47:10.293475",
     "exception": false,
     "start_time": "2025-12-09T07:47:10.058937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # SAVE MODEL TO KAGGLE OUTPUT\n",
    "# # ============================================================\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# OUTPUT_DIR = \"/kaggle/working/final_model_vimd\"\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# model_dir = \"/kaggle/working/output_vimd/best_model\"\n",
    "# eval_dir = \"/kaggle/working/output_vimd/eval\"\n",
    "\n",
    "# logger.info(\"=\" * 70)\n",
    "# logger.info(\"COPYING MODEL TO OUTPUT\")\n",
    "# logger.info(\"=\" * 70)\n",
    "\n",
    "# if os.path.exists(model_dir):\n",
    "#     dst_dir = f\"{OUTPUT_DIR}/best_model\"\n",
    "#     if os.path.exists(dst_dir):\n",
    "#         shutil.rmtree(dst_dir)\n",
    "#     shutil.copytree(model_dir, dst_dir)\n",
    "#     logger.info(f\"Copied model to: {dst_dir}\")\n",
    "# else:\n",
    "#     logger.warning(\"Model not found\")\n",
    "\n",
    "# if os.path.exists(eval_dir):\n",
    "#     dst_eval = f\"{OUTPUT_DIR}/eval\"\n",
    "#     if os.path.exists(dst_eval):\n",
    "#         shutil.rmtree(dst_eval)\n",
    "#     shutil.copytree(eval_dir, dst_eval)\n",
    "#     logger.info(f\"Copied eval to: {dst_eval}\")\n",
    "\n",
    "# logger.info(f\"All files saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d38593e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-09T07:47:10.751732Z",
     "iopub.status.busy": "2025-12-09T07:47:10.751403Z",
     "iopub.status.idle": "2025-12-09T07:47:10.755643Z",
     "shell.execute_reply": "2025-12-09T07:47:10.754872Z"
    },
    "papermill": {
     "duration": 0.231515,
     "end_time": "2025-12-09T07:47:10.757049",
     "exception": false,
     "start_time": "2025-12-09T07:47:10.525534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # LIST FINAL OUTPUT\n",
    "# # ============================================================\n",
    "# import os\n",
    "\n",
    "# OUTPUT_DIR = \"/kaggle/working/final_model_vimd\"\n",
    "\n",
    "# logger.info(\"=\" * 70)\n",
    "# logger.info(\"FINAL OUTPUT STRUCTURE\")\n",
    "# logger.info(\"=\" * 70)\n",
    "\n",
    "# if os.path.exists(OUTPUT_DIR):\n",
    "#     for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "#         level = root.replace(OUTPUT_DIR, '').count(os.sep)\n",
    "#         indent = '  ' * level\n",
    "#         logger.info(f\"{indent}{os.path.basename(root)}/\")\n",
    "#         sub_indent = '  ' * (level + 1)\n",
    "#         for file in files:\n",
    "#             size = os.path.getsize(os.path.join(root, file)) / 1024 / 1024\n",
    "#             logger.info(f\"{sub_indent}{file} ({size:.1f} MB)\")\n",
    "# else:\n",
    "#     logger.warning(\"Output directory not found\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8829252,
     "sourceId": 13859486,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8954101,
     "sourceId": 14067412,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18259.178009,
   "end_time": "2025-12-09T07:47:11.408952",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-09T02:42:52.230943",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
